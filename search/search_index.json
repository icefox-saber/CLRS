{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Solutions to Introduction to Algorithms Fourth Edition","text":""},{"location":"#refference","title":"Refference","text":"<ul> <li>Introduction to Algorithms Solution Third Edition.</li> </ul>"},{"location":"#more-information","title":"More Information","text":"<p>For a clear commit history, I rebase my repository regularly. Therefore, if you have forked the repository before, consider re-forking it again.</p>"},{"location":"#license","title":"License","text":"<p>Licensed under the MIT License.</p>"},{"location":"Chap01/1.1/","title":"1.1 Algorithms","text":""},{"location":"Chap01/1.1/#11-1","title":"1.1-1","text":"<p>Describe your own real-world example that requires sorting. Describe one that requires finding the shortest distance between two points.</p> <ul> <li>Sorting: rankings in games</li> <li>Finding the shortest distance between two points: takeout delivery always requires deliveryman finding the shortest distance</li> </ul>"},{"location":"Chap01/1.1/#11-2","title":"1.1-2","text":"<p>Other than speed, what other measures of efficiency might you need to consider in a real-world setting?</p> <p>Memory efficiency and coding efficiency.</p>"},{"location":"Chap01/1.1/#11-3","title":"1.1-3","text":"<p>Select a data structure that you have seen, and discuss its strengths and limitations.</p> <p>arrays:</p> <ul> <li>Strengths: random access.</li> <li>Limitations: push, pop, insertion and deletion.</li> </ul>"},{"location":"Chap01/1.1/#11-4","title":"1.1-4","text":"<p>How are the shortest-path and traveling-salesperson problems given above similar?How are they different?</p> <ul> <li>Similar: finding shortest-path.</li> <li>Different: the latter traves every points and returns.</li> </ul>"},{"location":"Chap01/1.1/#11-5","title":"1.1-5","text":"<p>Suggest a real-world problem in which only the best solution will do. Then come up with one in which \"approximately\" the best solution is good enough.</p> <ul> <li>Best: sovling an equaion.</li> <li>Aooroximately: find a solution for which an inequality holds.</li> </ul>"},{"location":"Chap01/1.1/#11-6","title":"1.1-6","text":"<p>Describe a real-world problem in which sometimes the entire input is available before you need to solve the problem, but other times the input is not entirely available in advance and arrives over time.</p> <p>In team work:</p> <ul> <li>Available: if teammates finish their work in time</li> <li>Not avalibale: teammates not finish their work you need in time</li> </ul>"},{"location":"Chap01/1.2/","title":"1.2 Algorithms as a technology","text":""},{"location":"Chap01/1.2/#12-1","title":"1.2-1","text":"<p>Give an example of an application that requires algorithmic content at the application level, and discuss the function of the algorithms involved.</p> <ul> <li>Application: Class schedule</li> <li>Function: avoid confict that a student has two classes at same time or a student has a class which requires completion one unfinished training course</li> </ul>"},{"location":"Chap01/1.2/#12-2","title":"1.2-2","text":"<p>Suppose we are comparing implementations of insertion sort and merge sort on the same machine. For inputs of size $n$ , insertion sort runs in $8n^2$ steps, while merge sort runs in $64n\\lg n$ steps. For which values of $n$ does insertion sort beat merge sort?</p> <p>$$ \\begin{aligned} 8n^2 &lt; 64n \\lg n \\cr 0 &lt; n &lt; 8 \\lg n \\cr 1&lt; 2^n &lt; n^8 \\cr 2 \\leq n \\leq 43 \\cr \\end{aligned} $$</p>"},{"location":"Chap01/1.2/#12-3","title":"1.2-3","text":"<p>What is the smallest value of n such that an algorithm whose running time is $100n^2$ runs faster than an algorithm whose running time is $2^n$ on the same machine?</p> <p>$$ \\begin{aligned} 100n^2 &lt; 2^n \\cr n \\ge 15.\\cr \\end{aligned} $$</p>"},{"location":"Chap01/Problems/1-1/","title":"1-1 Comparison of running times","text":""},{"location":"Chap01/Problems/1-1/#1-1","title":"1-1","text":"<p>For each function $f(n)$ and time $t$ in the following table, determine the largest size $n$ of a problem that can be solved in time $t$, assuming that the algorithm to solve the problem takes $f(n)$ microseconds.</p> Function 1 second 1 minute 1 hour 1 day 1 month 1 year 1 century $lg n$ $2^{10^6}$ $2^{6 \\times 10^7}$ $2^{3.6 \\times 10^9}$ $2^{8.64 \\times 10^{10}}$ $2^{2.59 \\times 10^{12}}$ $2^{3.15 \\times 10^{13}}$ $2^{3.15 \\times 10^{15}}$ $\\sqrt n$ $10^{12}$ $3.6 \\times 10^{15}$ $1.3 \\times 10^{19}$ $7.46 \\times 10^{21}$ $6.72 \\times 10^{24}$ $9.95 \\times 10^{26}$ $9.95 \\times 10^{30}$ $n$ $10^6$ $6 \\times 10^7$ $3.6 \\times 10^9$ $8.64 \\times 10^{10}$ $2.59 \\times 10^{12}$ $3.15 \\times 10^{13}$ $3.15 \\times 10^{15}$ $n lg n$ $6.24 \\times 10^4$ $2.8 \\times 10^6$ $1.33 \\times 10^8$ $2.76 \\times 10^9$ $7.19 \\times 10^{10}$ $7.98 \\times 10^{11}$ $6.86 \\times 10^{13}$ $n^2$ 1,000 7,745 60,000 293,938 1,609,968 5,615,692 56,156,922 $n^3$ 100 391 1,532 4,420 13,736 31,593 146,645 $2^n$ 19 25 31 36 41 44 51 $n!$ 9 11 12 13 15 16 17"},{"location":"Chap02/2.1/","title":"2.1 Insertion sort","text":""},{"location":"Chap02/2.1/#21-1","title":"2.1-1","text":"<p>Using Figure 2.2 as a model, illustrate the operation of $\\text{INSERTION-SORT}$ on the array $A = \\langle 31, 41, 59, 26, 41, 58 \\rangle$.</p> <p></p> <p>as shown in the figure above, the array A change as follow $$ \\begin{aligned} A = \\langle 31, 41, 59, 26, 41, 58 \\rangle\\cr A = \\langle 31, 41, 59, 26, 41, 58 \\rangle\\cr A = \\langle 26, 31, 41, 59, 41, 58 \\rangle\\cr A = \\langle 26, 31, 41, 41, 59, 58 \\rangle\\cr A = \\langle 26, 31, 41, 41, 58, 59 \\rangle\\cr A = \\langle 26, 31, 41, 41, 58, 59 \\rangle\\cr \\end{aligned} $$</p>"},{"location":"Chap02/2.1/#21-2","title":"2.1-2","text":"<p>Consider the procedure SUM-ARRAY on the facing page. It computes the sum of the n numbers in array $A[1:n]$. State a loop invariant for this procedure, and use its initialization, maintenance, and termination properties to show that the SUMARRAY procedure returns the sum of the numbers in $A[1:n]$.</p> <pre><code>SUM-ARRAY(A,n)\nsum = 0\nfor i = 1 to n\n  sum = sum + A[i]\nreturn sum\n</code></pre> <ul> <li>Loop invariant: At the start of the ith iteration of the for loop, the equation $sum = \\sum_{j=1}^{i-1} A[j]$ holds.</li> <li>Initialization: Before the first iteration (i=1), $sum = \\sum_{j=1}^{0} A[j] = 0$ holds.</li> <li>Maintenance: During the ith iteration, <code>sum = sum + A[i]</code> results $sum = \\sum_{j=1}^{i-1} A[j] + A[i] = \\sum_{j=1}^{i} A[j]$, at the end of this iteration, Incrementing i for the next iteration of the for loop then preserves the loop invariant.</li> <li>Termination: The loop terminates when $i &gt; n$, since i increase 1 at each iteration, $i=n+1$ at termination.$sum = \\sum_{j=1}^{i-1} A[j]=\\sum_{j=1}^{n} A[j]$ holds.</li> </ul>"},{"location":"Chap02/2.1/#21-3","title":"2.1-3","text":"<p>Rewrite the INSERTION-SORT procedure to sort into monotonically decreasing instead of monotonically increasing order.</p> <pre><code>INSERTION_SORT_NONINCEASE(A)\n  for i=2 to A.length()\n    key = A[i]\n    j = i -1\n    while j &gt;=1 and A[j] &gt; key\n      A[j+1] = A[j]\n    A[j+1] = key \n</code></pre>"},{"location":"Chap02/2.1/#21-4","title":"2.1-4","text":"<p>Consider the searching problem:</p> <p>Input: A sequence of n numbers $\\langle a1 , a2,..., an \\rangle$stored in array A[1:n] and a value x.</p> <p>Output: An index i such that x equals $A[i]$ or the special value NIL if x does not appear in $A$.</p> <p>Write pseudocode for linear search, which scans through the array from beginning to end, looking for x. Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties.</p> <pre><code>LINEAR_SEARCH (A,x)\n  for i = 1 to A.length()\n    if A[i] == x \n      return i\n    return NIL\n</code></pre> <ul> <li>Loop invariant: At the start of the ith iteration of the for loop, no element in the subarray $A[1:i-1] equal $x$</li> <li>Initialization: Before the first iteration (i=1), $A[1:0]$ is empty, so no elements in $A[1:0]$ eqaul $x$</li> <li>Maintenance: During the ith iteration, if $A[i]==x$ , the algorithms return the correct answer $i$.Otherwise, i increases $1$ and no element in $A[1:i]$ equal $x$, at the start of the ith iteration. Incrementing i for the next iteration of the for loop then preserves the loop invariant.</li> <li>Termination: The loop terminates when $i &gt; n$, since i increase 1 at each iteration, $i=n+1$ at termination. No element in $A[1:i-1]=A[1:n]$ equal $x$.</li> </ul>"},{"location":"Chap02/2.1/#21-5","title":"2.1-5","text":"<p>Consider the problem of adding two n-bit binary integers $a$ and $b$, stored in two $n$-element arrays $A[0:n-1]$ and $B[0:n-1]$, where each element is either $0$ or $1$. $a = \\sum_{i=0}^{n-1} A[i] \\cdot 2^i$, and $b = \\sum_{i=0}^{n-1} B[i] \\cdot 2^i$. The sum $c = a + b$ of the two integers should be stored in binary form in an ($n+1$)-element array $C[0:n]$, where $c = \\sum_{i=0}^{n} C[i] \\cdot 2^i$. Write a procedure ADD-BINARY-INTEGERS that takes as input arrays $A$ and $B$, along with the length n, and returns array C holding the sum.</p> <pre><code>ADD_BINARY_INTEGERS(A,B,n)\n    C[0:n]\n    carry=0\n    for i = 1 to n\n        C[i-1]=(A[i]+B[i]+carry)%2\n        carry =(A[i]+B[i]+carry)/2\n    C[n] = carry\n    return C;\n</code></pre>"},{"location":"Chap02/2.2/","title":"2.2 Analyzing algorithms","text":""},{"location":"Chap02/2.2/#22-1","title":"2.2-1","text":"<p>Express the function $n^3/1000+100n^2-100n+3$ in terms of $\\Theta$-notaion.</p> <p>$\\Theta (n^3)$</p>"},{"location":"Chap02/2.2/#22-2","title":"2.2-2","text":"<p>Consider sorting $n$ numbers stored in array $A[1:n]$ by first finding the smallest element of $A[1:n]$ and exchanging it with the element in $A[1]$. Then find the smallest element of $A[2:n]$, and exchange it with $A[2]$. Then find the smallest element of $A[3:n]$, and exchange it with $A[3]$. Continue in this manner for the first $n-1$ elements of $A$. Write pseudocode for this algorithm, which is known as selection sort. What loop invariant does this algorithm maintain? Why does it need to run for only the first $n-1$ elements, rather than for all $n$ elements? Give the worst-case running time of selection sort in $\\Theta$-notation. Is the best-case running time any better?</p> <ul> <li>pseudocode</li> </ul> <pre><code>SELECTION_SORT(A)\n    for i = 1 to A.length()-1\n        minindex = i\n        for j = i to A.length()\n            if A[j]&lt;A[minindex]\n                minindex = j\n        swap(A[i],A[minindex]) \n</code></pre> <ul> <li>loop invariant: At the start of $i$th iteration, $A[1:i-1]$ consist of the $i-1$ elements of $A[1:n]$ and it is at sort order.</li> <li>When the i-1 smallest elements are sort in the subarray $A[1:n-1], the biggest element must be $A[n]$, and then A[1:n] are sort.</li> <li>worse case: in each iteration, $j$ increase from $i$ to $n$, $running time = (n) + (n-1) + ... + (2) = \\Theta (n^2)$</li> <li>best case: the same as worst case since in each iteration in inner for loop, j must go from i to n to get the minindex.</li> </ul>"},{"location":"Chap02/2.2/#22-3","title":"2.2-3","text":"<p>Consider linear search again (see Exercise 2.1-4). How many elements of the input array need to be checked on the average, assuming that the element being searched for is equally likely to be any element in the array? How about in the worst case?</p> <p>Using $\\Theta$-notation, give the average-case and worst-case running times of linear search. Justify your answers.</p> <ul> <li>average-case: $\\text{running time}\\sum_{i=1}^{n} i/n = (n+1)/2 = \\Theta(n)$</li> <li>worst-case: $\\text{running time} = n =\\Theta(n)$</li> </ul>"},{"location":"Chap02/2.2/#22-4","title":"2.2-4","text":"<p>How can you modify any sorting algorithm to have a good best-case running time</p> <p>modidy combine with insertion sort ,such as check whether each element in array is smaller(bigger) than the righthand element, since it produce a best - case running time of $\\Theta(n)$, the same as travel throught the array, which is a neccessary step for sorting.</p>"},{"location":"Chap02/2.3/","title":"2.3 Designing algorithms","text":""},{"location":"Chap02/2.3/#23-1","title":"2.3-1","text":"<p>Using Figure 2.4 as a model, illustrate the operation of merge sort on an array initially containing the sequence $\\langle 3, 41, 52, 26, 38, 57, 9, 49 \\rangle$.</p> <p>$$ [3|41|52|26|38|57|9|49]$$</p> <p>$$\\downarrow$$</p> <p>$$[3|41|52|26] \\quad [38|57|9|49]$$</p> <p>$$\\downarrow$$</p> <p>$$[3|41] \\quad [52|26] \\quad [38|57] \\quad [9|49]$$</p> <p>$$\\downarrow$$</p> <p>$$[3] \\quad [41] \\quad [52] \\quad [26] \\quad [38] \\quad [57] \\quad [9] \\quad [49]$$</p> <p>$$\\downarrow$$</p> <p>$$[3|41] \\quad [26|52] \\quad [38|57] \\quad [9|49]$$</p> <p>$$\\downarrow$$</p> <p>$$[3|26|41|52] \\quad [9|38|49|57]$$</p> <p>$$\\downarrow$$</p> <p>$$[3|9|26|38|41|49|52|57]$$</p>"},{"location":"Chap02/2.3/#23-2","title":"2.3-2","text":"<p>The test in line 1 of the MERGE-SORT procedure reads \"if $p \\geq r$\" rather than \"if $p \\neq r$.\" If MERGE-SORT is called with $p &gt; r$, then the subarray $A[p:r]$ is empty. Argue that as long as the initial call of MERGE-SORT(A, 1, n) has $n \\geq 1$, the test \"if $p \\neq r$\" suffices to ensure that no recursive call has $p &gt; r$.</p> <ul> <li>inductive case:</li> </ul> <p>$$ \\begin{aligned} if \\quad p \\leq r\\cr if \\quad p &lt; r\\cr q = \\lfloor (p+r)/2 \\rfloor\\cr q \\geq p\\cr q+1 \\leq r\\cr MERGE-SORT(A,p,q)\\cr MERGE-SORT(A,q+1,r)\\cr \\text{else return}\\cr \\text{no recursive call has p&gt;r}\\cr p \\leq r \\text{ holds in new recursive call}\\cr \\end{aligned} $$</p> <ul> <li>base case:</li> </ul> <p>$$ \\begin{aligned} MERGE-SORT(A,1,n)\\cr n \\geq 1\\cr p \\leq r holds\\cr \\end{aligned} $$</p> <p>\"if $p \\neq r$\" suffices to ensure that no recursive call has $p &gt; r$.</p>"},{"location":"Chap02/2.3/#23-3","title":"2.3-3","text":"<p>State a loop invariant for the while loop of lines 12\u201318 of the MERGE procedure. Show how to use it, along with the while loops of lines 20\u201323 and 24\u201327, to prove that the MERGE procedure is correct.</p> <ul> <li> <p>loop invariant: at the start of each iteration, $A[p:k-1]$ consist of the smallest elements of union of $L$ and $R$ at sort order.</p> </li> <li> <p>when the while loop of lines 12-18 finish $A[p:k-1]$ consist of the smallest elements of union of $L$ and $R$ at sort order, either while loop of lines 20\u201323 or 24\u201327 will be run, $L$ or $R$ has the biggest rest elements in sort order.when the second while loop finish, they are added to the end of A, and A consist of all elements at sort order.</p> </li> </ul>"},{"location":"Chap02/2.3/#23-4","title":"2.3-4","text":"<p>Use mathematical induction to show that when $n \\geq 2$ is an exact power of 2, the solution of the recurrence</p> <p>$$ \\begin{aligned} T(n) = \\begin{cases} 2 &amp; \\text{if } n = 2, \\cr 2T(n/2) + n &amp; \\text{if } n &gt; 2 \\end{cases} \\end{aligned} $$ is $T(n) = n \\lg n$.</p> <p>$$ \\begin{aligned}     T(n) &amp; = T(2^t)\\cr     &amp; = 2T(2^{t-1}) + 2^t\\cr     &amp; = 4T(2^{t-2}) + 2\\cdot{2^t}\\cr     &amp; = 2^{t-1}T(2) + 2^t\\cdot{\\lg 2^t }\\cr     &amp; = n\\lg n \\end{aligned} $$</p>"},{"location":"Chap02/2.3/#23-5","title":"2.3-5","text":"<p>You can also think of insertion sort as a recursive algorithm. In order to sort $A[1:n]$, recursively sort the subarray $A[1:n-1]$ and then insert $A[n]$ into the sorted subarray $A[1:n-1]$. Write pseudocode for this recursive version of insertion sort. Give a recurrence for its worst-case running time.</p> <pre><code>INSERTIONSORT_RECURXIVE(A,n)\n    if n = 1\n        return\n    INSERTIONSORT_RECURXIVE(A,n-1)\n    i = n-1\n    key = A[n]\n    while A[i]&gt;key and i&gt;=1\n        A[i+1]=A[i]\n        i--\n    A[i+1]=key\n</code></pre>"},{"location":"Chap02/2.3/#23-6","title":"2.3-6","text":"<p>Referring back to the searching problem (see Exercise 2.1-4), observe that if the subarray being searched is already sorted, the searching algorithm can check the midpoint of the subarray against v and eliminate half of the subarray from further consideration. The binary search algorithm repeats this procedure, halving the size of the remaining portion of the subarray each time. Write pseudocode, either iterative or recursive, for binary search. Argue that the worst-case running time of binary search is $\\Theta (\\lg n)$. iterative:</p> <pre><code>BINARY_SEARCH_ITERATIVE(A,l,r,x)\n    while r&gt;=l \n        mid = (l+r)/2\n        if A[mid] == x\n            return mid\n        else if A[mid] &gt; x\n            r = mid -1\n        else\n            l = mid +1\n    return NIL\n</code></pre> <p>recursive:</p> <pre><code>BINARY_SEARCH_RECURVISE(A,l,r,x)\n    if(l&gt;r)\n        return NIL\n    mid= (l+r)/2\n    if A[mid] == x\n        return x\n    if A[mid] &lt; x\n        return A[A,mid+1,r,x]\n    if A[mid] &gt; x\n        return A[A,l,mid-1,x]\n</code></pre> <p>the worst-case is no x in $A$, BINARY SEARCH will end al condition l&gt;r which occurs when A[l:r] is empty. since BINARY SEARCH halving the size of A[l:r] each time, it cost $\\Theta (\\lg n)$ to make A[l:r] empty</p>"},{"location":"Chap02/2.3/#23-7","title":"2.3-7","text":"<p>The while loop of lines 5-7 of the INSERTION-SORT procedure in Section 2.1 uses a linear search to scan (backward) through the sorted subarray $A[1:j-1]$. What if insertion sort used a binary search (see Exercise 2.3-6) instead of a linear search? Would that improve the overall worst-case running time of insertion sort to $\\Theta(n \\lg n)$</p> <p>It can save search time, but can not save movement time. At worst -case, it cost $j-1 = \\Theta(j)$ time  to move elements bigger than $A[j]$ in $A[1:j-1]$.The same as the original INSERTION-SORT Therefore, that improve the overall worst-case running time of insertion sort to $\\Theta(n\\lg n)$</p>"},{"location":"Chap02/2.3/#23-8","title":"2.3-8","text":"<p>Dscribe Describe an algorithm that, given a set $S$ of $n$ integers and another integer $x$, determines whether $S$ contains two elements that sum to exactly $x$. Your algorithm should take $\\Theta (n \\lg n)$ time in the worst case.</p> <pre><code>FUCTION(S,x)\n    n=S.lenfth()\n    MERGE_SORT(S,1,n) //cost $\\Theta n \\lgn$\n    i=1\n    j=n\n    sum=A[i]+A[j]\n    while sum !=x and j&gt;i //cost \\Theta n\n        if sum &gt; x \n            j--\n        else i++ \n    if sum == x\n        return i j\n    else\n        return NIL\n</code></pre> <p>The time complexity of the algorithm is $\\Theta (n \\lg n)+\\Theta (n)$</p>"},{"location":"Chap02/Problems/2-1/","title":"2-1 Insertion sort on small arrays in merge sort","text":"<p>Although merge sort runs in $\\Theta(n \\lg n)$ worst-case time and insertion sort runs in $\\Theta (n^2)$ worst-case time, the constant factors in insertion sort can make it faster in practice for small problem sizes on many machines. Thus it makes sense to coarsen the leaves of the recursion by using insertion sort within merge sort when subproblems become sufficiently small. Consider a modification to merge sort in which $n/k$ sublists of length $k$ are sorted using insertion sort and then merged using the standard merging mechanism, where $k$ is a value to be determined.</p> <p>a. Show that insertion sort can sort the $n/k$ sublists, each of length k, in $\\Theta (nk)$ worst-case time.</p> <p>b. Show how to merge the sublists in $\\Theta (n \\lg (n/k))$ worst-case time.</p> <p>c. Given that the modified alogotithm runs in $\\Theta (nk+n \\lg (n/k))$ worst-case time, what is the largest value of k as a function of n for which the modified algorithm has the same running time as standard merge sort, in terms of $\\Theta$-notation?</p> <p>d. How should you choose k in practice?</p>"},{"location":"Chap02/Problems/2-1/#a","title":"a","text":"<p>Each sublists of length can be sorted in $\\Theta (k^2)$, there are n/k sublists, so in n $n/k \\cdot \\Theta(k^2) = \\Theta (nk)$ sorst-case time.</p>"},{"location":"Chap02/Problems/2-1/#b","title":"b","text":"<p>Each level of merging need to merge $n$ elements, and there are $\\lg (n/k)$levels(since we merge each two sublists in one in each level). So the worse-case time to merge the sublists is $\\Theta (n \\lg (n/k))$.</p>"},{"location":"Chap02/Problems/2-1/#c","title":"c","text":"<p>$$ \\begin{aligned}   &amp;  if \\quad \\Theta (nk+n \\lg (n/k)) = \\Theta (n \\lg n)\\cr   &amp; \\exist  \\quad c_{1} , c_{2} , n_{0} \\quad satisfies:\\cr   &amp;  c_{1}(n \\lg n) \\leq nk+n \\lg (n/k) \\leq c_{2}(n \\lg n) \\quad for \\quad n &gt; n_{0}\\cr     &amp;(c_{1}-1)(\\lg n )\\leq k - \\lg k \\leq (c_{2}-1)(\\lg n ) \\quad for \\quad n &gt; n_{0}\\cr    &amp; since \\quad lg k \\llless k, \\quad \\text{we have } k = \\Theta (\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap02/Problems/2-1/#d","title":"d","text":"<p>Choose k be the largest size of the sublists that insertion-sort is faster than merge-sort.</p>"},{"location":"Chap02/Problems/2-2/","title":"2-2 Correctness of bubblesort","text":"<p>Bubblesort is a popular, but inefficient, sorting algorithm. It works by repeatedly swapping adjacent elements that are out of order. The procedure BUBBLESORT sorts array $A[1:n]$.</p> <pre><code>BUBBLESORT(A,n)\n  for i = 1 to n - 1\n      for j = n downto i + 1\n          if A[j] &lt; A[j-1]\n              exchange A[j] with A[j-1]\n</code></pre> <p>a. Let $A'$ denote the array $A$ after BUBBLESORT&gt; (A, n) is executed. To prove that it terminates and that $$ A'[1] \\leq A'[2] \\leq \\cdots \\leq A'[n] $$ In order to show that BUBBLESORT actually sorts, what else do you need to prove?</p> <p>The next two parts prove inequality (2.5).</p> <p>b. State precisely a loop invariant for the for loop in lines 2-4, and prove that this loop invariant holds. Your proof should use the structure of the loop-nvariant proof presented in this chapter.</p> <p>c. Using the termination condition of the loop invariant proved in part (b), state a loop invariant for the for loop in lines 1-4 that allows you to prove inequal ity (2.5). Your proof should use the structure of the loop-invariant proof presented in this chapter.</p> <p>d. What is the worst-case running time of BUBBLESORT? How does it compare with the running time of INSERTION-SORT?</p>"},{"location":"Chap02/Problems/2-2/#a","title":"a","text":"<p>$A'$ consist of all the elements of $A$</p>"},{"location":"Chap02/Problems/2-2/#b","title":"b","text":"<p>loop invariant: At the start of each iteration of $for$ loop in lines 2-4, $A[j:n]$ consist of the elements originally in $A[j:n]$ before entering the loop but possibly in a different order, and $A[j]$ is the smallest one of them.</p> <p>Initialization: $j=n$, $A[n]$ consist of the elements originally in A$[n]$ and it is the smallest.</p> <p>Maintenance: During the iteration, if$A[j-1]&lt;A[j]$, we swap them. So A[j-1] will be the smallest and A[j-1:n] consist of the elements originally in $A[j-1:n]$. Decresementing j preserves the loop invariant.</p> <p>Termination: the loop terminates  when $j = i$, $A[i:n]$ consist of the elements originally in $A[i:n]$ before entering the loop but possibly in a different order, and $A[i]$ is the smallest one of them.</p>"},{"location":"Chap02/Problems/2-2/#c","title":"c","text":"<p>Loop invariant: At the start of each iteration of the for loop of lines 1-4, the subarray $A[1..i \u2212 1]$ consists of the $i - 1$ smallest elements in $A[1..n]$ in sorted order. $A[i..n]$ consists of the $n - i + 1$ remaining elements in $A[1..n]$.</p> <p>Initialization: Initially the subarray $A[1..i \u2212 1]$ is empty and trivially this is the smallest element of the subarray.</p> <p>Maintenance: From part (b), after the execution of the inner loop, $A[i]$ will be the smallest element of the subarray $A[i..n]$. And in the beginning of the outer loop, $A[1..i \u2212 1]$ consists of elements that are smaller than the elements of $A[i..n]$, in sorted order. So, after the execution of the outer loop, subarray $A[1..i]$ will consists of elements that are smaller than the elements of $A[i + 1..n]$, in sorted order.</p> <p>Termination: The loop terminates when $i = A.length$. At that point the array $A[1..n]$ will consists of all elements in sorted order.</p>"},{"location":"Chap02/Problems/2-2/#d","title":"d","text":"<p>The $i$th iteration of the for loop of lines 1-4 will cause $n \u2212 i$ iterations of the for loop of lines 2-4, each with constant time execution, so the worst-case running time of bubble sort is $\\Theta(n^2)$ which is same as the worst-case running time of insertion sort.</p>"},{"location":"Chap02/Problems/2-3/","title":"2-3 Correctness of Horner\u2019s rule","text":"<p>You are given the coefficents $a_0; a_1; a_2; \\dots; a_n$ of a polynomial $$ \\begin{aligned}     P(x) &amp; = \\sum_{k=0}^n a_kx^k \\cr     &amp;=a_0+a_1x+a_2x^2+\\cdots +a_{n-1}x^{n-1}+a_nx^n, \\end{aligned} $$ and you want to evaluate this polynomial for a given value of x. Horner's rule says to evalusate the polynomial according to this &gt; parenthesization: $$ P(x) = a_0 + x ( a_1 + x ( a_2 + \\cdots + x ( a_{n-1} + x a_n) &gt; \\cdots )). $$ The procedure HORNER implements Horner**\u2019s rule to &gt; evaluate $P(x)$, &gt; givem the coefficients $a_0,a_1,a_2,\\dots,&gt; a_n$ in an array $A[0:n]$ and &gt; the value of $x$.</p> <pre><code>HORNER (A,n,x)\n    p=0\n    for i = n downto 0\n        p = A[i] + x * p\n    return p\n</code></pre> <p>a. In terms of $\\Theta$-notation, what is the tunning time of this procedure?</p> <p>b. Write pseudocode to implement the naive polynomial-evaluation algorithm that computes each term of the polynomial from scratch. What is the running time of this algorithm? How does it compare with HORNER?</p> <p>c. Consider the following loop invariant for the procedure HORNER:</p> <p>At the start of each iteration of the for loop of lines 2-3,</p> <p>$$ p = \\sum _{k=0}^{n-(i+1)} A[k+i+1] \\cdot x^k $$</p> <p>Interpret a summation with no terms at equaling 0. Following the structure of th loop-invariant proof presented in this chapter, use this loop invariant to show that, at termination, $p = \\sum _{k=0} ^{n} A[k] \\cdot x^k$</p>"},{"location":"Chap02/Problems/2-3/#a","title":"a","text":"<p>$\\Theta(n)$</p>"},{"location":"Chap02/Problems/2-3/#b","title":"b","text":"<p>pseudocode:</p> <pre><code>naive_polynomia_evaluation (A,n,x)\n    p=0\n    for i = 0 to n\n        term = A[i]\n        for j = 1 to i\n            term = term * x\n        p = term + p\n    return p\n</code></pre> <p>$\\text{running time:} \\Theta (n^2)$, HORNER beat it asymptotically.</p>"},{"location":"Chap02/Problems/2-3/#c","title":"c","text":"<p>Initialization: $i=n, p=\\sum_{k=0}^{n-(n+1)} A[k+n+1] \\cdot{x^k} = 0$, loop invariant holds.</p> <p>Maintenance: in the end of each iteration:</p> <p>$$ \\begin{aligned}     p &amp; =  A[i] + x \\cdot{\\sum_{k=0}^{n-(i+1)} A[k+i+1] \\cdot {x^k}} \\cr     &amp; =  A[i] + \\sum_{k=0}^{n-(i+1)} A[k+i+1] \\cdot{x^{k+1}}\\cr     &amp; = A[i] \\cdot{x^0} + \\sum_{k=1}^{n-i} A[k+i] \\cdot{x^{k}}\\cr     &amp; = \\sum_{k=0}^{n-i} A[k+i] \\cdot{x^k}\\cr \\end{aligned} $$</p> <p>Decrementing i preserves the loop invariant.</p> <p>Termination: The loop terminates at $i=\u22121$. If we substitute,</p> <p>$$ p = \\sum_{k=0}^{n-(i+1)} A[k+i+1] \\cdot{x^k} = \\sum_{k=0}^{n} A[k] \\cdot{x^k} $$</p>"},{"location":"Chap02/Problems/2-4/","title":"2-4 Inversions","text":"<p>Let $A[1:n]$ be an array of n distinct numbers. If $i &lt; j$ and $A[i] &gt; A[j]$, then the pair $(i,j)$ is called an inversion of A.</p> <p>a. List the five inversions of the array $\\langle 2,3,8,6,1 \\rangle$.</p> <p>b. What array with elements from the set $\\langle 1,2,\\dots ,n \\rangle$. has the most inversions? How many does it have?</p> <p>c. What is the relationship between the running time of insertion sort and the number of inversions in the input array? Justify your answer.</p> <p>d. Give an algorithm that determines the number of inversions in any permutation. on n elementa in $\\Theta (n \\lg n)$ worse-case time. (Hint: Modify merge sort.)</p> <ul> <li> <p>a. (1,5),(2,5),(3,4),(3,5),(4,5)</p> </li> <li> <p>b. $\\langle n,n-1,\\dots ,1\\rangle \\text{ totally has } {n \\choose 2} = n(n-1)/2 \\text{ inversions}$</p> </li> <li> <p>c. The running time of insertsion sort is a constant times the number of inversions, since each time exchanging elements in insertion sort reduces Inversions num by 1.</p> </li> <li> <p>d.</p> </li> </ul> <pre><code>MERGE_INVERSIONS (A,l,r,mid)\n    inversions = 0\n    n1 = mid - l\n    n2 = r - mid -1\n    L[0:n1]\n    for i = 0 to n1\n        L[i] = A[l + i]\n    R[0:n2]\n    for j = 0 to n2\n        R[j] = A[mid + 1 + j]\n    i=0 \n    j=0\n    k=l\n    while i &lt;= n1 and j &lt;= n2\n        if L[i] &lt;= R[j]\n            A[k] = L[i]\n            i++\n            k++\n        else\n            A[k] = R[j]\n            j++\n            k++\n            inversions + = n1 - i + 1\n    while i &lt;= n1\n        A[k] = L[i]\n        k++\n        i++\n    while j &lt;= n2\n        A[k] = R[j]\n        k++\n        j++\nMERGE_SORT_INVERSIONS(A,l,r)\n    inversions = 0\n    if l &gt;=r\n        return inversions\n    mid = floor((l+r)/2)\n    inversions + = MERGE_SORT_INVERSIONS(A,l,mid)\n    inversions + = MERGE_SORT_INVERSIONS(A,mid+1,r)\n    inversions + = MERGE_INVERSIONS(A,l,mid,r)\n    return inversions\n</code></pre>"},{"location":"Chap03/3.1/","title":"3.1 O-notation, $\\Omega$-notation, and $\\Theta$-notation","text":""},{"location":"Chap03/3.1/#31-1","title":"3.1-1","text":"<p>Modify the lower-bound argument for insertion sort to handle input sizes that are not necessarily a multiple of 3.</p> <p>At least $\\lfloor \\frac n 3 \\rfloor \\geq \\frac 3n-1$ elements have to pass through at least $\\lfloor \\frac n 3 \\rfloor \\geq \\frac 3n-1$ the time taken by INSERTION-SORT in the worst case is at least proportional to $(\\frac 3n-1)(\\frac 3n-1)=\\Omega(n^2)$</p>"},{"location":"Chap03/3.1/#31-2","title":"3.1-2","text":"<p>Using reasoning similar to what we used for insertion sort, analyze the running time of the selection sort algorithm from Exercise 2.2-2.</p> <p>Inner loop in selection sort iterates $(n-i+1)$ times, for i = 1 to n-1, so the running time of selection sort is $\\sum_{i=1}^{n-1} (n-i+1)=(n+2)(n-1)/2=\\Theta(n^2)$</p>"},{"location":"Chap03/3.1/#31-3","title":"3.1-3","text":"<p>Suppose that $\\alpha$ is a fraction in the range $0 &lt; \\alpha &lt; 1$. Show how to generalize the lower-bound argument for insertion sort to consider an input in which the $\\alpha n$ largest values start in the first $\\alpha n$ positions. What additional restriction do you need to put on $\\alpha$? What value of $\\alpha$ maximizes the number of times that the $\\alpha n$ largest values must pass through each of the middle $(1-2 \\alpha)$ array positions?</p> <p>At least $\\alpha n$ values have to pass through at least $(1-2 \\alpha)n$ times. insertion sort is at least proportional to $(\\alpha n)(1-2 \\alpha)n = \\alpha(1-2\\alpha)n^2 = \\Omega(n^2)$ if $\\alpha(1-2\\alpha)=\\Omega(1)$</p> <p>$\\max(\\alpha(1-2\\alpha))=\\frac 1 8$ when $\\alpha = \\frac 1 4$</p>"},{"location":"Chap03/3.2/","title":"3.2Asymptotic notation formal definitions","text":""},{"location":"Chap03/3.2/#32-1","title":"3.2-1","text":"<p>Let $f(n)$ and $g(n)$ be the asympotically nonnegative functions. Using the basic definition of $\\Theta$-notation, prove that max ${f(n),g(n)} = \\Theta (f(n) + g(n))$.</p> <p>$$ \\begin{aligned} \\text{Let } n_0 \\text{ be a value that makes f(n) greater than 0 for n &gt;}n_0\\cr \\text{Then for }n&gt;n_0\\cr \\frac 1 2 (f(n)+g(n)) \\leq \\max(f(n),g(n)) \\leq 1(f(n)+g(n))\\cr \\max(f(n),g(n))=\\Theta(f(n)+g(n))\\cr \\end{aligned} $$</p> <p>Tips: I will omit details like \"$\\text{Let } n_0 \\text{ be a value that makes f(n) greater than 0 for n &gt;}n_0$\"\"$\\exist c,n_0$\" and \"for $n &gt; n_0$\" for convenience from here.</p>"},{"location":"Chap03/3.2/#32-2","title":"3.2-2","text":"<p>Explain why the statement, \"The running time of algorithm A is at least $O(n^2)$,\"if meaningless.</p> <p>$A = O(n^2)$ means $\\exist n_0,c:A \\leq cn^2$ for $n&gt;n_0$.</p> <p>$A$ is at least $O(n)$ means $A \\geq \\text{a function that }\\leq cn^2$</p> <p>You can say A can be any function or no function. Both are meaningless.</p>"},{"location":"Chap03/3.2/#32-3","title":"3.2-3","text":"<p>Is $2^{n+1}=O(2^n)$? Is $2^{2n}=O(2^n)$?</p> <ul> <li>$2^{n+1} \\leq 2*2^n$, Yes</li> <li>$2^{2n} \\leq c \\cdot 2^n \\text{ implies } c \\geq 2^n$, no constant satisfies it. No</li> </ul>"},{"location":"Chap03/3.2/#32-4","title":"3.2-4","text":"<p>Prove Theorem 3.1.</p> <p>$$ \\begin{aligned} &amp; f(n) = O(g(n)) \\quad and \\quad f(n)=\\Omega(g(n))\\cr &amp; \\implies c_1 g(n)\\leq f(n) \\leq c_2g(n)\\cr &amp; \\implies f(n) = \\Theta(g(n))\\cr &amp; f(n) = \\Theta(g(n))\\cr &amp; \\implies c_1 g(n)\\leq f(n) \\leq c_2g(n)\\cr &amp; \\implies f(n) = O(g(n)) \\quad and \\quad f(n)=\\Omega(g(n))\\cr \\end{aligned} $$</p> <p>We have $f(n) = \\Theta(g(n))$ if and only if $f(n) = \\Omega(g(n))$ and $f(n) = O(g(n))$</p>"},{"location":"Chap03/3.2/#32-5","title":"3.2-5","text":"<p>Prove that the running time of an algorithm is $\\Theta(g(n))$ if and only if its worst-case running time $O(g(n))$ and its best-casse running time is $\\Omega (g(n))$.</p> <p>$$ \\begin{aligned}     A=\\Theta(g(n)) &amp; \\implies c_1 g(n) \\leq A \\leq c_2 g(n) \\text{ for all cases}\\cr     &amp; \\implies \\text{worst-case running time}= O(n)\\cr     &amp; \\And \\text{best-case running time}=\\Omega(g(n))\\cr     \\text{worst-case running time}= O(n) &amp; \\implies A \\leq c_2 g(n) (1)\\cr     \\text{best-case running time}=\\Omega(g(n)) &amp; \\implies c_1 g(n) \\leq A (2)\\cr     (1) \\And (2) &amp; \\implies A=\\Theta(g(n))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/3.2/#32-6","title":"3.2-6","text":"<p>Prove that $o(g(n)) \\cap \\omega (g(n))$ is empty set.</p> <p>$f(n) &lt; cg(n) \\And f(n) &gt; c(g(n)) \\implies NIL$</p>"},{"location":"Chap03/3.2/#32-7","title":"3.2-7","text":"<p>We can extend our notation to the case of two parameters n and m that can go to $\\infty$ independently at different rates. For a given function $g(n,m)$, we denote by $O(g(n,m))$ the set of functions</p> <p>$$ \\begin{aligned} O(g(n, m)) =\\lbrace f(n, m):   &amp; \\text{ there exist positive constants } c, n_0, \\text{ and } m_0 \\cr   &amp; \\text{ such that } 0 \\leq f(n, m) \\leq cg(n, m)\\cr   &amp; \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0.\\rbrace\\cr \\end{aligned} $$</p> <p>Give corresponding definitionss for $\\Omega(g(n,m))$ and $\\Theta(g(n,m))$.</p> <p>$$ \\begin{aligned} \\Theta(g(n, m)) =\\lbrace f(n, m): &amp; \\text{ there exist positive constants } c_1,c_2, n_0, \\text{ and } m_0 \\cr &amp; \\text{ such that } 0 \\leq c_1g(n,m)\\leq f(n, m) \\leq c_2g(n, m)\\cr &amp; \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0.\\rbrace\\cr \\Omega(g(n, m)) =\\lbrace f(n, m): &amp; \\text{ there exist positive constants } c, n_0, \\text{ and } m_0 \\cr &amp; \\text{ such that } 0 \\leq cg(n,m)\\leq f(n, m)\\cr &amp; \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0.\\rbrace\\cr \\end{aligned} $$</p>"},{"location":"Chap03/3.3/","title":"3.3 Standard notations and common functions","text":""},{"location":"Chap03/3.3/#33-1","title":"3.3-1","text":"<p>Show that if $f(n)$ and $g(n)$ are monotonically increasing functions, then so are the functions $f(n)+g(n)$ and $f(g(n))$, and if $f(n)$ and $g(n)$ are in addition nonnegative, then $f(n)\\cdot g(n)$</p> <p>$$ \\begin{aligned}     \\forall n_1 \\geq n_2: &amp; f(n_1) \\geq f(n_2) \\And g(n_1) \\geq g(n_2)\\cr     \\implies &amp; f(n_1)+g(n_1) \\geq f(n_2)+g(n_2)\\cr     \\implies &amp; f(g(n_1) \\geq g(n_2)) \\geq f(g(n_2))\\cr     &amp; if \\quad f(n) \\geq 0 \\And g(n) \\geq 0\\cr     \\implies &amp; f(g(n_1)\\geq g(n_2)\\geq 0)\\geq f(g(n_2)) \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-2","title":"3.3-2","text":"<p>Prove that $\\lfloor \\alpha n \\rfloor +\\lceil (1-\\alpha )n \\rceil = n$ for any integer n and real number $\\alpha$ in the range $0 \\leq \\alpha \\leq 1$</p> <p>$$ \\begin{aligned}     \\lfloor \\alpha n \\rfloor +\\lceil (1-\\alpha )n \\rceil &amp; = \\lfloor \\alpha n \\rfloor +\\lceil -\\alpha n\\rceil + n\\cr     &amp;=\\lfloor \\alpha n \\rfloor - \\lfloor \\alpha n \\rfloor +n\\cr     &amp;=n \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-3","title":"3.3-3","text":"<p>Use equation (3.14) or other means to show that $(n+o(n))^k = \\Theta(n^k)$ for any real constant k. Conclude that $\\lceil n \\rceil ^k$ and $\\lfloor n \\rfloor ^k = \\Theta(n^k)$.</p> <p>$$ \\begin{aligned}     n^k\\leq(n+o(n))^k=n^k(1+\\frac {o(n)}n)^k \\leq n^k e^{\\frac {o(n)}n} \\leq en^k\\cr     \\implies (n+o(n))^k = \\Theta(n^k)\\cr     \\lceil n \\rceil ^k= (n+o(n))^k=\\Theta(n^k)\\cr     \\lfloor n \\rfloor ^k = (n+o(n))^k=\\Theta(n^k)\\cr \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-4","title":"3.3-4","text":"<p>Prove the following</p> <p>a. Equation(3.21).</p> <p>b. Equations(3.26)-(3.28).</p> <p>c. $\\lg(\\Theta(n))=\\Theta(\\lg n)$</p> <ul> <li>a. $a^{\\lg _b c} = a^{\\frac {lg _a c}{\\lg _a b}}=c^{\\lg _b a}$</li> <li>b.</li> </ul> <p>$$ \\begin{aligned}     n! = \\sqrt {2\\pi n}(\\frac n e)^n(1+\\Theta(\\frac{1}{n}))\\cr     \\forall c&gt;0,\\lim_{n \\to \\infin} \\frac {n!=\\frac{\\sqrt {2\\pi n}} {e^n}(1+\\Theta(\\frac 1 n))n^n} {cn^n} = \\lim_{n \\to \\infin} \\frac{\\sqrt {2\\pi n}} {e^n}(1+\\Theta(\\frac 1 n)) = 0\\cr     \\implies n!=o(n^n)\\text{ (Equation(3.21))}\\cr     \\lim_{n \\to \\infin} \\frac {n!=\\frac{\\sqrt {2\\pi n}} {e^n}(1+\\Theta(\\frac 1 n))n^n} {2^n} = \\lim_{n \\to \\infin }\\sqrt{2\\pi n}(1+\\Theta(\\frac 1 n))(\\frac{n}{2e})^n = \\infin \\cr     \\implies n! =\\omega(n) \\text{ (Equation(3.27))}\\cr     \\lg (n!)=n\\lg n+ \\frac{1}{2}\\lg(2\\pi n)+n\\lg e+\\lg(1+\\Theta(\\frac{1}{n})) = \\Theta(n\\lg n)\\text{ (Equation(3.28))}\\cr \\end{aligned} $$</p> <ul> <li>c.</li> </ul> <p>$$ \\begin{aligned}     \\forall f(n) \\in \\Theta(n)\\cr     c_3\\lg n\\leq \\lg {c_1 n} \\leq \\lg(f(n))\\leq \\lg{c_2 n} \\leq c_4\\lg n\\cr     \\implies \\lg(\\Theta(n))=\\Theta(\\lg n) \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-5","title":"3.3-5","text":"<p>Is the function $\\lceil \\lg n \\rceil !$ polynomially bounded? Is the function $\\lceil \\lg \\lg n \\rceil !$ polynomially bounded?</p> <p>Polynomially bounded:$f(n) \\leq cn^k \\implies \\lg(f(n))=ck\\lg(n)\\implies \\lg(f(n))=O(\\lg n)$</p> <p>$$ \\begin{aligned}     \\lg(\\lceil \\lg n\\rceil !)&amp; =\\Theta(\\lceil \\lg n\\rceil \\lg(\\lceil \\lg n\\rceil))\\cr     &amp;=\\Theta(\\lg n \\lg(\\lg n))\\cr     &amp;\\implies \\lceil \\lg n\\rceil !\\text{ is not polynomially bouded}\\cr     \\lg \\lceil \\lg \\lg n \\rceil !&amp;=\\Theta(\\lceil \\lg \\lg n\\rceil \\lg\\lceil\\lg\\lg n\\rceil)\\cr     &amp; =O(\\lg^2\\lg n)\\cr     &amp; =O(\\lg n)\\cr     &amp; \\implies\\lceil\\lg\\lg n\\rceil \\text{ is Polynomially bounded} \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-6","title":"3.3-6","text":"<p>Which is asymptotically larger: $\\lg(\\lg^{\\ast}n)$ or $\\lg^{\\ast}(\\lg n)$?</p> <p>$$ \\begin{aligned}     n=2^k\\cr     \\lim_{n\\to\\infin}\\frac{\\lg(\\lg^{\\ast}n)}{\\lg^{\\ast}(\\lg n)}&amp; = \\lim_{k\\to\\infin}\\frac{\\lg(\\lg^{\\ast} 2^k)}{\\lg^{\\ast}(\\lg 2^k)}\\cr     &amp;= \\lim_{k\\to\\infin} \\frac{\\lg(1+\\lg^{\\ast}k)}{\\lg^{\\ast}k}\\cr     &amp; =\\lim_{t\\to \\infin}\\frac{\\lg(1+t)}{t}\\cr     &amp; = 0 \\end{aligned} $$ $\\lg^{\\ast}(\\lg n)$ is larger.</p>"},{"location":"Chap03/3.3/#33-7","title":"3.3-7","text":"<p>Show that the golden ratio $\\phi$ and its conjugate $\\hat{\\phi}$ both satisfy the equation $x^2 = x + 1$</p> <p>$$ \\begin{aligned}     ax^2+bx+c=0\\implies x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\cr     x^2-x-1=0 \\implies x=\\frac{1 \\pm\\sqrt{5}}{2}=\\phi \\And \\hat\\phi \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-8","title":"3.3-8","text":"<p>prove by induction that the i-th Fibonacci num satisfies the equation $$ F_i = (\\phi^i - \\hat{\\phi}^i)/\\sqrt{5} $$ where $\\phi$ is the golden ratio and $\\hat{\\phi}$ is its conjugate.</p> <p>For inductive case:</p> <p>$$ \\begin{aligned}     &amp; \\text{truth: }\\phi\\hat\\phi=-1,\\phi+\\hat\\phi=1\\cr     \\text{assume }:\\cr     F_{i-1} &amp;= (\\phi^{i-1} - \\hat{\\phi}^{i-1})/\\sqrt{5}\\cr     F_{i} &amp;= (\\phi^{i} - \\hat{\\phi}^{i})/\\sqrt{5}\\cr     \\text{then:}\\cr     F_{i-1}+F_i &amp; =(\\phi^{i-1} - \\hat{\\phi}^{i-1})/\\sqrt{5}+(\\phi+\\hat\\phi)(\\phi^i - \\hat{\\phi}^i)/\\sqrt{5}\\cr     &amp; = (\\phi^{i+1}-\\hat\\phi^{i+1})/\\sqrt 5+((1+\\phi\\hat\\phi)\\phi^{i-1}-(1+\\phi\\hat\\phi)\\hat\\phi^{i-1})/\\sqrt 5\\cr     &amp; =F_{i+1}\\cr \\end{aligned} $$</p> <p>For base case: $$ \\begin{aligned}     (\\phi^{1} - \\hat{\\phi}^{1})/\\sqrt{5}=1=F_1\\cr     (\\phi^{2} - \\hat{\\phi}^{2})/\\sqrt{5}=(\\phi^{1}+1 - (\\hat{\\phi}^{1}+1))/\\sqrt{5}=1=F_2\\cr \\end{aligned} $$</p> <p>Tips: I will omit details like\"assume $F_i$ $F_{i-1}$\" and proof of base case from now on for convenience.</p>"},{"location":"Chap03/3.3/#33-9","title":"3.3-9","text":"<p>Show that $k\\lg k = \\Theta(n)$ implies $k = \\Theta(n/\\lg n)$.</p> <p>$$ \\begin{aligned}     &amp; k\\lg k=\\Theta(n)\\cr     \\implies &amp; n=\\Theta(k\\lg k)\\cr     \\implies &amp; \\lg n = \\Theta(\\lg k + \\lg\\lg k)=\\Theta(\\lg k)\\cr     \\implies &amp; n/\\lg n = \\Theta(k\\lg k)/\\Theta(\\lg k)=\\Theta (k)\\cr     \\implies &amp; k = \\Theta(n/\\lg n) \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-1/","title":"3-1 Asymptotic behavior of polynomials","text":"<p>Let</p> <p>$$ P(n) = \\sum_{i=0}^d a_i n^i, $$</p> <p>where $a_d &gt; 0$, be a degree-d polynomial in n, and let k be a constant. Use the definitions of the asymptotic notations to prove the following properties.</p> <p>a. if $k \\geq d$, then $p(n)=O(n^k)$</p> <p>b. if $k \\leq d$, then $p(n)=\\Omega(n^k)$</p> <p>c. if $k =d$, then $p(n)=\\Theta(n^k)$</p> <p>d. if $k &gt; d$, then $p(n)=o(n^k)$</p> <p>e. if $k &lt; d$, then $p(n)=\\omega(n^k)$</p>"},{"location":"Chap03/Problems/3-1/#a","title":"a","text":"<p>To prove $p(n)=O(n^k)$, we only need to prove:</p> <p>$$ \\begin{aligned}     \\sum_{i=0}^{d}a_in^i \\leq cn^k (\\text{ for }n \\geq n_0)\\cr \\end{aligned} $$</p> <p>Which is implied by</p> <p>$$ \\begin{aligned}     c \\geq \\sum_{i=0}^da_in^{i-k}  (\\text{ for }n \\geq n_0)\\cr \\end{aligned} $$ since $\\sum_{i=0}^{d}a_i \\geq \\sum_{i=0}^da_in^{i-k} (\\text{ for }n \\geq 1)$</p> <p>$\\text{choose } c = \\sum_{i=0}^{d}a_i$ and $n_0 = 1$ satisfies.</p> <ul> <li>Tips: the description of $n&gt;n_0$ and $c$ will be omitted next time.</li> </ul>"},{"location":"Chap03/Problems/3-1/#b","title":"b","text":"<p>We only need to prove:</p> <p>$$     \\sum_{i=0}^{d}a_in^i \\geq cn^k $$</p> <p>which is implied by:</p> <p>$$ \\begin{aligned}     c \\leq \\sum_{i=0}^da_in^{i-k}\\cr     c=a_d \\leq \\sum_{i=0}^da_in^{i-k}\\cr     n_0=1 \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-1/#c","title":"c","text":"<p>We only need to prove:</p> <p>$$     c_1n^d\\leq \\sum_{i=0}^{d}a_in^i \\leq c_2n^{d} $$</p> <p>which is implied by:</p> <p>$$ \\begin{aligned}     c_1 \\leq \\sum_{i=0}^da_in^{i-d} \\leq c_2\\cr     c_1=a_d \\leq \\sum_{i=0}^da_in^{i-d} \\leq \\sum_{i=0}^{d} a_i=c_2\\cr     n_0=1 \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-1/#d","title":"d","text":"<p>We only need to prove: $$ \\begin{aligned}     \\forall c &gt;0,\\exist n_0 \\text{ satisfies: }\\cr     \\sum_{i=0}^{d} a_i n^i &lt; cn^k (\\text{ for } n &gt; n_0)\\cr     \\impliedby c &gt; \\sum_{i=0}^{d} a_in^{i-k}\\cr     \\text{since }\\sum_{i=0}^{d} a_in^{d-k} \\geq \\sum_{i=0}^{d} a_in^{i-k}\\cr     n_0=\\sqrt[d-k]{\\frac {c} {\\sum_{i=0}^d a_i}} \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-1/#e","title":"e","text":"<p>We only need to prove: $$ \\begin{aligned}     \\forall c &gt;0,\\exist n_0 \\text{ satisfies: }\\cr     \\sum_{i=0}^{d} a_in^i &gt; cn^k (\\text{ for }n&gt;n_0)\\cr     \\impliedby c &lt; \\sum_{i=0}^{d} a_in^{i-k}\\cr     \\text{since } a_d n^{d-k} \\leq \\sum_{i=0}^{d} a_in^{i-k}\\cr     n_0=\\sqrt[d-k]{\\frac {c} {a_d}} \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-2/","title":"3-2 Relative asymptotic growths","text":"<p>Indicate, for each pair of expressions $(A, B)$ in the table below whether A is $O, o,\\Omega,\\omega$, or $\\Theta$ of B. Assume that $k \\geq 1,\\epsilon &gt;,$ and $c&gt;1$ are constants. Write your answer in the foem of the table with \"yes\" or \"no\" written in each box.</p> A B O o $\\Omega$ $\\omega$ $\\Theta$ $\\lg^k n$ $n^{\\epsilon}$ yes yes no no no $n^k$ $c^n$ yes yes no no no $\\sqrt{n}$ $c^{\\sin n}$ no no no no no $2^n$ $2^{n/2}$ no no yes yes no $n^{\\lg c}$ $c^{\\lg n}$ yes no yes no yes $\\lg (n!)$ $\\lg (n^n)$ yes no yes no yes"},{"location":"Chap03/Problems/3-3/","title":"3-3 Ordering by asymptotic growth rates","text":"<p>a. Rank the following fuctions vy oefer of growth. That is, find an arrangement $g_1,g_2,\\dots,g_{30}$ of the functions satisfying $g_1=\\Omega(g_2),g_2=\\Omega(g_3),\\dots ,g_{29}=\\Omega(g_{30})$. Partion your list into equivalence classes such that functions $f(n)$ and $g(n)$ belong to the same class if and only if $f(n)$ = $\\Theta g(n)$</p> <p>$$ \\begin{array}{} &amp; \\lg(\\lg^{\\ast}n) &amp; 2^{\\lg^{\\ast}n} &amp; (\\sqrt {2})^{\\lg n} &amp; n^2 &amp; n! &amp; (\\lg n)!\\cr &amp; (3/2)^n &amp; n^3 &amp; \\lg^2n &amp; \\lg(n!) &amp; 2^{2^n} &amp; n^{1/\\lg n}\\cr &amp; \\ln\\ln n &amp; \\lg^{\\ast}n &amp; n\\cdot 2^n &amp; n^{\\lg\\lg n} &amp; \\ln n &amp; 1\\cr &amp; 2^{\\lg n} &amp; (\\lg n)^{\\lg n} &amp; e^n &amp; 4^{\\lg n} &amp; (n+1)! &amp; \\sqrt{\\lg n}\\cr &amp; \\lg^{\\ast}(\\lg n) &amp; 2^{\\sqrt{2 \\lg n}}&amp; n &amp; 2^n&amp; n\\lg n  &amp; 2^{2^{n+1}}\\cr \\end{array} $$</p> <p>b. Give an example of a single nonnegative function $f(n)$ such that for all functions $g_i(n)$ in part (a), $f(n)$ is neither $O(g_i(n))$ nor $\\Omega (g_i(n))$.</p>"},{"location":"Chap03/Problems/3-3/#a","title":"a","text":"<p>$$ \\begin{aligned}     2^{2^{n+1}}\\cr     2^{2^n}\\cr     (n+1)!\\cr     n!\\cr     e^n\\cr     n\\cdot 2^n\\cr     2^n\\cr     (3/2)^n\\cr     n^{\\lg\\lg n} \\And (\\lg n)^{\\lg n}\\cr     (\\lg n)!\\cr     n^3\\cr     n^2 \\And 4^{\\lg n} \\cr     \\lg(n!) \\And n\\lg n\\cr     2^{\\lg n} \\And n\\cr     (\\sqrt {2})^{\\lg n}\\cr     2^{\\sqrt{2\\lg n}}\\cr     \\lg^2 n\\cr     \\ln n\\cr     \\sqrt{\\lg n}\\cr     \\ln\\ln n\\cr     2^{\\lg^{\\ast}n}\\cr     \\lg^{\\ast}n \\And \\lg^{\\ast}(\\lg n)\\cr     \\lg(\\lg^{\\ast}n)  \\cr     n^{1/\\lg n}\\And 1\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-3/#b","title":"b","text":"<p>$|\\sin{n}|\\cdot 2^{2^{2^{n+1}}}$</p>"},{"location":"Chap03/Problems/3-4/","title":"3-4 Asymptotic notation properties","text":"<p>Let $f(n)$ and $g(n)$ be asymptotically positive functions. Prove or disprove each of the following conjectures.</p> <p>a. $f(n)=O(g(n))$ implies $g(n)=O(f(n))$.</p> <p>b. $f(n)+g(n)=\\Theta(\\min(f(n),g(n)))$.</p> <p>c. $f(n)=O(g(n))$ implies $\\lg(f(n))=O(\\lg(g(n)))$, where $\\lg g(n)\\geq 1$ and $f(n)\\geq 1$ for all sufficiently large $n$.</p> <p>d. $f(n)=O(g(n))$ implies $2^{f(n)} = O(2^{g(n)})$.</p> <p>e. $f(n)=O((f(n))^2)$.</p> <p>f. $f(n)=O(g(n))$ implies $g(n)=\\Omega(f(n))$.</p> <p>g. $f(n)=\\Theta(f(n/2))$.</p> <p>h. $f(n)+o(f(n))=\\Theta(f(n))$. </p>"},{"location":"Chap03/Problems/3-4/#a","title":"a","text":"<p>Disprove: $n=O(n^2)$ but $n^2\\neq O(n)$</p>"},{"location":"Chap03/Problems/3-4/#b","title":"b","text":"<p>Disprove:$n+n^2\\neq \\Theta(\\min(n,n^2))$</p>"},{"location":"Chap03/Problems/3-4/#c","title":"c","text":"<p>prove:</p> <p>$$ \\begin{aligned}     0\\leq f(n)\\leq cg(n) &amp; \\implies \\lg(f(n))\\leq \\lg c+\\lg(g(n))\\cr     \\text{to prove: }&amp;\\lg(f(n)) \\leq d\\lg(g(n))\\cr     \\text{only need to prove: }&amp; \\lg c + \\lg(g(n)) \\leq d\\lg(g(n))\\cr     \\text{choose } &amp; \\quad d = 1+\\lg c \\text{ satisfies} \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-4/#d","title":"d","text":"<p>prove:</p> <p>$$ \\begin{aligned}     0\\leq f(n)\\leq cg(n) \\implies 2^{f(n)}\\leq 2^{cg(n)}=2^c\\cdot c^{g(n) } \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-4/#e","title":"e","text":"<p>disprove: $\\frac{1}{n}\\neq O(\\frac{1}{n^2})$</p>"},{"location":"Chap03/Problems/3-4/#f","title":"f","text":"<p>prove: $0\\leq f(n)\\leq cg(n) \\implies g(n)=\\Omega (f(n))$</p>"},{"location":"Chap03/Problems/3-4/#g","title":"g","text":"<p>disprove:$2^n\\neq\\Theta(2^{\\frac{n}{2}})$</p>"},{"location":"Chap03/Problems/3-4/#h","title":"h","text":"<p>prove: $f(n)\\leq f(n)+o(f(n))\\leq 2f(n)$ since $0\\leq o(f(n))&lt; f(n)$</p>"},{"location":"Chap03/Problems/3-5/","title":"3-5 Manipulating asymptotic notation","text":"<p>Let $f(n)$ and $g(n)$ be asymptotically positive functions. Prove the following identities: a. $\\Theta(\\Theta(f(n)))=\\Theta(f(n))$.</p> <p>b. $\\Theta(f(n))+O(f(n))=\\Theta(f(n))$.</p> <p>c. $\\Theta(f(n))+\\Theta(g(n)) =\\Theta(f(n)+g(n))$.</p> <p>d. $\\Theta(f(n))\\cdot \\Theta(g(n))=\\Theta(f(n)\\cdot g(n))$.</p> <p>e. Argue that for any real constants $a_1,b_1&gt;0$ and integer constants $k_1,k_2$, the following asymptotic bound holds:</p> <p>$(a_1 n)^{k_1}\\lg^{k_2}(a_2 n)=\\Theta(n^{k_1}\\lg^{k_2}n)$.</p> <p>$\\star$ f. Prove that for $S\\subseteq \\Z$, we have</p> <p>$$ \\sum_{k\\in S}\\Theta(f(k))=\\Theta(\\sum_{k\\in S}f(k)), $$</p> <p>assuming that both sums converge.</p> <p>$\\star$ g. Show that for $S \\subseteq \\Z$, the following asymptotic bound does not necessarily hold, even assuming that both products converge, by giving a counterexample:</p> <p>$$ \\prod_{k\\in S}\\Theta(f(k))=\\Theta(\\prod_{k\\in S}f(k)). $$ </p>"},{"location":"Chap03/Problems/3-5/#a","title":"a","text":"<p>$$ \\begin{aligned}     g(n)=\\Theta(f(n))\\cr     \\implies c_1 f(n)\\leq g(n) \\leq c_2 f(n)\\cr     h(n)=\\Theta(g(n))\\cr     \\implies c_1 d_1 f(n)\\leq d_1 g(n)\\leq h(n)\\leq d_2g(n)\\leq c_2 d_2 f(n)\\cr     \\implies h(n)=\\Theta(f(n))\\cr     \\implies\\Theta(\\Theta(f(n)))=\\Theta(f(n))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#b","title":"b","text":"<p>$$ \\begin{aligned}     \\forall g(n)=\\Theta(f(n)) \\And h(n)=\\Theta(f(n))\\cr     \\implies c_2 f(n) \\leq g(n)\\leq c_1 f(n) \\And 0 \\leq h(n)\\leq      df(n)\\cr      \\implies c_1f(n)\\leq g(n)+h(n)\\leq (c_1+d)f(n)\\cr      \\implies g(n)+h(n)=\\Theta(f(n))\\cr      \\implies \\Theta(f(n))+O(f(n))=\\Theta(f(n))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#c","title":"c","text":"<p>$$ \\begin{aligned}     \\forall A(n)=\\Theta(f(n))\\And B(n)=\\Theta(g(n))\\cr     \\implies c_1 f(n)\\leq A(n)\\leq c_2 f(n)\\And d_1 f(n)\\leq B(n) \\leq d_2 f(n)\\cr     \\implies \\min(c_1,d_1)(f(n)+g(n))\\leq A(n)+B(n)\\leq \\max(c_2+d_2)(f(n)+g(n))\\cr     \\implies \\Theta(f(n))+\\Theta(g(n)) =\\Theta(f(n)+g(n)) \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#d","title":"d","text":"<p>$$ \\begin{aligned}     \\forall A(n)=\\Theta(f(n))\\And B(n)=\\Theta(g(n))\\cr     \\implies c_1 f(n)\\leq A(n)\\leq c_2 f(n)\\And d_1 f(n)\\leq B(n) \\leq d_2 f(n)\\cr     \\implies c_1 d_1f(n)\\cdot g(n)\\leq A(n)\\cdot B(n)\\leq c_2 d_2f(n)\\cdot g(n)\\cr     \\implies \\Theta(f(n))\\cdot \\Theta(g(n)) =\\Theta(f(n)\\cdot g(n)) \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#e","title":"e","text":"<p>$$ \\begin{aligned}     \\text{we only need to prove: }c_1 n^{k_1}\\lg^{k_2}n\\leq (a_1 n)^{k_1}\\lg^{k_2}(a_2 n)\\leq c_2 n^{k_1}\\lg^{k_2}n\\cr     \\text{which is implied by: }d_1 lg^{k_2}n\\leq\\lg^{k_2}(a_2 n)\\leq d_2\\lg^{k_2}n\\cr     \\text{which is implied by: }e_1\\lg n\\leq \\lg a+\\lg n \\leq e_2 \\lg n\\cr     \\text{which is implied by: }e_1\\leq \\lg a/\\lg n +1 \\leq e_2\\cr     \\text{choose }e_1=1/2 \\quad e_2=2 \\quad n_0=\\lg max(a^2,2^{\\lg a}) \\text{ satisfies}\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#star-f","title":"$\\star$ f","text":"<p>$$ \\begin{aligned}     g(k)=\\Theta(f(k))\\cr     \\implies \\sum_{k\\in S} c_kf(k)\\leq \\sum_{k\\in S} g(k)\\leq \\sum_{k\\in S} d_kf(k)\\cr     \\implies \\min( c_k) \\sum_{k\\in S} f(k)\\leq \\sum_{k\\in S} g(k)\\leq \\max(d_k)\\sum_{k\\in S} f(k)\\cr     \\implies \\sum_{k\\in S}\\Theta(f(k))=\\Theta(\\sum_{k\\in S}f(k))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#star-g","title":"$\\star$ g","text":"<p>$\\prod_{k\\in S} \\frac 1 2=\\prod_{k\\in S} \\Theta(\\frac{1}{2^n})=\\Theta(0)\\neq \\Theta(\\prod_{k\\in S} 1)=\\Theta(1)$</p>"},{"location":"Chap03/Problems/3-6/","title":"3-6 Variations on $O$ and $\\Omega$","text":"<p>Some authors define $\\Omega$-notation in a slightly different way than this textbook does. We'll use the nomenclature $\\stackrel{\\infty}{\\Omega}$ (read \"omega infinity\") for this alternative definition. We say that f(n) = $\\stackrel{\\infty}{\\Omega}(g(n))$if there exists a positive constant $c$ such that $f(n) \\geq cg(n) \\geq 0$ for infinitely many integers n.</p> <p>a. Show that for any two asymptotically nonnegative functions $f(n)$ and $g(n)$, we have $f(n)=O(g(n))$ or $f(n)=\\stackrel{\\infty}{\\Omega}(g(n))$ (or both).</p> <p>b. Show that there exist two asymptotically nonnegative functions $f(n)$ and $g(n)$ for which neither $f(n) = O(g(n))$ nor $f(n) = \\Omega(g(n))$ holds.</p> <p>c. Describe the potential advantages and disadvantages of using $\\stackrel{\\infty}{\\Omega}$-notation instead of $\\Omega$-notation to characterize the running times of programs.</p> <p>Some authors also define $O$ in a slightly different manner. We\u2019ll use $O'$ for the alternative definition: $f(n)=O'(g(n))$ if and only if $|f(n)|=O(g(n))$.</p> <p>d. What happens to each direction of the \"if and only if\" in Theorem 3.1 on page 56 if we substitute $O'$ for $O$ but still use $\\Omega$?</p> <p>Some authors define $\\tilde{O}$(read \"soft-oh\") to mean O with logarithmic factors ignored:</p> <p>$$ \\begin{aligned} \\tilde{O}(g(n))=\\lbrace f(n) : &amp; \\text{ there exist positive constants }c, k, \\text{ and } n_0 \\text{ such that }\\cr &amp; 0\\leq f(n)\\leq cg(n)\\lg^k(n) \\text{ for all } n\\geq n_0\\rbrace .\\cr \\end{aligned} $$</p> <p>e. Define $\\tilde{\\Omega}$ and $\\tilde{\\Theta}$ in a similar manner. Prove the corresponding analog to Theorem 3.1.</p>"},{"location":"Chap03/Problems/3-6/#a","title":"a","text":"<p>$$ \\begin{aligned}     \\text{if } f(n)\\neq O(g(n))\\cr     \\text{there must be infinite integers that }f(n) \\geq cg(n)\\geq 0\\cr     \\text{otherwise choose the last } n = \\text{ that } f(n)\\geq cg(n) \\text{ as } n_0\\cr     \\text{for all }n\\geq n_0,f(n)\\leq cg(n)\\implies f(n)=O(g(n))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-6/#b","title":"b","text":"<p>$f(n)=n|\\sin n|,g(n)=1$</p>"},{"location":"Chap03/Problems/3-6/#c","title":"c","text":"<ul> <li>advantages: By a we can know that $O$ and $\\stackrel{\\infty}{\\Omega}$ can characterize relationship between any two function.</li> <li>disadvantages: Not precise.</li> </ul>"},{"location":"Chap03/Problems/3-6/#d","title":"d","text":"<p>$$ \\begin{aligned}     \\forall f(n) , g(n):\\cr     f(n)=\\Theta(g(n)) \\implies f(n)=\\Omega(g(n)) \\And f(n)=O'(g(n))\\cr     \\text{but the conversion is not true} \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-6/#e","title":"e","text":"<p>$$ \\begin{aligned}     \\tilde\\Omega(g(n))=\\lbrace f(n): &amp; \\text{ there exist positive constants }c,k,\\text{ and } n_0 \\text{ such that}\\cr     &amp; 0\\leq f(n) \\leq cg(n)\\lg^{k}(n) \\text{ for all }n \\geq n_0\\rbrace .\\cr     \\tilde\\Theta(g(n))=\\lbrace f(n): &amp; \\text{ there exist positive constants }c_1,c_2,k,\\text{ and } n_0 \\text{ such that}\\cr     &amp; 0\\leq c_1g(n)\\lg^{k}(n)\\leq f(n) \\leq c_2g(n)\\lg^{k}(n) \\text{ for all }n \\geq n_0\\rbrace .\\cr     \\text{according to there definity: } &amp; f(n)=\\tilde\\Theta(g(n))\\iff f(n)=\\tilde O(g(n)) \\And f(n)=\\tilde\\Omega (g(n))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-7/","title":"3-7 Iterated functions","text":"<p>We can apply the iteration operator * used in the $\\lg^{\\ast}$ function to any monotonically increasing function $f(n)$ over the real. For a given constant $c \\in \\R$, we define the iterated dunction $f_{c}^{\\ast}$ by</p> <p>$f_{c}^{\\ast}=\\min \\lbrace i\\geq0:f^{(i)}\\leq c\\rbrace$,</p> <p>which need not be well defined in all cases. In other words, the quantity $f_{c}^{\\ast}(n)$ is the minimum number of iterated applications of the function $f$ required to reduce its argument down to $c$ or less.</p> <p>For each of the functions $f(n)$ and constants $c$ in the table below, give as tight a bound as possible on $f_{c}^{\\ast}(n)$. If there is no $i$ such that $f^{(i)} \\leq c$, write \"undefined\" as your answer.</p> $f(n)$ $c$ $f_{c}^{\\ast}(n)$ $n-1$ $0$ $\\Theta(n)$ $\\lg n$ $1$ $\\Theta(\\lg^{\\ast}n)$ $n/2$ $1$ $\\Theta(\\lg n)$ $n/2$ $2$ $\\Theta(\\lg n)$ $\\sqrt{n}$ $2$ $\\theta(\\lg\\lg n)$ $\\sqrt{n}$ $1$ undefined $n^{1/3}$ $2$ $\\Theta(\\log_3\\lg n)$"},{"location":"Chap04/4.1/","title":"4.1 Multiplying square matrices","text":"<p>Note: You may wish to read Section 4.5 before attempting some of these exercises.</p>"},{"location":"Chap04/4.1/#41-1","title":"4.1-1","text":"<p>Generalize MATRIX-MULTIPLY-RECURSIVE to multiple $n\\times n$ matrices for which $n$ is not necessarily an exact power of 2. Give a recurrence describing its running time. Argue that it runs in $\\Theta(n^3)$ time in the worst case.</p> <pre><code>MATRIX_MULTIPLY_RECURSIVE_GENERALIZE(A,B,C,n)\n    if n is not exact power of 2\n        new_n = next power of 2 of n//O(n)\n    use 0 to extend A B C into new_n*new_n matrix//O(n^2)\n        MATRIX_MULTIPLY_RECURSIVE(A,B,C,new_n)//Theta(n^3)\n    choose the n*n part of C as result by index method//O(1)\n    return \n</code></pre> <p>running time = $\\Theta(n^3)+O(n^2)+O(1)+O(n)=\\Theta(n^3)$</p>"},{"location":"Chap04/4.1/#41-2","title":"4.1-2","text":"<p>How quickly can you multiply a $kn\\times n$ matrix (kn rows and n columns) by an $n\\times kn$ matrix, where $k\\geq 1$ , using MATRIX-MULTIPLY-RECURSIVE as a subtoutine? Answer the same question for multiplying an $n\\times kn$ matrix by a $kn\\times n$ matrix. Which is asymptotically faster, and by how much?</p> <ul> <li>mulyiplying a $kn\\times n$ matrix by an $n\\times kn$ matrix needs $k^2$ times' call of MATRIX-MULTIPLY-RECURSIVE(A,B,C,n), taking $\\Theta(k^2n^3)$ in total.</li> <li>mulyiple a $n\\times kn$ matrix by an $kn\\times n$ matrix needs $k$ times' call of MATRIX-MULTIPLY-RECURSIVE(A,B,C,n), and need $k-1$ times' addition, taking $\\Theta(kn^3+kn^2)=\\Theta(kn^3)$ in total.</li> <li>The latter is asympototicallt faster.</li> </ul>"},{"location":"Chap04/4.1/#41-3","title":"4.1-3","text":"<p>Suppose that instead of partitioning matrices by index calculation in MATRIX-MULTIPLY-RECURSIVE, you copy the appropriate elements of A,B,and C into separate $n/2\\times n/2$ submarices $A_{11},A_{12},A_{21},A_{22};B_{11},B_{12},B_{21},B_{22}$; and $C_{11},C_{12},C_{21},C_{22}$, respectively. After the recursive calls, you copy the results from $C_{11},C_{12},C_{21}$, and $C_{22}$ back into the appropriate places in $C$. How does recurrence (4.9) change, and what is its solution?</p> <p>$T(n)=8T(n/2)+\\Theta(n^2)$</p> <p>According to the master theorem, $T(n)=\\Theta(n^3)$</p>"},{"location":"Chap04/4.1/#41-4","title":"4.1-4","text":"<p>Write pseudocode for a divide-and-conquer algorithm MATRIX-ADD-RECURSIVE that sums two $n\\times n$ matrices $A$ and $B$ by partitioning each of them into four $n/2\\times n/2$ submatrices and then recursively summing corresponding pairs of submatrices. Assume that matrix partitioning uses $\\Theta(1)$-time index calculations. Write a recurrence for the worst-case running time of MATRIX-ADD-RECURSIVEE, and solve your recurrence. What happens if you use $\\Theta(n^2)$-time copying to implement the partitioning instead of index calculations?</p> <pre><code>MATRIX_ADD_RECURSIVEE(A,B,C,n)\n    if n==1\n        c11=a11+b11\n    partition A,B,C into n/2*n/2 submatrixs\n        A11,A12,A21,A22;B11,B12,B21,B22;and C11,C12,C21,C22; \n    MATRIX_ADD_RECURSIVEE(A11,B11,C11,n/2)\n    MATRIX_ADD_RECURSIVEE(A12,B12,C12,n/2)\n    MATRIX_ADD_RECURSIVEE(A21,B21,C21,n/2)\n    MATRIX_ADD_RECURSIVEE(A22,B22,C22,n/2)\n</code></pre> <p>$T(n)=4T(n/2)+\\Theta(1)$</p> <p>According to master theorem, $T(n)=\\Theta(n^2)$</p> <p>if use copying,$T(n)=4T(n/2)+\\Theta(n^2)\\implies T(n)=\\Theta(n^2\\lg n)$</p>"},{"location":"Chap04/4.2/","title":"4.2 Strassen\u2019s algorithm for matrix multiplication","text":"<p>Note: You may wish to read Section 4.5 before attempting some of these exercises.</p>"},{"location":"Chap04/4.2/#42-1","title":"4.2-1","text":"<p>Use Strassen\u2019s algorithm to compute the matrix product</p> <p>$$ \\begin{pmatrix}   1 &amp; 3\\cr   7 &amp; 5 \\end{pmatrix} \\begin{pmatrix}   6 &amp; 8\\cr   4 &amp; 2 \\end{pmatrix} $$</p> <p>Show your work.</p> <p>$$ \\begin{aligned}     S_1 &amp; = B_{12} - B_{22} = 8 - 2 = 6\\cr     S_2 &amp; = A_{11} + A_{12} = 1 + 3 = 4\\cr     S_3 &amp; = A_{21} + A_{22} = 7 + 5 = 12\\cr     S_4 &amp; = B_{21} - B_{11} = 4 - 6 = -2\\cr     S_5 &amp; = A_{11} + A_{22} = 1 + 5 = 6\\cr     S_6 &amp; = B_{11} + B_{22} = 6 + 2 = 8\\cr     S_7 &amp; = A_{12} - A_{22} = 3 - 5 = -2\\cr     S_8 &amp; = B_{21} + B_{22} = 4 + 2 = 6\\cr     S_9 &amp; = A_{11} - A_{21} = 1 - 7 = -6\\cr     S_{10} &amp; = B_{11} + B_{12} = 6 + 8 = 14\\cr     P_1 &amp; = A_{11} \\cdot S_1 = 1 * 6 = 6\\cr     P_2 &amp; = S_2 \\cdot B_{22}=4 * 2 = 8\\cr     P_3 &amp; = S_3 \\cdot B_{11} = 12 * 6 = 72\\cr     P_4 &amp; = A_{22} \\cdot S_{4} = 5 * (-2) = -10\\cr     P_5 &amp; = S_5 \\cdot S_6 = 6 * 8 = 48\\cr     P_6 &amp; = S_7 \\cdot S_8 = -2 * 6 = -12\\cr     P_7 &amp; = S_9 \\cdot S_10 = -6 * 14 = -84\\cr     C_{11} &amp; = C_{11} + P_5 + P_4 - P_2 + P_6 = 48 + (-10) - 8 + (-12) = 18\\cr     C_{12} &amp; = C_{12} + P_1 + P_2 = 6 + 8 = 14\\cr     C_{21} &amp; = C_{21} + P_3 + P_4 = 72 + (-10) = 62\\cr     C_{22} &amp;  =C_{22} + P_5 + P_1 - P_3 - P_7 = 48 + 6 - 72 - (-84) = 66\\cr     C &amp; =     \\begin{pmatrix}     18 &amp; 14\\cr     62 &amp; 66\\cr     \\end{pmatrix} \\end{aligned} $$</p>"},{"location":"Chap04/4.2/#42-2","title":"4.2-2","text":"<p>Write pseudocode for Strassen\u2019s algorithm.</p> <pre><code>Strassen(A,B,C,n)\n    if n ==1\n        c11=c11+a11*b11\n    partition A,B,C into (n/2)*(n/2) submatrixs\n    A11,A12,A21,A22;B11,B12,B21,B22;C11,C12,C21,C22\n    //get S O(n^2)\n    S1=B12-B22\n    S2=A11+A12\n    S3=A21+A22\n    S4=B21-B11\n    S5=A11+A22\n    S6=B11+B22\n    S7=A12-A22\n    S8=B21+B22\n    S9=A11-A21\n    S10=B11+B12\n    //creat P O(n^2)\n    creat P1...7\n    //recursion\n    Strassen(A11,S1,P1,n/2)\n    Strassen(S2,B22,P2,n/2)\n    Strassen(S3,B11,P3,n/2)\n    Strassen(A22,S4,P4,n/2)\n    Strassen(S5,S6,P5,n/2)\n    Strassen(S7,S8,P6,n/2)\n    Strassen(S9,S10,P7,n/2)\n    //conquer O(n^2)\n    C11=C11+P5+P4-P2+P6\n    C12=C12+P1+P2\n    C21=C21+P3+P4\n    C22=C22+P5+P1-P3-P7\n</code></pre>"},{"location":"Chap04/4.2/#42-3","title":"4.2-3","text":"<p>What is the largest $k$ such that if you can multiply $3\\times 3$ matrices using $k$ multiplications (not assuming commutativity of multiplication), then you can multiply $n\\times n$ matrices in $o(n^{\\lg 7})$ time? What is the running time of this algorithm?</p> <p>$$ \\begin{aligned}     T(n)=kT(n/3)+O(1)=o(n^{\\lg 7})\\cr     \\implies T(n)=\\Theta(n^{\\log_{3} k})=o(n^{\\lg 7})\\cr     \\implies \\log_{3} k &lt; \\lg 7\\cr     \\implies k &lt; 21.8\\cr     \\implies \\max(k) = 21\\cr     \\text{running time }= n^{\\log_{3} 21}\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.2/#42-4","title":"4.2-4","text":"<p>V. Pan discovered a way of multiplying $68\\times 68$ matrices using 132,464 multiplications, a way of multiplying $70\\times 70$ matrices using 143,640 multiplications, and a way of multiplying $72\\times 72$ matrices using 155,424 multiplications. Which method yields the best asymptotic running time when used in a divide-and-conquer matrix-multiplication algorithm? How does it compare with Strassen\u2019s algorithm?</p> <p>$n^{\\log_{68}132464}=n^{2.79513};n^{\\log_{70} 143640}=n^{2.79512};n^{\\log_{72} 155424}=n^{2.79515};n^{\\lg 7}=n^{2.80735}$</p> <p>the second method is the best; better than Strassen's algorithm.</p>"},{"location":"Chap04/4.2/#42-5","title":"4.2-5","text":"<p>Show how to multiply the complex numbers $a+bi$ and $c+di$ using only three multiplications of real numbers. The algorithm should take $a,b,c$, and $d$ as input and produce the real component $ac-bd$ and the imaginary component $ad+bc$ separately.</p> <p>$$ \\begin{aligned}     S_1=a(c-d)=ac-ad\\cr     S_2=b(c+d)=bc+bd\\cr     S_3=d(a-b)=ad-bd\\cr     S_1+S_3=ac-bd\\cr     S_2+S_3=ad+bc\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.2/#42-6","title":"4.2-6","text":"<p>Suppose that you have a $\\Theta(n^{\\alpha})$-time algorithm for squaring $n \\times n$ matrices, where $\\alpha \\geq 2$. Show how to use that algorithm to multiply two different $n \\times n$ matrices in $\\Theta(n^{\\alpha})$ time.</p> <p>$$ \\begin{aligned}     A\\cdot B=\\frac{(A+B)^{2}-(A-B)^2}{4} \\end{aligned} $$</p>"},{"location":"Chap04/4.3/","title":"4.3 The substitution method for solving recurrences","text":""},{"location":"Chap04/4.3/#43-1","title":"4.3-1","text":"<p>Use the substitution method to show that each of the following recurrences defined on the reals has the asymptotic solution specified:</p> <p>a. $T(n)=T(n-1)+n$ has solution $T(n) = O(n^2)$</p> <p>b. $T(n)=T(n/2)+\\Theta(1)$ has solution $T(n)=O(\\lg n)$</p> <p>c. $T(n)=2T(n/2)+n$ has solution $T(n)=O(n\\lg n)$.</p> <p>d. $T(n)=2T(n/2+17)+n$ has solution $T(n) = O(n\\lg n)$.</p> <p>e. $T(n)=2T(n/3)+\\Theta(n)$ has solution $T(n)=\\Theta(n)$</p> <p>f. $T(n) = 4T(n/2)+\\Theta(n)$ has solution $T(n)=\\Theta(n^2)$.</p> <p>a.</p> <p>$$ \\begin{aligned}     \\text{ inductive case: } &amp;\\cr     &amp; \\text{assume }T(n')\\leq cn'^2 \\text{ for all } n_0 \\leq n'\\leq n\\cr     T(n)&amp; =T(n-1)+n\\cr     &amp; \\leq c(n-1)^2+n\\cr     &amp; = cn^2+(1-2c)n+c\\cr     &amp; \\leq cn^2\\cr     &amp; \\text{while the last step holds for }c &gt; \\frac{1}{2}\\cr     \\text{base case: } &amp;\\cr     T(n) &amp; \\leq c \\leq cn \\text{ for all }n \\leq n_0 \\text{ is true if and only if choose a large enough } n_0\\cr \\end{aligned} $$</p> <ul> <li>tips: base case and detail like $\\text{ for all } n_0 \\leq n'\\leq n$ will be omitted in this kind of problems for convenience.</li> </ul> <p>b.</p> <p>assume  $T(n) \\leq c\\lg n$</p> <p>$$ \\begin{aligned}     T(n)&amp;=T(n/2)+c'\\cr     &amp;\\leq c\\lg n -c\\lg 2 +c'\\cr     &amp;\\leq c\\lg n\\cr \\end{aligned} $$</p> <p>the last step holds for $c&gt;c'/\\lg 2$.</p> <p>c.</p> <p>assume $T(n)\\leq cn\\lg n$</p> <p>$$ \\begin{aligned}     T(n) &amp; = 2T(n/2)+n\\cr     &amp; \\leq 2c(n/2)\\lg (n/2) +n\\cr     &amp; = cn\\lg n + (1-c\\lg 2)n\\cr     &amp; \\leq cn\\lg n \\end{aligned} $$</p> <p>the last step holds for $c&gt;1/\\lg 2$.</p> <p>d.</p> <p>assume $T(n)\\leq c(n-a)\\lg (n-a)$</p> <p>$$ \\begin{aligned}     T(n)&amp;=2T(n/2+17)+n\\cr     &amp; \\leq 2(c(n/2+17-a))\\lg(n/2-a+17)+n\\cr     &amp; = c(n-2a+34)\\lg(n -a +34)+(1-c\\lg 2)n +c(-2a+34)\\lg 2\\cr     &amp; \\leq c(n-a)\\lg (n-a)\\cr \\end{aligned} $$</p> <p>the last step holds for $n\\geq 34$</p> <p>e. assume $c_{1}n\\leq T(n)\\leq c_{2}n$</p> <p>$$ \\begin{aligned}     T(n) &amp; =2T(n/3)+\\Theta(n)\\cr     &amp; \\geq 2(c_{1}n/3)+c_{3}n\\cr     &amp; =(2c_{1}/3+c_{3})n\\cr     &amp; \\geq c_{1}n\\cr \\end{aligned} $$</p> <p>the last step holds for $c_{1}\\leq 3c_{3}$</p> <p>$c_{3}$ is the lowwer bound constant in $\\Theta(n)$.</p> <p>$$ \\begin{aligned}     T(n) &amp; =2T(n/3)+c_{4}n\\cr     &amp; \\leq (2c_{2}/3+c_{4})n\\cr     &amp; \\leq c_{2}n\\cr \\end{aligned} $$</p> <p>the last step holds for$c_{2}\\geq 3c_{4}$</p> <p>$c_{4}$ is the upper bound constant in $\\Theta(n)$.</p> <p>f.</p> <p>assume $c_{1}n^2 \\leq T(n)\\leq c_{2}n^2-c_{0}n$</p> <p>$$ \\begin{aligned}     T(n) &amp; =4T(n/2)+\\Theta(n)\\cr     &amp; \\geq c_{1}n^2+c_{3}n\\cr     &amp; \\geq c_{1}n^2\\cr     T(n) &amp; =4T(n/2)+\\Theta(n)\\cr     &amp; \\leq c_{2}n^2-2c_{0}n+c_{4}n\\cr     &amp; \\leq c_{2}n^2-c_{0}n\\cr \\end{aligned} $$</p> <p>the last step holds for $c_{0}\\geq c_{4}$</p> <p>$c_{3}$ is the lowwer bound constant in $\\Theta(n)$ and $c_{4}$ is the upper bound constant in $\\Theta(n)$.</p>"},{"location":"Chap04/4.3/#43-2","title":"4.3-2","text":"<p>The solution to the recurrence $T(n) = 4T(n/2) + n$ turns out to be $T(n) = \\Theta(n^2)$.Show that a substitution proof with the assumption $T(n) \\leq cn^2$ fails. Then show how to subtract a lower-order term to make a substitution proof work.</p> <p>assume $T(n)\\leq cn^2$</p> <p>$$ \\begin{aligned}     T(n) &amp; =4T(n/2)+n\\cr     &amp; \\leq cn^2+n\\cr \\end{aligned} $$</p> <p>It can not further imply $T(n)\\leq cn^2+n$</p> <p>assume $T(n)\\leq c_{1}n^2-c_{2}n$</p> <p>$$ \\begin{aligned}     T(n) &amp; =4T(n/2)+n\\cr     &amp; \\leq c_{1}n^2-2c_{2}n+n\\cr     &amp; \\leq c_{1}n^2-c_{2}n\\cr \\end{aligned} $$</p> <p>the last step holds for $c_{2}\\geq 1$</p>"},{"location":"Chap04/4.3/#43-3","title":"4.3-3","text":"<p>The recurrence $T(n)=2T(n-1)+1$ has the solution T(n) = O(2^n). Show that a subtitution proof fails with the assumption $T(n)\\leq c2^n$, where $c &gt; 0$ is constant. Then show how to subtract a lower-order term to make a substitution proof work.</p> <p>assume $T(n)\\leq c2^{n}$</p> <p>$$ \\begin{aligned}     T(n) &amp; =2T(n-1)+1\\cr     &amp; \\leq 2\\cdot c2^{n-1}+1\\cr     &amp; =c2^{n}+1\\cr \\end{aligned} $$</p> <p>It can not further imply $T(n)\\leq c2^{n}$</p> <p>assume $T(n)\\leq c2^{n}-d$</p> <p>$$ \\begin{aligned}     T(n) &amp; =2T(n-1)+1\\cr     &amp; \\leq 2\\cdot c2^{n-1}-2d+1\\cr     &amp; =c2^{n}-d\\cr \\end{aligned} $$</p> <p>the last step holds for $d\\geq 1$</p>"},{"location":"Chap04/4.4/","title":"4.4 The recursion-tree method for solving recurrences","text":""},{"location":"Chap04/4.4/#44-1","title":"4.4-1","text":"<p>For each of the following recurrences, sketch its recursion tree, and guess a good asymptotic upper bound on its solution. Then use the substitution method to verify your answer.</p> <p>a. $T(n)=T(n/2)+n^3$.</p> <p>b. $T(n) = 4T(n/3)+n$.</p> <p>c. $T(n) = 4T(n/2)+n$.</p> <p>d. $T(n) = 3T(n-1)+1$.</p> <p>a.</p> <p></p> <p>guess $T(n)=O(n^3)$,assume $T(n)\\leq cn^3$</p> <p>$$ \\begin{aligned}     T(n) &amp; = T(n/2)+n^3\\cr     &amp; \\leq \\frac{cn^3}{8}+n^3\\cr     &amp; \\leq (1+\\frac{c}{8})n\\cr     &amp; \\leq cn^3\\cr \\end{aligned} $$</p> <p>the last step holds for $c\\geq 7/8$</p> <p>b.</p> <p></p> <p>guess $T(n)=O(n^{\\log_{3} 4})$, assume $T(n)\\leq cn^{\\log_3 4}-dn$</p> <p>$$ \\begin{aligned}     T(n) &amp; =4T(n/3)+n\\cr     &amp; \\leq cn^{\\log_{3} 4} -\\frac{4dn}{3}+ n\\cr     &amp; \\leq cn^{\\log_{3} 4}\\cr \\end{aligned} $$</p> <p>the last step holds for $d\\geq 4/3$</p> <p>c.</p> <p></p> <p>guess $T(n)=O(n^2)$, assume $T(n)\\leq cn^2-dn$</p> <p>$$ \\begin{aligned}     T(n) &amp; =4T(n/2)+n\\cr     &amp; \\leq cn^2-2dn+n\\cr     &amp; \\leq cn^2\\cr \\end{aligned} $$</p> <p>the last step holds for $d\\geq 1/2$</p> <p>d.</p> <p></p> <p>guess $T(n)=O(3^n)$, assume $T(n)\\leq c3^n-d$</p> <p>$$ \\begin{aligned}     T(n) &amp; =3T(n-1)+1\\cr     &amp; \\leq 3\\cdot 3^{n-1}-3d +1\\cr     &amp; \\leq 3^n\\cr \\end{aligned} $$</p> <p>the last step holds for $d\\geq 1/3$</p>"},{"location":"Chap04/4.4/#44-2","title":"4.4-2","text":"<p>Use the substitution method to prove that recurrence (4.15) has the asymptotic lower bound $L(n)=\\Omega(n)$. Conclude that $L(n)=\\Theta(n)$.</p> <p>assume$L(n)\\geq cn$</p> <p>$$ \\begin{aligned}     L(n) &amp; =L(n/3)+L(2n/3)\\cr     &amp; \\geq c(n/3)+c(n2/3)\\cr     &amp; = cn\\cr \\end{aligned} $$</p> <p>since the textbook has proved that $L(n)=O(n)$, and now $L(n)=\\Omega(n)$ is proved, we can conclude that $L(n)=\\Theta(n)$.</p>"},{"location":"Chap04/4.4/#44-3","title":"4.4-3","text":"<p>Use the substitution method to prove that recurrence (4.14) has the solution $T(n) = \\Omega(n\\lg n)$. Conclude  that $T(n)=\\Theta(n\\lg n)$.</p> <p>guess $T(n)\\leq cn\\lg n$</p> <p>$$ \\begin{aligned}     T(n) &amp; =T(n/3)+T(2n/3)+\\Theta(n)\\cr     &amp; \\leq c(n/3)\\lg(n/3)+c(2n/3)\\lg(2n/3)+dn\\cr     &amp; = cn\\lg{n}-\\frac{cn}{3}(3\\lg{3}-2-3d/c)\\cr     &amp; \\leq cn\\lg{n}\\cr \\end{aligned} $$</p> <p>the last step holds for $c\\geq 3d/2$.</p>"},{"location":"Chap04/4.4/#44-4","title":"4.4-4","text":"<p>Use a recursion tree to justify a good guess for the solution to the recurrence $T(n)=T(\\alpha n)+T((1-\\alpha)n)+\\Theta(n)$, where $\\alpha$ is a constant in the range $0 &lt; \\alpha 1$.</p> <p>assume $\\alpha&lt;1/2$, since otherwise let $\\beta=1-\\alpha =$ and solve it for $\\beta$.</p> <p>let $L(n)$ denotes leaves number, then $L(n)=L(\\alpha n)+L((1-\\alpha)n)\\implies L(n)=\\Theta(n)$</p> <p>the smallest depth of leaves is $\\log_{1/\\alpha}n$, while the largest depth is $\\log_{\\alpha}n$, total cost over all nodes at depth $i$, for $i=0,1,\\dots,\\log_{1/\\alpha}n-1$ is $\\Theta(n)$</p> <p>$$ \\begin{aligned}     &amp; \\sum_{i=0}^{\\log_{1/\\alpha}n-1}c_{1}n+d_{1}n\\leq T(n)\\leq \\sum_{i=0}^{\\log_{\\alpha}n-1}c_{2}n+d_{2}n\\cr     \\implies &amp; c_{1}n\\log_{1/\\alpha}n+d_{1}n \\leq T(n) \\leq c_{2}n\\log_{\\alpha}n+d_{2}n\\cr     \\implies &amp; T(n)=\\Theta(n\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.5/","title":"4.5 The master method for solving recurrences","text":""},{"location":"Chap04/4.5/#45-1","title":"4.5-1","text":"<p>Use the master method to give tight asymptotic bounds for the following recurrences.</p> <p>a. $T(n)=2T(n/4)+1$.</p> <p>b. $T(n)=2T(n/4)+\\sqrt{n}$.</p> <p>c. $T(n)=2T(n/4)+\\sqrt{n}\\lg^2 n$.</p> <p>d. $T(n)=2T(n/4)+n$.</p> <p>e. $T(n)=2T(n/4)+n^2$.</p> <p>a.</p> <p>$$ \\begin{aligned}     a=2,b=4,n^{\\log_{b}a}=n^{1/2},f(n)=1=O(n^{\\log_{b}a-1/2})\\cr     T(n)=\\Theta(n^{1/2})\\cr \\end{aligned} $$</p> <p>b.</p> <p>$$ \\begin{aligned}     a=2,b=4,n^{\\log_{b}a}=n^{1/2},f(n)=n^{1/2}=\\Theta(n^{\\log_{b}a})\\cr     T(n)=\\Theta(n^{1/2}\\lg n)\\cr \\end{aligned} $$</p> <p>c.</p> <p>$$ \\begin{aligned}     a=2,b=4,n^{\\log_{b}a}=n^{1/2},f(n)=n^{1/2}\\lg^{2}n=\\Theta(n^{\\log_{b}a}\\lg^{2}n)\\cr     T(n)=\\Theta(n^{1/2}\\lg^{3} n)\\cr \\end{aligned} $$</p> <p>d.</p> <p>$$ \\begin{aligned}     a=2,b=4,n^{\\log_{b}a}=n^{1/2},f(n)=n=\\Omega(n^{\\log_{b}a+1/2})\\cr     \\text{regularity condition: }af(n/b)=n/2\\leq (1/2)n\\cr     T(n)=\\Theta(n)\\cr \\end{aligned} $$</p> <p>e.</p> <p>$$ \\begin{aligned}     a=2,b=4,n^{\\log_{b}a}=n^{1/2},f(n)=n^2=\\Omega(n^{\\log_{b}a+3/2})\\cr     \\text{regularity condition: }af(n/b)=n/8\\leq (1/8)n\\cr     T(n)=\\Theta(n^2)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.5/#45-2","title":"4.5-2","text":"<p>Professor Caesar wants to develop a matrix-multiplication algorithm that is asymptotically faster than Strassen\u2019s algorithm. His algorithm will use the divide-and-conquer method, dividing each matrix into $n/4\\times n/4$ submatrices, and the divide and combine steps together will take $\\Theta(n^2)$ time. Suppose that the professor\u2019s algorithm creates a recursive subproblems of size $n/4$. What is the largest integer value of $a$ for which his algorithm could possibly run asymptotically faster than Strassen\u2019s?</p> <p>$$ \\begin{aligned}     n^{\\log_{b}a}=n^{\\log_{4}a} &lt; n^{\\lg 7}\\cr     \\implies a &lt; 49\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.5/#45-3","title":"4.5-3","text":"<p>Use the master method to show that the solution to the binary-search recurrence $T(n) = T(n/2) + \\Theta(1)$ is $T(n) = \\Theta(\\lg n)$. (See Exercise 2.3-6 for a description of binary search.)</p> <p>$$ \\begin{aligned}     n^{\\log_{b}a}=n^{\\log_{2}1}=1\\cr     f(n)=1=\\Theta(n^{\\log_{b}a}\\lg^{0}n)\\cr     T(n)=\\Theta(\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.5/#45-4","title":"4.5-4","text":"<p>Consider the function $f(n) = \\lg n$. Argue that although $f(n/2) &lt; f(n)$, the regularity condition $af(n/b)\\leq cf(n)$ with $a=1$ and $b=2$ does not hold for any constant $c&lt;1$. Argue further that for any $\\epsilon &gt; 0$, the condition in case 3 that $f(n) = \\Omega(n^{\\log_{b}a+\\epsilon})$ does not hold.</p> <p>$$ \\begin{aligned}     &amp;\\forall c&lt;1\\cr     &amp;f(n/2)\\leq cf(n)\\cr     \\iff &amp; \\lg n-1\\leq c\\lg n\\cr     \\iff &amp; \\lg n\\leq 1/(1-c)\\cr     \\iff &amp; n\\leq 2^{1/(1-c)}\\cr \\end{aligned} $$</p> <p>so the regularity condition does not hold for $n\\geq 2^{1/(1-c)}$</p> <p>$$ \\begin{aligned}     n^{\\log_{b}a}=n^{\\log_{2}1}=1\\cr     \\forall \\epsilon&gt;0\\cr     cn^{\\epsilon}\\leq \\lg n \\text{ does not hold for }n\\to \\infty\\cr \\end{aligned} $$</p> <p>so $f(n) = \\Omega(n^{\\log_{b}a+\\epsilon})$ does not hold.</p>"},{"location":"Chap04/4.5/#45-5","title":"4.5-5","text":"<p>Show that for suitable constants $a,b$, and $\\epsilon$, the function $f(n)=2^{\\lceil\\lg n\\rceil}$ satisfies all the conditions in case 3 of the master theorem except the regularity condition.</p> <p>$$ \\begin{aligned}     a=1.1,b=1.2,n^{\\log_{b}a}=n^{\\log_{1.2}1.1}\\cr     f(n)\\geq 2^{\\lg n}=n\\geq n^{\\log_{1.2}1.1+\\log_{1.2}1.001}\\cr     \\implies \\exist \\epsilon&gt;0 \\text{ such that }f(n)=\\Omega(n^{\\log_{b}a+\\epsilon})\\cr \\end{aligned} $$</p> <p>$f(n)$ satisfies all the conditions in case 3 of the master theorem except the regularity condition.</p> <p>$1.1f(\\frac{1.3\\cdot 2^k}{1.2})=1.1\\cdot 2^{k+1}\\leq cf(1.3\\cdot 2^k)=c\\cdot 2^{k+1}$ dose not hold for any c&lt;1.</p>"},{"location":"Chap04/4.6/","title":"$\\star$ 4.6 Proof of the continuous master theorem","text":""},{"location":"Chap04/4.6/#46-1","title":"4.6-1","text":"<p>Show that $\\sum_{j=0}^{\\lfloor \\log_{b}n\\rfloor}(\\log_{b} n-j)^k=\\Omega(\\log_b^{k+1}n)$.</p> <p>$$ \\begin{aligned}     \\sum_{j=0}^{\\lfloor \\log_{b}n\\rfloor}(\\log_{b} n-j)^k &amp; \\geq \\sum_{j=0}^{\\lfloor \\log_{b}n\\rfloor}(\\lfloor \\log_{b}n\\rfloor n-j)^k\\cr     &amp; = \\sum_{j=0}^{\\lfloor \\log_{b}n\\rfloor}(j)^k\\cr     &amp; \\geq \\sum_{j=0}^{\\log_{b}n-1}(j)^k\\cr     &amp; = \\Theta((\\log_{b}n-1)^{k+1})\\cr     &amp; = \\Omega(\\log_b^{k+1}n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.6/#star-46-2","title":"$\\star$ 4.6-2","text":"<p>Show that case 3 of the master theorem is overstated (which is also why case 3 of Lemma 4.3 does not require that $f(n)=\\Omega(n^{\\log_b a+\\epsilon})$) in the sense that the regularity condition $af(n/b)\\leq cf(n)$ for some constant $c&lt;1$ implies that there exists a constant $\\epsilon &gt; 0$ sunch that $f(n)=\\Omega(n^{\\log_b a+\\epsilon})$.</p> <p>$$ \\begin{aligned}     &amp; af(n/b) \\leq cf(n)\\cr     \\implies &amp; f(n)\\geq \\frac{a}{c}f(n/b)\\cr     \\implies &amp; f(n) \\geq (\\frac{a}{c})^i f(n/{b^i})\\cr     &amp; \\text{let } n=b^i\\cr     \\implies &amp; f(n)\\geq (\\frac{a}{c})^{\\log_{b}n}f(1)\\cr     \\implies &amp; f(n) \\geq n^{\\log_{b}\\frac{a}{c}}\\cr     &amp; \\because c&lt;0,\\therefore \\frac {a}{c}=a+\\epsilon \\text{ for some constant }\\epsilon &gt;0\\cr     \\implies &amp; f(n)=\\Omega(n^{\\log_{b}a+\\epsilon})\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.6/#star-46-3","title":"$\\star$ 4.6-3","text":"<p>For $f(n)=\\Theta(n^{\\log_b a}/\\lg n)$, prove that the summation in equation (4.19) has solution $g(n)=\\Theta(n^{\\log_b{a}}\\lg\\lg n)$. Conclude that a master recurrence $T(n)$ using $f(n)$ as its driving function has solution $T(n) = \\Theta(n^{\\log_b a}\\lg\\lg n)$.</p> <p>$$ \\begin{aligned}     &amp; \\text{let }n=b^i\\cr     g(n) = &amp; \\sum_{j=0}^{i}a^jf(n/b^j)\\cr     = &amp; \\sum_{j=0}^{i}a^jf(b^{i-j})\\cr     = &amp; \\sum_{j=0}^{i}a^j\\Theta(a^{i-j}/(i-j))\\cr     = &amp; \\sum_{j=0}^{i}a^i\\Theta(1/j)\\cr     = &amp; a^i\\Theta(\\lg i)\\cr     = &amp; \\Theta(a^{\\log_{b}n} \\lg\\log_{b}n)\\cr     = &amp; \\Theta(n^{\\log_{b}a} \\lg\\lg n)\\cr     g(bn) = &amp; \\Theta((bn)^{\\log_{b}a} \\lg\\lg bn)=(n^{\\log_{b}a} \\lg\\lg n)\\cr     \\implies g(n)=&amp; (n^{\\log_{b}a} \\lg\\lg n)\\text{ even if }n \\neq b^i\\cr \\end{aligned} $$</p> <p>We can concluded that $T(n)=g(n)+n^{\\log_{b}a}=n^{\\log_{b}a} \\lg\\lg n$.</p>"},{"location":"Chap04/4.7/","title":"$\\star$ 4.7 Akra-Bazzi recurrences","text":""},{"location":"Chap04/4.7/#star-47-1","title":"$\\star$ 4.7-1","text":"<p>Consider an Akra-Bazzi recurrence $T(n)$ on the reals as given in recurrence (4.22), and define $T'(n)$ as</p> <p>$$ \\begin{aligned}   T'(n)=cf(n)+\\sum_{i=1}^k a_iT'(n/b_i), \\end{aligned} $$</p> <p>where $c &gt; 0$ is constant. Prove that whatever the implicit initial conditions for T(n) might be, there exist initial conditions for $T'(n)$ such that $T'(n) = cT(n)$ for all $n&gt;0$. Conclude that we can drop the asymptotics on a driving function in any Akra-Bazzi recurrence without affecting its asymptotic solution.</p> <p>assume $T'(n)=cT(n)$</p> <p>$$ \\begin{aligned}     T'(n) &amp; =cf(n)+\\sum_{i=1}^k a_iT'(n/b_i)\\cr     &amp; =cf(n)+\\sum_{i=1}^k a_icT(n/b_i)\\cr     &amp; =c(f(n)+\\sum_{i=1}^k a_iT(n/b_i))\\cr     &amp; =cT(n)\\cr \\end{aligned} $$</p> <p>$T'(n)=T(n)$ holds only if the base case(initial conditions) that $T'(n)=T(n)$ for all $n&lt;n_{}0$ is true</p>"},{"location":"Chap04/4.7/#47-2","title":"4.7-2","text":"<p>Show that $f(n)=n^2$ satisfies the polynomial-growth condition but that $f(n)=2^n$ does not.</p> <p>$$ \\begin{aligned}     &amp; \\forall \\phi \\forall \\psi:1 \\leq \\psi \\leq \\phi\\cr     &amp; n^2/d\\leq (\\psi n)^2\\leq dn^2\\cr     \\iff &amp; \\max(1/\\psi^{2})\\leq d \\wedge \\max(\\psi^{2}) \\leq d\\cr     \\iff &amp; \\phi^{2}\\leq d\\qquad(1)\\cr     &amp; 2^n/d\\leq 2^{\\psi n}\\leq d2^n\\cr     \\iff &amp; 2^{n-\\psi n}\\leq d\\wedge 2^{\\psi n-n}\\leq d\\qquad (2)\\cr \\end{aligned} $$</p> <p>$d=\\phi^{2}$ satisfies $(1)$, since $2^{(\\psi-1)n}\\to\\infty$ therefore no constant $d$ satisfies $(2)$</p>"},{"location":"Chap04/4.7/#47-3","title":"4.7-3","text":"<p>Let f(n) be a function that satisfies the polynomial-growth condition. Prove that $f(n)$ is asymptotically positive, that is, there exists a constant $n_0\\geq 0$ such that $f(n)\\geq 0$ for all $n\\geq n_0$.</p> <p>$$ \\begin{aligned}     \\exist \\hat{n} \\text{ that }\\forall n &gt; \\hat{n}:\\cr     f(n)/d \\leq f(\\psi n) \\leq df(n)\\cr     \\implies 0\\leq (d^2-1) f(n)\\cr     \\because d&gt;1\\cr     \\therefore f(n)&gt;0\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.7/#star-47-4","title":"$\\star$ 4.7-4","text":"<p>Give an example of a function $f(n)$ that does not satisfy the polynomial-growth condition but for which $f(\\Theta(n)) = \\Theta(f(n))$.</p> <ul> <li>TODO</li> </ul>"},{"location":"Chap04/4.7/#47-5","title":"4.7-5","text":"<p>Use the Akra-Bazzi method to solve the following recurrences.</p> <p>a. $T(n) = T(n/2)+T(n/3)+T(n/6)+n\\lg n$.</p> <p>b. $T(n) = 3T(n/3)+8T(n/4)+n^2/\\lg n$.</p> <p>c. $T(n) = (2/3)T(n/3)+(1/3)T(2n/3)+\\lg n$.</p> <p>d. $T(n) = (1/3)T(n/3) + 1/n$.</p> <p>e. $T(n) = 3T(n/3) + 3T(2n/3)+n^2$.</p> <p>a.</p> <p>$$ \\begin{aligned}     &amp; (\\frac{1}{2})^p+(\\frac{1}{3})^p+(\\frac{1}{6})^p=1\\cr     \\implies &amp; p=1\\cr     T(n) &amp; =\\Theta(n^p(1+\\int_{1}^{n}\\frac{f(x)}{x^{p+1}}dx))\\cr     &amp; = \\Theta(n^1(1+\\int_{1}^{n}\\frac{x\\lg x}{x^{2}}dx))\\cr     &amp; = \\Theta(n^1(1+\\int_{1}^{n}\\frac{\\ln x}{x\\ln 2}dx))\\cr     &amp; = \\Theta(n^1(1+\\left[\\frac{\\ln^{2} x}{2\\ln 2}\\right]_{1}^{n}))\\cr     &amp; = \\Theta(n(1+\\frac{\\ln^{2} n}{2\\ln 2}))\\cr     &amp; =\\Theta(n\\lg^{2} n) \\end{aligned} $$</p> <p>b.</p> <p>$$ \\begin{aligned}     &amp; \\frac{3}{3^p}+\\frac{8}{4^p}=1\\cr     &amp; \\frac{3}{3^{1.5}}+\\frac{8}{4^{1.5}}=3^{-(1/2)}+1\\cr     &amp; \\frac{3}{3^2}+\\frac{8}{4^2}=\\frac{5}{6}\\cr     \\implies &amp; 1.5&lt;p&lt;2\\cr     T(n) &amp; =\\Theta(n^p(1+\\int_{1}^{n}\\frac{f(x)}{x^{p+1}}dx))\\cr     &amp; =\\Theta(n^p(1+\\int_{1}^{n}\\frac{x^2/\\lg x}{x^{p+1}}dx))\\cr     &amp; =\\Theta(n^p(1+\\int_{1}^{n}\\frac{1}{x^{p-1}\\lg x}dx))\\cr     &amp; =\\Omega(n^p(1+\\lg n\\int_{1}^{n}\\frac{1}{x^{p-1}}dx))\\cr     &amp; =\\Omega(n^p(1+n^{2-p}\\lg n ))\\cr     &amp; =\\Omega(n^2/\\lg n)\\cr     T(n) &amp; =O(n^p(1+\\lg 1(\\text{regarded as constant})\\int_{1}^{\\sqrt{n}}\\frac{1}{x^{p-1}}dx+\\lg \\sqrt{n}\\int_{\\sqrt{n}}^{n}\\frac{1}{x^{p-1}}dx))\\cr     &amp; = O(n^p(1+\\Theta(n^{(p-1)/2})+\\Theta(n^{p-1}/\\lg n)))\\cr     &amp; =O(n^2/\\lg n)\\cr \\end{aligned} $$</p> <p>c.</p> <p>$$ \\begin{aligned}     &amp; \\frac{2}{3}(\\frac{1}{3})^p+\\frac{1}{3}(\\frac{2}{3})^p=1\\cr     \\implies &amp; \\frac{2+2^p}{3^{p+1}}=1\\cr     \\implies &amp; p=0\\cr     T(n) &amp; =\\Theta(n^p(1+\\int_{1}^{n}\\frac{f(x)}{x^{p+1}}dx))\\cr     &amp; =\\Theta(1+\\int_{1}^{n}\\frac{\\lg x}{x}dx)\\cr     &amp; =\\Theta(\\lg^{2}n) \\end{aligned} $$</p> <p>d.</p> <p>$$ \\begin{aligned}     &amp; \\frac{1}{3}(\\frac{1}{3})^p=1\\cr     \\implies &amp; p=-1\\cr     T(n) &amp; =\\Theta(n^p(1+\\int_{1}^{n}\\frac{f(x)}{x^{p+1}}dx))\\cr     &amp; =\\Theta(n^{-1}(1+\\int_{1}^{n}\\frac{1}{x}dx))\\cr     &amp; =\\Theta(n^{-1}(1+\\left[lg n\\right]_{1}^{n}))\\cr     &amp; =\\Theta(n^{-1}lg n)\\cr \\end{aligned} $$</p> <p>e. $T(n) = 3T(n/3) + 3T(2n/3)+n^2$.</p> <p>$$ \\begin{aligned}     &amp; 3(\\frac{1}{3})^p + 3(\\frac{2}{3})^p=1\\cr     \\implies &amp; p=2\\cr     T(n) &amp; =\\Theta(n^p(1+\\int_{1}^{n}\\frac{f(x)}{x^{p+1}}dx))\\cr     &amp; =\\Theta(n^{2}(1+\\int_{1}^{n}\\frac{x^2}{x^3}dx))\\cr     &amp; =\\Theta(n^2\\lg n) \\end{aligned} $$</p>"},{"location":"Chap04/4.7/#star-47-6","title":"$\\star$ 4.7-6","text":"<p>Use the Akra-Bazzi method to prove the continuous master theorem.</p> <p>$$ \\begin{aligned}     &amp; T(n)=aT(n/b)+f(n)\\cr     &amp; \\frac{a}{b^p}=1\\cr     \\implies &amp; p=\\log_{b}a\\cr     T(n) &amp; =\\Theta(n^p(1+\\int_{1}^{n}\\frac{f(x)}{x^{p+1}}dx))\\cr     &amp; =\\Theta(n^{\\log_{b}a}(1+\\int_{1}^{n}\\frac{f(x)}{x^{\\log_{b}a+1}}dx))\\cr     &amp; G(n)=\\int_{1}^{n}\\frac{f(x)}{x^{\\log_{b}a+1}}dx\\cr     \\text{case 1:} &amp; \\cr     &amp; f(n)=O(n^{\\log_{b}a-\\epsilon})(\\epsilon&gt;0)\\cr     G(n) &amp; = O(\\int_{1}^{n}\\frac{1}{x^{1+\\epsilon}}dx)\\cr     &amp; = O(x^{-\\epsilon})\\cr     T(n) &amp; =\\Theta(n^{\\log_{b}a}(1+G(n)))\\cr     &amp; = \\Theta(n^{\\log_{b}a})\\cr     \\text{case 2:} &amp; \\cr     &amp; f(n)=\\Theta(n^{\\log_{b}a}\\lg^{k}n)(k\\geq 0)\\cr     G(n) &amp; = \\Theta(\\int_{1}^{n}\\frac{lg^{k}n}{x^{1}}dx)\\cr     &amp; = \\Theta(\\lg^{k+1}n)\\cr     T(n) &amp; =\\Theta(n^{\\log_{b}a}(1+G(n)))\\cr     &amp; = \\Theta(n^{\\log_{b}a}\\lg^{k+1}n)\\cr     \\text{case 3:} &amp; \\cr     T(n) &amp; =\\Omega(f(n))\\cr     &amp; f(n)=\\Omega(n^{\\log_{b}a+\\epsilon})(\\epsilon&gt;0)\\cr     &amp;\\text{if } af(n/b)&lt;cf(n)\\text{ for some }c&lt;1\\cr     G(n) &amp; = O(f(n)\\int_{1}^{n}\\frac{1}{x^{\\log_{b}a+1}}dx)\\cr     &amp; = O(\\frac{f(n)}{x^{\\log_{b}a}})\\cr     T(n) &amp; =\\Theta(n^{\\log_{b}a}(1+G(n)))\\cr     &amp; =O(f(n))\\cr     T(n)&amp; =\\Theta(f(n)) \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-1/","title":"4-1 Recurrence examples","text":"<p>Give asymptotically tight upper and lower bounds for $T(n)$ in each of the following algorithmic recurrences. Justify your answers.</p> <p>a. $T(n) = 2T(n/2)+n^3$.</p> <p>b. $T(n) = T(8n/11)+n$.</p> <p>c. $T(n) = 16T(n/4)+n^2$.</p> <p>d. $T(n)=4T(n/2)+n^2\\lg n$.</p> <p>e. $T(n) = 8T(n/3)+n^2$.</p> <p>f. $T(n)=7T(n/2)+n^2\\lg n$.</p> <p>g. $T(n)=2T(n/4)+\\sqrt{n}$.</p> <p>h. $T(n)=T(n-2)+n^2$.</p>"},{"location":"Chap04/Problems/4-1/#a","title":"a","text":"<p>$$ \\begin{aligned}     n^{\\log_{b}a}=n^{\\log_{2}2}=n\\cr     f(n)=n^3=\\Omega(n^{2+\\log_{b}a})\\cr     2f(n/2)=n^3/4\\leq \\frac{1}{4}f(n)\\cr     T(n)=\\Theta(f(n))=\\Theta(n^3)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-1/#b","title":"b","text":"<p>$$ \\begin{aligned}     n^{\\log_{b}a}=n^{\\log_{11/8}1}=1\\cr     f(n)=n=\\Omega(n^{1+\\log_{b}a})\\cr     f(8n/11)=8n/11\\leq \\frac{8}{11}f(n)\\cr     T(n)=\\Theta(f(n))=\\Theta(n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-1/#c","title":"c","text":"<p>$$ \\begin{aligned}     n^{\\log_{b}{a}}=n^{\\log_{4}{16}}=n^{2}\\cr     f(n)=n^2=\\Theta(n^{\\log_{b}a}\\lg^{0}n)\\cr     T(n)=\\Theta(n^2\\lg^{} n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-1/#d","title":"d","text":"<p>$$ \\begin{aligned}     n^{\\log_{b}{a}}=n^{\\log_{2}{4}}=n^{2}\\cr     f(n)=n^2\\lg^{} n=\\Theta(n^{\\log_{b}a}\\lg^{}n)\\cr     T(n)=\\Theta(n^2\\lg^{2} n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-1/#e","title":"e","text":"<p>$$ \\begin{aligned}     n^{\\log_{b}{a}}=n^{\\log_{3}{8}}\\cr     f(n)=n^2=\\Omega(n^{\\log_{b}a})\\cr     f8(n/3)=\\frac{8n^2}{9}\\leq \\frac{8}{9}f(n)\\cr     T(n)=\\Theta(f(n))=\\Theta(n^2)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-1/#f","title":"f","text":"<p>$$ \\begin{aligned}     n^{\\log_{b}{a}}=n^{\\log_{2}{7}}\\cr     f(n)=n^2\\lg^{} n=O(n^{\\log_{b}a-\\log_2{(7/4)}})\\cr     T(n)=\\Theta(n^{\\log_{2}{7}})\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-1/#g","title":"g","text":"<p>$$ \\begin{aligned}     n^{\\log_{b}{a}}=n^{\\log_{4}{2}}=n^{1/2}\\cr     f(n)=n^{1/2}=\\Theta(n^{\\log_{b}a}\\lg^{0}n)\\cr     T(n)=\\Theta(n^{1/2}\\lg^{} n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-1/#h","title":"h","text":"<p>$$ \\begin{aligned}     T(n)=\\frac{1}{2}\\sum_{i=1}^{n}i^2=\\Theta(n^3)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-2/","title":"4-2 Parameter-passing costs","text":"<p>Throughout this book, we assume that parameter passing during procedure calls takes constant time, even if an N -element array is being passed. This assumption is valid in most systems because a pointer to the array is passed, not the array itself. This problem examines the implications of three parameter-passing strategies:</p> <ol> <li> <p>Arrays are passed by pointer. Time $= \\Theta(1)$.</p> </li> <li> <p>Arrays are passed by copying. Time $=\\Theta(N)$, where $N$ is the size of the array.</p> </li> <li> <p>Arrays are passed by copying only the subrange that might be accessed by the called procedure. Time $=\\Theta(n)$ if the subarray contains $n$ elements.</p> </li> </ol> <p>Consider the following three algorithms:</p> <p>a. The recursive binary-search algorithm for finding a number in a sorted array(see Exercise 2.3-6).</p> <p>b. The MERGE-SORT procedure from Section 2.3.1.</p> <p>c. The MATRIX-MULTIPLY-RECURSIVE procedure from Section 4.1.</p> <p>Give nine recurrences $T_{a1}(N,n),T_{a2}(N,n),\\dots ,T_{c3}(T,n)$ for the worst-case running times of each of the three algorithms above when arrays and matrices are passed using each of the three parameter-passing strategies above. Solve your recurrences, giving tight asymptotic bounds.</p>"},{"location":"Chap04/Problems/4-2/#a","title":"a","text":"<p>a1</p> <p>$$ \\begin{aligned}     T_{a1}(N,n)=T(N,\\frac{n}{2})+\\Theta(1)\\cr     n^{\\log_{b}a}=n^{\\log_{2}1}=1\\cr     f(n)=\\Theta(1)=\\Theta(n^{\\log_{b}a})\\cr     T_{a1}(N,n)=\\Theta(\\lg n)\\cr \\end{aligned} $$</p> <p>a2</p> <p>$$ \\begin{aligned}     T_{a2}(N,n)=T(N,\\frac{n}{2})+\\Theta(N)\\cr     n^{\\log_{b}a}=n^{\\log_{2}1}=1\\cr     T_{a2}(N,n)=\\Theta(N\\lg n)\\cr \\end{aligned} $$</p> <p>a3</p> <p>$$ \\begin{aligned}     T_{a3}(N,n)=T(N,\\frac{n}{2})+\\Theta(n)\\cr     n^{\\log_{b}a}=n^{\\log_{2}1}=1\\cr     f(n)=\\Theta(n)=\\Omega(n^{1+\\log_{b}a})\\cr     af(\\frac{n}{b})=f(\\frac{n}{2})\\leq \\frac{1}{2}f(n)\\cr     T_{a3}(N,n)=\\Theta(f(n))=\\Theta(n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-2/#b","title":"b","text":"<p>b1</p> <p>$$ \\begin{aligned}     T_{b1}(N,n)=2T(N,\\frac{n}{2})+\\Theta(n)\\cr     n^{\\log_{b}a}=n^{\\log_{2}2}=n\\cr     f(n)=\\Theta(n)=\\Theta(n^{\\log_{b}a}\\lg^{0}n)\\cr     T_{b1}(N,n)=\\Theta(n \\lg n)\\cr \\end{aligned} $$</p> <p>b2</p> <p>$$ \\begin{aligned}     T_{b2}(N,n)=2T(N,\\frac{n}{2})+\\Theta(N+n)=2T(N,\\frac{n}{2})+\\Theta(N)\\cr     n^{\\log_{b}a}=n^{\\log_{2}1}=n\\cr     T_{b2}(N,n)=\\Theta(n+N\\lg n)=\\Theta(N\\lg n)\\cr \\end{aligned} $$</p> <p>b3</p> <p>$$ \\begin{aligned}     T_{b3}(N,n)=2T(N,\\frac{n}{2})+\\Theta(n)\\cr     n^{\\log_{b}a}=n^{\\log_{2}2}=n\\cr     f(n)=\\Theta(n)=\\Theta(n^{\\log_{b}a}\\lg^{0}n)\\cr     T_{b3}(N,n)=\\Theta(n \\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-2/#c","title":"c","text":"<p>c1</p> <p>$$ \\begin{aligned}     T_{c1}(N,n)=8T(N,\\frac{n}{2})+\\Theta(1)\\cr     n^{\\log_{b}a}=n^{\\log_{2}8}=n^3\\cr     f(n)=\\Theta(1)=O(n^{\\log_{b}a-3})\\cr     T_{c1}(N,n)=\\Theta(n^3)\\cr \\end{aligned} $$</p> <p>c2</p> <p>$$ \\begin{aligned}     T_{c2}(N,n)=8T(N,\\frac{n}{2})+\\Theta(N)\\cr     n^{\\log_{b}a}=n^{\\log_{2}8}=n^3\\cr     T_{c2}(N,n)=\\Theta(n^3+N\\lg N)\\cr \\end{aligned} $$</p> <p>c3</p> <p>$$ \\begin{aligned}     T_{c3}(N,n)=8T(N,\\frac{n}{2})+\\Theta(n)\\cr     n^{\\log_{b}a}=n^{\\log_{2}8}=n^3\\cr     f(n)=\\Theta(n)=O(n^{\\log_{b}a-2})\\cr     T_{c3}(N,n)=\\Theta(n^3)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-3/","title":"4-3 Solving recurrences with a change of variables","text":"<p>Sometimes, a little algebraic manipulation can make an unknown recurrence similar to one you have seen before. Let\u2019s solve the recurrence</p> <p>$T(n)=2T(\\sqrt{n})+\\Theta(\\lg n)$ (4.25)</p> <p>by using the change-of-variables method.</p> <p>a. Define $m=\\lg n$ and $S(m)=T(2^m)$. Rewrite recurrence (4.25) in terms of m and $S(m)$.</p> <p>b. Solve your recurrence for $S(m)$.</p> <p>c. Use your solution for $S(m)$ to conclude that $T(n)=\\Theta(\\lg n\\lg\\lg n)$.</p> <p>d. Sketch the recursion tree for recurrence (4.25), and use it to explain intuitively why the solution is $T(n)=\\Theta(\\lg n \\lg\\lg n)$.</p> <p>Solve the following recurrences by changing variables:</p> <p>e. $T(n)=2T(\\sqrt{n})+\\Theta(1)$.</p> <p>f. $T(n)=3T(\\sqrt[3]{n})+\\Theta(n)$.</p>"},{"location":"Chap04/Problems/4-3/#a","title":"a","text":"<p>$S(m)=T(2^m)=2T(\\sqrt{2^m})+\\Theta(m)=2T(2^{m/2})+\\Theta(m)=2S(m/2)+\\Theta(m)$</p>"},{"location":"Chap04/Problems/4-3/#b","title":"b","text":"<p>$$ \\begin{aligned}     m^{\\log_{b}a}=m\\cr     f(m)=m=\\Theta(m^{\\log_{b}a})\\cr     S(m)=\\Theta(m\\lg m) \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-3/#c","title":"c","text":"<p>$T(n)=S(\\lg n)=\\Theta(\\lg n\\lg\\lg n)$</p>"},{"location":"Chap04/Problems/4-3/#d","title":"d","text":"<p>depth:$\\lg\\lg n +1$, leaves: $2^{\\lg\\lg n}$</p> <p>The total cost over all nodes at depth $i$,for $i=0,1,\\dots,\\lg\\lg n-1$ is $2^i\\cdot \\frac{1}{2^i}\\Theta(\\lg n)=\\Theta(\\lg n)$</p> <p>$$ \\begin{aligned}     T(n) &amp; =2^{\\lg\\lg n}\\Theta(1)+\\sum_{i=0}^{\\lg\\lg n-1}\\Theta(\\lg n)\\cr     &amp; =\\Theta(\\lg n)+\\lg\\lg n\\Theta(\\lg n)\\cr     &amp; = \\Theta(\\lg n\\lg\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-3/#e","title":"e","text":"<p>depth:$\\lg\\lg n +1$, leaves: $2^{\\lg\\lg n}$</p> <p>The total cost over all nodes at depth $i$,for $i=0,1,\\dots,\\lg\\lg n-1$ is $2^i\\cdot \\Theta(1)=\\Theta(2^i)$</p> <p>$$ \\begin{aligned}     T(n) &amp; =2^{\\lg\\lg n}\\Theta(1)+\\sum_{i=0}^{\\lg\\lg n-1}2^i\\Theta(1)\\cr     &amp; =\\Theta(\\lg n)+(2^{\\lg\\lg n}-1)\\Theta(1)\\cr     &amp; = \\Theta(\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-3/#f","title":"f","text":"<p>$S(m)=T(3^m)=3T(3^{m/3})+\\Theta(3^m)=3S(m/3)+\\Theta(3^m)$.</p> <p>$$ \\begin{aligned}     m^{\\log_{b}a}=m^{\\log_{3}3}=m\\cr     f(m)=3^m=\\Omega(m^{2+\\log_{b}a})\\cr     3f(m/3)=3\\cdot 3^{m/3}\\leq (1/3)f(m)\\cr     S(m)=\\Theta(f(m))=\\Theta(3^m)\\cr     T(n)=S(\\log_{3}n)=\\Theta(n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-4/","title":"4-4 More recurrence examples","text":"<p>Give asymptotically tight upper and lower bounds for T(n) in each of the following recurrences. Justify your answers.</p> <p>a. $T(n)=5T(n/3)+n\\lg n$.</p> <p>b. $T(n)=3T(n/3)+n/\\lg n$.</p> <p>c. $T(n)= 8T(n/2)+n^3\\sqrt{n}$.</p> <p>d. $T(n)=2T(n/2-2)+n/2$.</p> <p>e. $T(n)=2T(n/2)+n/\\lg n$.</p> <p>f. $T(n)=T(n/2)+T(n/4)+T(n/8)+n$.</p> <p>g. $T(n)=T(n-2)+1/n$.</p> <p>h. $T(n)=T(n-1)+\\lg n$.</p> <p>i. $T(n)=T(n-2)+1/\\lg n$.</p> <p>j. $T(n)=\\sqrt{n}T(\\sqrt{n})+n$.</p>"},{"location":"Chap04/Problems/4-4/#a","title":"a","text":"<p>$$ \\begin{aligned}     n^{\\log_{b}{a}} = n^{\\log_{3}{5}}\\cr     f(n) = n\\lg{n} = O(n^{\\log_{b}{a}-\\log_{3}{(5/4)}})\\cr     T(n)=\\Theta({n^{\\log_{b}{a}}})=\\Theta(n^{\\log_{3}{5}})\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-4/#b","title":"b","text":"<p>$$ \\begin{aligned}     &amp; 3(\\frac{1}{3})^p=1\\cr     \\implies &amp; p = 1\\cr     T(n) &amp; =\\Theta(n^p(1+\\int_{1}^{n}\\frac{f(x)}{x^{p+1}}dx))\\cr     &amp; = \\Theta(n^1(1+\\int_{1}^{n}\\frac{x/\\lg x}{x^{2}}dx))\\cr     &amp; = \\Theta(n^1(1+\\int_{1}^{n}\\frac{1}{x^{}\\lg x}dx))\\cr     &amp; = \\Theta(n^1(1+\\left[\\lg\\lg x\\right]_{1}^{n}))\\cr     &amp; = \\Theta (n\\lg\\lg n)\\cr \\end{aligned} $$</p> <ul> <li>Tips: if $f(1)=\\infty$, we will assume $T(1)=f(1)=\\Theta(1)$</li> </ul>"},{"location":"Chap04/Problems/4-4/#c","title":"c","text":"<p>$$ \\begin{aligned}     n^{\\log_{b}{a}} = n^{\\log_{2}{8}}=n^{3}\\cr     f(n) = n^{7/2} = \\Omega(n^{\\log_{b}{a}+1/2})\\cr     af(n/b)=2^{-1/2}(n^{7/2})\\leq 2^{-1/2}f(n)\\cr     T(n)=\\Theta(f(n))=\\Theta(n^{7/2})\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-4/#d","title":"d","text":"<p>$$ \\begin{aligned}     f(n)=n/2 \\text{ satisfies polynomial-growth condition}\\cr     T(n)=2T(n/2)+n/2\\cr     n^{\\log_{b}{a}} = n^{\\log_{2}{2}}=n^{}\\cr     f(n) = n^{}/2 = \\Theta(n^{\\log_{b}{a}}\\lg^{0}n)\\cr     T(n)=\\Theta(n^{\\log_{b}{a}}\\lg n)=\\Theta(n\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-4/#e","title":"e","text":"<p>$$ \\begin{aligned}     &amp; 2(\\frac{1}{2})^p=1\\cr     \\implies &amp; p = 1\\cr     T(n) &amp; =\\Theta(n^p(1+\\int_{1}^{n}\\frac{f(x)}{x^{p+1}}dx))\\cr     &amp; = \\Theta(n^1(1+\\int_{1}^{n}\\frac{x/\\lg x}{x^{2}}dx))\\cr     &amp; = \\Theta(n^1(1+\\int_{1}^{n}\\frac{1}{x^{}\\lg x}dx))\\cr     &amp; = \\Theta(n^1(1+\\left[\\lg\\lg x\\right]_{1}^{n}))\\cr     &amp; = \\Theta(n\\lg\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-4/#f","title":"f","text":"<p>$$ \\begin{aligned}     &amp; (\\frac{1}{2})^{p}+(\\frac{1}{4})^{p}+(\\frac{1}{8})^{p}=1\\cr     &amp; 0.5&lt;p&lt;1\\cr     T(n) &amp; =\\Theta(n^p(1+\\int_{1}^{n}\\frac{f(x)}{x^{p+1}}dx))\\cr     &amp; = \\Theta(n^p(1+\\int_{1}^{n}\\frac{x}{x^{p+1}}dx))\\cr     &amp; = \\Theta(n^p(1+\\int_{1}^{n}\\frac{1}{x^{p}}dx))\\cr     &amp; = \\Theta(n^p(1+\\left[x^{1-p}\\right]_{1}^{n}))\\cr     &amp; = \\Theta(n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-4/#g","title":"g","text":"<p>$$ \\begin{aligned}     T(n) &amp; = \\frac{1}{2}\\sum_{i=1}^{n}(\\frac{1}{i})\\cr     &amp; \\leq \\frac{1}{2}+1/2\\int_{1}^{n-1}\\frac{1}{x}\\cr     &amp; = \\Theta(\\frac{1}{2}+\\frac{1}{2}\\lg (n-1))\\cr     &amp; = \\Theta(\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-4/#h","title":"h","text":"<p>$T(n)=T(n-1)+\\lg n$ $$ \\begin{aligned}     T(n) &amp; = \\sum_{i=1}^{n}\\lg i\\cr     &amp; = \\lg(n!)\\cr     &amp; = \\Theta(n\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-4/#i","title":"i","text":"<p>$$ \\begin{aligned}     T(n) &amp; =1+\\sum_{i=2}^{n}\\frac{1}{\\lg i}\\cr     &amp; = \\Omega(\\frac{n^2}{\\sum_{i=2}^{n}\\lg n})\\cr     &amp; = \\Omega(\\frac{n}{\\lg n})\\cr     T(n) &amp; =O(\\sum_{i=2}^{\\sqrt{n}-1}\\frac{1}{\\lg 2}+\\sum_{i=\\sqrt{n}}^{n}\\frac{1}{\\lg \\sqrt{n}})\\cr     &amp; = O(\\sqrt{n}+\\frac{n}{\\lg n})\\cr     T(n) &amp; = \\Theta(n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-4/#j","title":"j","text":"<p>depth: $\\lg\\lg n+1$, leaves: $n$</p> <p>total cost of over all nodes at depth $i$, for $i=0,1,...,\\lg \\lg n-1$, is n</p> <p>$$ \\begin{aligned}     T(n)=\\Theta(n\\lg\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-5/","title":"4-5 Fibonacci numbers","text":"<p>This problem develops properties of the Fibonacci numbers, which are defined by recurrence (3.31) on page 69. We\u2019ll explore the technique of generating functions to solve the Fibonacci recurrence. Define the generating function (or formal power series) $\\mathcal{F}$ as</p> <p>$$ \\begin{aligned} \\mathcal{F}(z) &amp; = \\sum_{i=0}^{\\infty}F_iz^i\\cr &amp; = 0+z+z^2+2z^3+3z^4+5z^5+8z^6+13z^7+21z^8+\\cdots,\\cr \\end{aligned} $$ where $F_i$ is the $i$th Fibonaccci nunber.</p> <p>a. Show that $\\mathcal{F}(z)=z+z\\mathcal{F}(z)+z^2\\mathcal{F}(z)$.</p> <p>b. Show that</p> <p>$$ \\begin{aligned} \\mathcal{F}(z) &amp; = \\frac{z}{1-z-z^2}\\cr &amp; = \\frac{z}{(1-\\phi z)(1-\\hat\\phi z)}\\cr &amp; = \\frac{1}{\\sqrt{5}}(\\frac{1}{1-\\phi z}-\\frac{1}{1-\\hat\\phi z})\\cr \\end{aligned} $$</p> <p>where $\\phi$ is the golden ratio, and $\\hat{\\phi}$ is its conjugatr(see page 69).</p> <p>c. Show that</p> <p>$$ \\mathcal{F}(z)=\\sum_{i=0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi^{i}-\\hat{\\phi}^i)z^i. $$</p> <p>You may use without proof the generating-function version of equation (A.7) on page 1142, $\\sum_{k=0}^{\\infty} x^k=1/(1-x)$. Because this equation involves a generating function, $x$ is formal variable, not a real-valued variable, so that you don\u2019t have to worry about convergence of the summation or about the requirement in equation (A.7) that $|x|&lt;1$, which doesn\u2019t make sense here.</p> <p>d. Use part (c) to prove that $F_i=\\phi^i/\\sqrt{5}$ for $i&gt;0$, rounded  to the nearest integer.</p> <p>(Hint: Observe that $|\\hat\\phi&lt;1|$.)</p> <p>e Prove that $F_{i+2}\\geq \\phi^{i}$ for $i\\geq 0$.</p>"},{"location":"Chap04/Problems/4-5/#a","title":"a","text":"<p>$$ \\begin{aligned}     z+z\\mathcal{F}(z)+z^2\\mathcal{F}(z) &amp; =z+z\\sum_{i=0}^{\\infty}F_iz^i + z^{2}\\sum_{i=0}^{\\infty}F_iz^i\\cr     &amp; = z+\\sum_{i=1}^{\\infty}F_{i-1}z^i + \\sum_{i=2}^{\\infty}F_{i-2}z^i\\cr     &amp; = z+F_{0}z^1 + \\sum_{i=2}^{\\infty}(F_{i-2}+F_{i-1})z^i\\cr     &amp; = z+0 + \\sum_{i=2}^{\\infty}F_{i}z^i\\cr     &amp; =\\sum_{i=0}^{\\infty}F_{i}z^i\\cr     &amp; =\\mathcal{F}(z)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-5/#b","title":"b","text":"<p>$$ \\begin{aligned}     &amp;\\mathcal{F}(z)=z+z\\mathcal{F}(z)+z^2\\mathcal{F}(z)\\cr     \\implies &amp; \\mathcal{F}(z)  = \\frac{z}{1-z-z^2}\\cr     &amp; \\phi\\hat{\\phi}=-1;\\phi + \\hat{\\phi}=1;\\phi - \\hat{\\phi}=\\sqrt{5}\\cr     \\implies &amp; \\mathcal{F}(z)  = \\frac{z}{1-(\\phi + \\hat{\\phi})z-(-\\phi\\hat{\\phi})z^2}\\cr     \\implies &amp; \\mathcal{F}(z)  = \\frac{z}{(1-\\phi z)(1-\\hat\\phi z)}\\cr     \\implies &amp; \\mathcal{F}(z)  = \\frac{1}{\\sqrt{5}}\\frac{(\\phi - \\hat{\\phi}) z}{(1-\\phi z)(1-\\hat\\phi z)}\\cr     \\implies &amp; \\mathcal{F}(z)  = \\frac{1}{\\sqrt{5}}(\\frac{1}{1-\\phi z}-\\frac{1}{1-\\hat\\phi z})\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-5/#c","title":"c","text":"<p>$$ \\begin{aligned}     \\sum_{i=0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi^{i}-\\hat{\\phi}^i)z^i &amp; = \\sum_{i=0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi z)^i - \\sum_{i=0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\hat{\\phi}z)^i\\cr     &amp; = \\frac{1}{\\sqrt{5}}(\\frac{1}{1-\\phi z}-\\frac{1}{1-\\hat{\\phi} z})\\cr     &amp; = \\mathcal{F}(z)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-5/#d","title":"d","text":"<p>$$ \\begin{aligned}     F_{i} &amp; =\\frac{1}{\\sqrt{5}}(\\phi^{i}-\\hat{\\phi}^i)\\cr     &amp; = \\frac{\\phi^{i}}{\\sqrt{5}}-\\frac{\\hat{\\phi}^i}{\\sqrt{5}}\\cr     &amp; \\because |\\hat{\\phi}^i|&lt;1 \\therefore |\\hat{\\phi}^i/\\sqrt{5}|&lt;0.5\\cr     \\implies &amp; F_{i}=round(\\frac{\\phi^{i}}{\\sqrt{5}})\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-5/#e","title":"e","text":"<p>$$ \\begin{aligned}     F_{2} &amp; =1\\geq\\phi^{0}=1\\cr     F_{3} &amp; =2\\geq\\phi^{1}=\\frac{1+\\sqrt{5}}{2}\\cr     F_{i+2} &amp; = F_{i+1}+F_{i}\\cr     &amp; \\geq \\phi^{i-1}+\\phi^{i-2}\\cr     &amp; =\\phi^{i-2}(\\phi+1)\\cr     &amp; =\\phi^{i-2}(\\phi^2)\\cr     &amp; = \\phi^{i}\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-6/","title":"4-6 Chip testing","text":"<p>Professor Diogenes has $n$ supposedly identical integrated-circuit chips that in principle are capable of testing each other. The professor\u2019s test jig accommodates two chips at a time. When the jig is loaded, each chip tests the other and reports whether it is good or bad. A good chip always reports accurately whether the other chip is good or bad, but the professor cannot trust the answer of a bad chip. Thus, the four possible outcomes of a test are as follows:</p> Chip A says Chip B says Conclusion B is good A is good both are good, or both are bad B is good A is bad at least one is bad B is bad A is good at least one is bad B is bad A is bad at least one is bad <p>a. Show that if at least $n/2$ chips are bad, the professor cannot necessarily determine which chips are good using any strategy based on this kind of pairwise test. Assume that the bad chips can conspire to fool the professor.</p> <p>Now you will design an algorithm to identify which chips are good and which are bad, assuming that more than $n/2$ of the chips are good. First, you will determine how to identify one good chip.</p> <p>b. Show that $\\lfloor n/2 \\rfloor$ pairwise tests are sufficient to reduce the problem to one of nearly half the size. That is, show how to use $\\lfloor n/2 \\rfloor$ pairwise tests to obtain a set with at most $\\lceil n/2 \\rceil$ chips that still has the property that more than half of the chips are good.</p> <p>c. Show how to apply the solution to part (b) recursively to identify one good chip. Give and solve the recurrence that describes the number of tests needed to identify one good chip.</p> <p>You have now determined how to identify one good chip.</p> <p>d. Show how to identify all the good chips with an additional $\\Theta(n)$ pairwise tests.</p>"},{"location":"Chap04/Problems/4-6/#a","title":"a","text":"<p>If the professor want to determine which chip is good, he must get case 1 that A and B both say the other is good, and ensure there is no bad chip left. So he need to use case 2,3,4 to drop bad chips, since bad chips conspire to fool the professor, each time will drop a bad chip together a good one.</p> <p>If bad chips number is more than good chips, the professor cannot necessarily determine which chips are good.</p>"},{"location":"Chap04/Problems/4-6/#b","title":"b","text":"<pre><code>findgoodCchip(chips)\n    if chips num is zero//end is good chips number equal bad chips  \n        return NIL\n    if chips num is one//must be good chips\n        return this one\n    if chips num is odd//good chips is more than bad chips \n        A=left one chip aside \n    else \n        A=NIL\n    divide rest chips into pairs\n    //floor(n/2) operation\n    for each pairs//this for loop ensure good chips number are great than or eqaul bad chips\n        if the report says at least one of them is bad\n            drop both\n        else\n            drop one in the pair//worst-case: halve the size\n    flag=findgoodCchip(chips)\n    if flag = NIL and A!=NIL//A is good chip\n        return A\n    if flag = NIL and A==NIL//no good chip from this call stack\n        return flag\n    if flag\n        return flag\n</code></pre>"},{"location":"Chap04/Problems/4-6/#c","title":"c","text":"<p>already apply the solution recursively in part(b),</p> <p>$$ \\begin{aligned}     T(n)=n/2+T(n/2)\\cr     T(n)=\\Theta(n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-6/#d","title":"d","text":"<p>use the good chip to test other chips$\\Theta(n)$.</p>"},{"location":"Chap04/Problems/4-7/","title":"4-7 Monge arrays","text":"<p>An $m\\times n$ array $A$ of real numbers is a Monge array if for all $i,j,k$, and $l$ such that $1\\leq i &lt;k \\leq m$ and $i \\leq j &lt; l \\leq n$, we have</p> <p>$A[i,j]+A[k,l]\\leq A[i,l]+A[k,j]$.</p> <p>In other words, whenever we pick two rows and two columns of a Monge array and consider the four elements at the intersections of the rows and the columns, the sum of the upper-left and lower-right elements is less than or equal to the sum of the lower-left and upper-right elements. For example, the following array is Monge:</p> <p>$$ \\begin{array}{}     10 &amp; 17 &amp; 13 &amp; 28 &amp; 23\\cr     17 &amp; 22 &amp; 16 &amp; 29 &amp; 23\\cr     24 &amp; 28 &amp; 22 &amp; 34 &amp; 24\\cr     11 &amp; 13 &amp; 6  &amp; 17 &amp; 7\\cr     45 &amp; 44 &amp; 32 &amp; 37 &amp; 23\\cr     36 &amp; 33 &amp; 19 &amp; 21 &amp; 6\\cr     75 &amp; 66 &amp; 51 &amp; 53 &amp; 34\\cr \\end{array} $$</p> <p>a. Prove that an array is Monge if and only if for all $i=1,2,\\dots,m-1$ and $j=1,2,\\dots,n-1$, we have</p> <p>$$ A[i,j]+A[i+1,j+1]\\leq A[i,j+1]+A[i+1,j]. $$</p> <p>(Hint: For the \"if\" part, use induction separately on rows and columns.)</p> <p>b. The following array is not Monge. Change one element in order to make it Monge. (Hint: Use part (a).)</p> <p>$$ \\begin{array}{} 37 &amp; 23 &amp; 22 &amp; 32\\cr 21 &amp; 6 &amp; 7 &amp; 10\\cr 53 &amp; 34 &amp; 30 &amp; 31\\cr 32 &amp; 13 &amp; 9 &amp; 6\\cr 43 &amp; 21 &amp; 15 &amp; 8\\cr \\end{array} $$</p> <p>c. Let $f(i)$ be the index of the column containing the leftmost minimum elementof row $i$. Prove that $f(1)\\leq f(2)\\leq \\cdots \\leq f(m)$ for any $m\\times n$ Monge array.</p> <p>d. Here is a description of a divide-and-conquer algorithm that computes the left most minimum element in each row of an $m\\times n$ Monge array A:</p> <p>Construct a submatrix $A'$ of $A$ consisting of the even-numbered rows of $A$. Recursively determine the leftmost minimum for each row of $A'$. Then compute the leftmost minimum in the odd-numbered rows of $A'$.</p> <p>Explain how to compute the leftmost minimum in the odd-numbered rows of $A$ (given that the leftmost minimum of the even-numbered rows is known) in $O(m+n) time.</p> <p>e. Write the recurrence for the running time of the algorithm in part (d). Show that its solution is $O(m+n\\log m)$.</p>"},{"location":"Chap04/Problems/4-7/#a","title":"a","text":"<p>$$ \\begin{aligned}     \\text{assume }A[i,j]+A[i+k,j+l]\\leq A[i,j+l]+A[i+k,j]\\cr     A[i+k,j]+A[(i+1)+k,j+l] \\leq A[i+k,j+l]+A[i+k+1,j]\\cr     \\implies A[i,j]+A[i+k+1,j+l]\\leq A[i,j+l]+A[i+k+1,j]\\cr     A[i,j+l]+A[i+k,j+l+1]\\leq A[i,j+l+1]+A[i+k,j+1]\\cr     \\implies A[i,j]+A[i+k,(j+1)+l]\\leq A[i,j+l+1]+A[i+k,j]\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-7/#b","title":"b","text":"<p>$$ \\begin{array}{} 37 &amp; 23 &amp; 22 &amp; 32\\cr 21 &amp; 6 &amp; (5) &amp; 10\\cr 53 &amp; 34 &amp; 30 &amp; 31\\cr 32 &amp; 13 &amp; 9 &amp; 6\\cr 43 &amp; 21 &amp; 15 &amp; 8\\cr \\end{array} $$</p>"},{"location":"Chap04/Problems/4-7/#c","title":"c","text":"<p>$$ \\begin{aligned}     i \\lt k\\cr     \\text{since } A[i,f(i)] = \\min(A[i,x]),A[k,f(k)] = \\min(A[k,x])\\cr     A[i,f(i)] + A[k,f(k)] \\leq A[i,f(k)] + A[k,f(i)]\\cr     \\text{assume }f(i) \\gt f(k)\\cr     \\implies A[i,f(k)] + A[k,f(i)] \\leq A[i,f(i)] + A[k,f(k)]\\cr     \\implies A[i,f(k)] + A[k,f(i)] = A[i,f(i)] + A[k,f(k)]\\cr     A[i,f(i)] \\leq A[i,f(k)]\\cr     A[k,f(k)] \\leq A[k,(f(i))]\\cr     \\implies A[i,f(i)] = A[i,f(k)]\\cr     \\implies A[k,f(k)]= A[k,(f(i))]\\cr     \\implies f(i)\\leq f(k)\\cr     \\text{Contradictory to assumption}\\cr     \\implies f(i)\\leq f(k)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/Problems/4-7/#d","title":"d","text":"<p>since $f(2k) \\leq f(2k+1)\\leq f(2k+2)$</p> <p>in each odd row, we only need to compare $f(2k+2)-f(2k)+1$ elements. totally need to deal with $O(m)+O(n)=O(m+n)$ over all odd rows.</p>"},{"location":"Chap04/Problems/4-7/#e","title":"e","text":"<p>$$ \\begin{aligned}     T(m,n) &amp; = T(m/2,n)+O(m+n)\\cr     T(m,n) &amp; = \\sum_{i=0}^{\\lg m}O(m/2^i+n)\\cr     &amp; = O(n\\lg m)+\\sum_{i=1}^{\\lg m}O(m/2^i)\\cr     &amp; = O(n\\lg m)+O(m)\\cr     &amp; = O(m+n\\lg m)\\cr \\end{aligned} $$</p>"},{"location":"Chap05/5.1/","title":"5.1 The hiring problem","text":""},{"location":"Chap05/5.1/#51-1","title":"5.1-1","text":"<p>Show that the assumption that you are always able to determine which candidate is best, in line 4 of procedure HIRE-ASSISTANT, implies that you know a total order on the ranks of the candidates.</p> <p>In mathematics, a total order or linear order is a partial order in which any two elements are comparable.That is, a total order is a binary $\\geq$ relation on some set, which satisfies reflexive,transitive,antisymmetric and strongly connected relation.</p> <ul> <li>reflexive: since the procedure does not involve a candidate compares with himself, we can just assume that the candidate $i\\geq i$</li> <li>transitive: The assumption that line 4 can determine which is best implies that the two candidates $i,best$, if $i\\geq best$, then $i\\geq j,j=0,1,...,i-1$, otherwise we can not ensure $i$ is the best.</li> <li>antisymmetric: $i\\geq best\\implies best\\ngeq i$</li> <li>strongly connected: any two candidates can be comparable</li> </ul>"},{"location":"Chap05/5.1/#star-51-2","title":"$\\star$ 5.1-2","text":"<p>Describe an implementation of the procedure RANDOM$(a,b)$ that makes calls only to RANDOM$(0,1)$. What is the expected running time of your procedure, as a function of a and b?</p> <pre><code>RANDOM(a, b)\n    range = b - a\n    if range == 0\n        return a + 0\n    bits = floor(log(2, range))+1\n    result = 0\n    for i = 1 to bits\n        r = RANDOM(0, 1)\n        result = result &lt;&lt; 1 + r\n    if result &gt; range\n        return RANDOM(a, b)\n    else return a + result\n</code></pre> <p>Each calling of RAMDOM(a,b) takes $\\Theta(\\lfloor\\lg(b-a)\\rfloor +1)=\\Theta(\\lg(b-a))$, the expected times of calling RAMDOM(a,b) is $\\frac{2^{\\lfloor\\lg(b-a)\\rfloor +1}}{(b-a)}=\\Theta(1)$, expected running time of the procedure is $\\Theta(\\lg(b-a))$.</p>"},{"location":"Chap05/5.1/#star-51-3","title":"$\\star$ 5.1-3","text":"<p>You wish to implement a program that outputs 0 with probability $1/2$ and 1 with probability $1/2$. At your disposal is a procedure BIASED-RANDOM that outputs either 0 or 1, but it outputs 1 with some probability $p$ and 0 with probability $1-p$, where $0 &lt; p &lt; 1$. You do not know what $p$ is. Give an algorithm that uses BIASED-RANDOM as a subroutine, and returns an unbiased answer, returning 0 with probability $1/2$ and 1 with probability $1/2$. What is the expected running time of your algorithm as a function of $p$?</p> <pre><code>UNBIASED-RAMDON()\n    a=BIASED-RANDOM()\n    b=BIASED-RANDOM()\n    if a==0 and b==1\n        return 1\n    if a==1 and b==0\n        return 0\n    return UNBIASED-RAMDON()\n</code></pre> <p>Each calling of UNBIASED-RAMDON() take $\\Theta(1)$, the expected times of calling UNBIASED-RAMDON() is $\\frac{1}{2p(1-p)}$, the expected running time of this algorithm is $\\Theta(\\frac{1}{2p(1-p)})$.</p>"},{"location":"Chap05/5.2/","title":"5.2 Indicator random variables","text":""},{"location":"Chap05/5.2/#52-1","title":"5.2-1","text":"<p>In HIRE-ASSISTANT, assuming that the candidates are presented in a random order, what is the probability that you hire exactly one time? What is the probability that you hire exactly $n$ times?</p> <p>the probability that you hire exactly one time is $\\frac{(n-1)!}{n!}=\\frac{1}{n}$</p> <p>the probability that you hire exactly $n$ times is $\\frac{1}{n!}=\\frac{1}{n}$</p>"},{"location":"Chap05/5.2/#52-2","title":"5.2-2","text":"<p>In HIRE-ASSISTANT, assuming that the candidates are presented in a random order, what is the probability that you hire exactly twice?</p> <p>Let $B_{i}$ be the event that the best candidate is the $i$th candidate be interviewed. Then $Pr(B_{i})=\\frac{1}{n}$ for $i=1,2,...,n$.</p> <p>The probability that the first candidate is the best of the first $i-1$ candidates is $\\frac{1}{i-1}$</p> <p>$$ \\begin{aligned}     P &amp; = \\sum_{i=2}^{n}\\frac{1}{n}\\cdot \\frac{1}{i-1}\\cr     &amp; =\\sum_{i=1}^{n-1}\\frac{1}{n}\\cdot \\frac{1}{i}\\cr     &amp; =\\frac{\\lg(n)+O(1)}{n} \\end{aligned} $$</p>"},{"location":"Chap05/5.2/#52-3","title":"5.2-3","text":"<p>Use indicator random variables to compute the expected value of the sum of $n$ dice.</p> <p>for a dice: $X_{j}=i$ if the result of dice is $i$, for $i=1,2,3,4,5,6$.</p> <p>$S=\\sum_{j=1}^{n}X_{j}$</p> <p>$$ \\begin{aligned}     E[S] &amp; = E\\left[\\sum_{j=1}^{n}X_{j}\\right]\\cr     &amp; = \\sum_{j=1}^{n} E[X_{j}]\\cr     &amp; = \\sum_{j=1}^{n}\\sum_{i=1}^{6}i\\cdot PR(X_{j}=i)\\cr     &amp; = \\sum_{j=1}^{n}\\sum_{i=1}^{6}i\\cdot \\frac{1}{6}\\cr     &amp; = \\frac{7n}{2}\\cr \\end{aligned} $$</p>"},{"location":"Chap05/5.2/#52-4","title":"5.2-4","text":"<p>This exercise asks you to (partly) verify that linearity of expectation holds even if the random variables are not independent. Consider two 6-sided dice that are rolled independently. What is the expected value of the sum? Now consider the case where the first die is rolled normally and then the second die is set equal to the value shown on the first die. What is the expected value of the sum? Now consider the case where the first die is rolled normally and the second die is set equal to 7 minus the value of the first die. What is the expected value of the sum?</p> <ol> <li>$E[sum] = E[X_{1}+X_{2}]=E[X_{1}]+E[X_{2}]=7/2+7/2=7$</li> <li>$E[sum] = E[2X_{1}]=2E[X_{1}]=7$</li> <li>$E[sum] = E[X_{1}+7-X_{1}]=E[7]=7$</li> </ol>"},{"location":"Chap05/5.2/#52-5","title":"5.2-5","text":"<p>Use indicator random variables to solve the following problem, which is known as the hat-check problem. Each of n customers gives a hat to a hat-check person at a restaurant. The hat-check person gives the hats back to the customers in a random order. What is the expected number of customers who get back their own hat?</p> <p>$$ \\begin{aligned}    X_{i} &amp; =    \\begin{cases}        1 &amp; \\text{the ith customers get back his own hat}\\cr        0 &amp; \\text{else}\\cr    \\end{cases}\\cr E[S] &amp; = E\\left[\\sum_{i=1}^{n}X_{i}\\right]\\cr &amp; = \\sum_{i=1}^{n}E[X_{i}]\\cr &amp; = \\sum_{i=1}^{n}\\frac{1}{n}\\cr &amp; = 1\\cr \\end{aligned} $$</p>"},{"location":"Chap05/5.2/#52-6","title":"5.2-6","text":"<p>Let $A[1:n]$ be an array of n distinct numbers. If $iA[j]$, then the pair $(i,j)$ is called an inversion of A.  (See Problem 2-4 on page 47 for more on inversions.) Suppose that the elements of $A$ form a uniform random permutation of $&lt;1,2,...,n&gt;$. Use indicator random variables to compute the expected number of inversions. <p>$$\\begin{aligned}     X_{i,j} &amp; =     \\begin{cases}         1 &amp; \\text{if } (i,j) \\text{ is an inversion}\\cr         0 &amp; \\text{else}\\cr     \\end{cases}\\cr     E[S] &amp; = E\\left[\\sum_{1\\leq 1 \\leq j\\leq n}X_{i,j}\\right]\\cr     &amp; = \\sum_{1\\leq 1 \\leq j\\leq n}E[X_{i,j}]\\cr     &amp; = \\sum_{1\\leq 1 \\leq j\\leq n}\\frac{1}{2}\\cr     &amp; = \\frac{n(n-1)}{4}\\cr \\end{aligned} $$</p>"},{"location":"Chap05/5.3/","title":"5.3 Randomized algorithms","text":""},{"location":"Chap05/5.3/#53-1","title":"5.3-1","text":"<p>Professor Marceau objects to the loop invariant used in the proof of Lemma 5.4. He questions whether it holds prior to the first iteration. He reasons that we could just as easily declare that an empty subarray contains no 0-permutations. Therefore, the probability that an empty subarray contains a 0-permutation should be 0, thus invalidating the loop invariant prior to the first iteration. Rewrite the procedure RANDOMLY-PERMUTE so that its associated loop invariant applies to a nonempty subarray prior to the first iteration, and modify the proof of Lemma 5.4 for your procedure.</p> <pre><code>RANDOMLY-PERMUTE-MODIFIED(A,n)\n    swap(A[1], A[RANDOM(1, n)])\n    for i = 2 to n\n        swap(A[i], A[RANDOM(i, n)])\n</code></pre> <p>Modify the proof of Lemma by starting with $i=2$ instead of $i=1$. This resolves the the issue of 0-permutations.</p>"},{"location":"Chap05/5.3/#53-2","title":"5.3-2","text":"<p>Professor Kelp decides to write a procedure that produces at random any permutation except the identity permutation, in which every element ends up where it started. He proposes the procedure PERMUTE-WITHOUT-IDENTITY. Does this procedure do what Professor Kelp intends?</p> <pre><code>PERMUTE-WITHOUT-IDENTITY(A,n)\n  for i D 1 to n - 1\n      swap A[i] with A[RANDOM(i+1,n)]\n</code></pre> <p>No, some non-identity permutation like $$ can not be produced."},{"location":"Chap05/5.3/#53-3","title":"5.3-3","text":"<p>Consider the PERMUTE-WITH-ALL procedure on the facing page, which instead of swapping element $A[i]$ with a random element from the subarray $A[i:n]$, swaps it with a random element from anywhere in the array. Does PERMUTE-WITH-ALL produce a uniform random permutation? Why or why not?</p> <pre><code>PERMUTE-WITH-ALL(A,n)\n  for i = 1 to n\n      swap A[i] with A[RAMDOM(1,n)]\n</code></pre> <p>There are $n^{n}$ possible results produced by this procedure with same probability, but there are $n!$ distinct permutation. if $n^{n}\\equiv n!$ (mod 3) does not hold, n! permutation can not have equal probability. For instance, if $n=3$, there are 27 results, but there are 6 permutation, they can not have eqaul probability.</p>"},{"location":"Chap05/5.3/#53-4","title":"5.3-4","text":"<p>Professor Knievel suggests the procedure PERMUTE-BY-CYCLE to generate a uniform random permutation. Show that each element $A[i]$ has a $1/n$ probability of winding up in any particular position in $B$. Then show that Professor Knievel is mistaken by showing that the resulting permutation is not uniformly random.</p> <pre><code>PERMUTE-BY-CYCLE(A,n)\nlet B[1:n] ne a new array\noffset = RAMDOM(1,n)\nfor i = 1 to n\n  dest = i + offset\n  if dest &gt; n\n      dest = dest - n\n  B[dest] = A[i]\nreturn B\n</code></pre> <p>$A[i]$ winds up in $B[dest]$, $dest=(i+offset)\\% n$. For a position $j$, the probability that $j=dest$ is the probability that $j=(i+RAMDOM(1,n))\\% n$, which is 1/n.</p> <p>The resulting permutation is not uniformly since the procedure can not produce peimutation like $$."},{"location":"Chap05/5.3/#53-5","title":"5.3-5","text":"<p>Professor Gallup wants to create a random sample of the set {1,2,3,...,n}, that is, an $m$-element subset S, where $0 \\leq m \\leq n$, such that each $m$-subset is equally likely to be created. One way is to set $A[i] = i$, for $i = 1,2,3...,n$, call RANDOMLY-PERMUTE(A), and then take just the first m array elements. This method makes n calls to the RANDOM procedure. In Professor Gallup\u2019s application, n is much larger than m, and so the professor wants to create a random sample with fewer calls to RANDOM.</p> <pre><code>RANDOM-SAMPLE(m,n)\nS = \u00d8\nfor k = n - m + 1 to n\n  i = RANDOM(1,k)\n  if i \u2208 S\n      S = S \u222a {k}\n  else\n      S = S \u222a {i}\nreturn S\n</code></pre> <p>Show that the procedure RANDOM-SAMPLE on the previous page returns a random $m$-subset S of {1,2,3,...,n}, in which each $m$-subset is equally likely, while making only $m$ calls to RANDOM.</p> <p>Loop invariant: At the start of $i$th $(k=n-m+i)$ iteration, each $(i-1)$-subset of the set ${1,2,...,n-m+i-1}$ is equally likely.</p> <p>Initialization: $0$-subset, that is, $\\emptyset$, is equally likely.</p> <p>Maintenance: Since each $(i-1)$-subset is eqaully likely, that is, each one's probability is $1/ {{n-m+i-a}\\choose{i-1}}$. After the $i$th iteration the probability of each $i$-subset with $n-m+i(k)$ is $\\frac{1}{{n-m+i-1}\\choose{i-1}}\\cdot \\frac{i}{n-m+i}=1/{{n-m+i}\\choose{i}}$. Since each of others must have same probability, the sum of the probability of all subset is 1, and there are $n-m+i\\choose i$ subset, the probability of each of others must be $n-m+i\\choose i$.</p> <p>Termination: when $k = n(i=m)$, the loop terminates. The probability of each subset is $n\\choose m$, equally likely.</p> <p>Obviouly, all $m$-subset is possible, and according to the loop invariant, we can prove the procedure returns a random $m$-subset S.</p>"},{"location":"Chap05/5.4/","title":"5.4 Probabilistic analysis and further uses of indicator random variables","text":""},{"location":"Chap05/5.4/#54-1","title":"5.4.-1","text":"<p>How many people must there be in a room before the probability that someone has the same birthday as you do is at least $1/2$? How many people must there be before the probability that at least two people have a birthday on July 4 is greater than $1/2$?</p> <p>For the first question</p> <p>$$ \\begin{aligned}     &amp; 1-(\\frac{n-1}{n})^k\\geq \\frac{1}{2}\\cr     \\implies &amp; (\\frac{n-1}{n})^k\\leq \\frac{1}{2}\\cr     \\implies &amp; k\\lg(\\frac{n-1}{n})\\leq -1\\cr     \\implies &amp; k\\geq \\frac{-1}{\\lg(364/365)}\\approx 252.652\\cr     \\implies &amp; k\\geq 253\\cr \\end{aligned} $$</p> <p>For the seacond question</p> <p>$$ \\begin{aligned}     P( X \\geq 2 ) &amp; = 1 - P( X = 1 ) - P( X = 0 )\\cr     &amp; = 1 - \\frac{k}{n}(\\frac{n-1}{n})^{k-1} - (\\frac{n-1}{n})^k\\cr     &amp; = 1 - (\\frac{n+k-1}{n})(\\frac{n-1}{n})^{k-1}\\cr     &amp; \\geq \\frac{1}{2}\\cr     \\implies &amp; \\frac{n+k-1}{n}(\\frac{n-1}{n})^{k-1}  \\leq \\frac{1}{2}\\cr     \\implies &amp;  \\lg(\\frac{n+k-1}{n}) + (k-1)\\lg(\\frac{n-1}{n})  \\leq -1\\cr \\end{aligned} $$</p> <p>use the c++ program below, we can find the smallest $k$ is 613.</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;cmath&gt; // For std::log\n\nint findSmallestK(int n) {\n    int k = 1;\n    double log_half = std::log2(0.5);\n    double log_n_minus_1_over_n = std::log2((n - 1) / static_cast&lt;double&gt;(n));\n\n    while (true) {\n        double lhs = std::log2((n + k - 1) / static_cast&lt;double&gt;(n)) + (k - 1) * log_n_minus_1_over_n;\n        if (lhs &lt;= log_half) {\n            return k;\n        }\n        ++k;\n    }\n}\n\nint main() {\n    int n;\n    std::cout &lt;&lt; \"Enter n: \";\n    std::cin &gt;&gt; n;\n\n    int k = findSmallestK(n);\n    std::cout &lt;&lt; \"The smallest k is: \" &lt;&lt; k &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"Chap05/5.4/#54-2","title":"5.4-2","text":"<p>How many people must there be in a room before the probability that two people have the same birthday is at least $0.99$? For that many people, what is the expected number of pairs of people who have the same birthday?</p> <p>For the first question:</p> <p>$$ \\begin{aligned}     1 - \\frac{P(n,k)}{n^k}\\geq 0.99\\cr     1 - \\frac{n!}{n^{k}(n-k)!}\\geq 0.99\\cr     \\implies \\frac{n!}{n^{k}(n-k)!} \\leq 0.01\\cr \\end{aligned} $$</p> <p>use the c++ program below, we can find the smallest $k$ is 57.</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;cmath&gt; // For std::log\n\nint findSmallestK(int n)\n{\n    // Precompute log2(0.01) for comparison\n    double log_threshold = std::log2(0.01);\n\n\n    int k = 1;\n    // Compute log2(n!/(n-k)!)\n    double log_n_minus_k_factorial = 0;\n    // compute log2(n^n) = n * log2(n)\n    double log_n_power_n = 0;\n\n    while (k &lt;= n)\n    {\n        log_n_minus_k_factorial += std::log2(n-k+1);\n        log_n_power_n  += std::log2(n);\n        // Calculate the left-hand side of the inequality\n        double lhs = log_n_minus_k_factorial - log_n_power_n;\n\n        // Check if the inequality is satisfied\n        if (lhs &lt;= log_threshold)\n        {\n            return k;\n        }\n        ++k;\n    }\n\n    // If no such k is found, return -1 or an appropriate error value\n    return -1;\n}\n\nint main()\n{\n    int n;\n    std::cout &lt;&lt; \"Enter n: \";\n    std::cin &gt;&gt; n;\n\n    int k = findSmallestK(n);\n    if (k != -1)\n    {\n        std::cout &lt;&lt; \"The smallest k is: \" &lt;&lt; k &lt;&lt; std::endl;\n    }\n    else\n    {\n        std::cout &lt;&lt; \"No valid k found for the given n.\" &lt;&lt; std::endl;\n    }\n\n    return 0;\n}\n</code></pre> <p>For the second question</p> <p>Each two person has probability of $1/365$ to be a pair. There are $57\\choose 2$ combination.</p> <p>$$ \\begin{aligned}     E[pairs] &amp; = \\frac{57\\choose 2}{365}\\cr     &amp; = \\frac{5756}{3652}\\cr     &amp; \\approx 4.37\\cr \\end{aligned} $$</p> <p>Or you can use indicator random variables,although I do not like it.</p> <p>$$ \\begin{aligned}     X_{i,j} &amp; =     \\begin{cases}         1 &amp; \\text{i, j has same birthday}\\cr         0 &amp; \\text{else}\\cr     \\end{cases}\\cr     E[pairs] &amp; = E\\left[\\sum_{1\\leq i \\leq j \\leq 57}X_{i,j} \\right]\\cr     &amp; = \\sum_{1\\leq i \\leq j \\leq 57} E[X_{i,j}]\\cr     &amp; = {57 \\choose 1}\\cdot 1 \\cdot \\frac{1}{365}\\cr     &amp; \\approx 4.37\\cr \\end{aligned} $$</p>"},{"location":"Chap05/5.4/#54-3","title":"5.4-3","text":"<p>You toss balls into $b$ bins until some bin contains two balls. Each toss is independent, and each ball is equally likely to end up in any bin. What is the expected number of ball tosses?</p> <p>Let $B$ denote the number of ball tosses</p> <p>$$ \\begin{aligned}     Pr(B=k)=Pr(B\\geq k)\\cdot Pr(B=k|B\\geq k)\\cr     Pr(B\\geq k)=\\frac{P(b,k-1)}{b^{k-1}}\\cr     Pr(B=k|B\\geq k)=\\frac{k-1}{b}\\cr     E[B]=\\sum_{k=2}^{b+1}\\frac{P(b,k-1)}{b^{k-1}}\\cdot \\frac{k-1}{b}\\cdot k\\cr     \\because Pr(B\\geq k) = Pr(B=k,k+1,...,b+1)\\cr     \\therefore \\sum_{k=t}^{b+1}\\frac{P(b,k-1)}{b^{k-1}}\\cdot \\frac{k-1}{b}=\\frac{P(b,k-1)}{b^{k-1}}\\cr     \\therefore E[B] = 1 + \\sum_{k=1}^{b}\\frac{P(b,k)}{b^{k}}\\cr     \\sum_{k=1}^{b}\\frac{P(b,k)}{b^{k}}\\approx \\sqrt{\\frac{\\pi b}{2}}-\\frac{1}{3} \\end{aligned} $$</p>"},{"location":"Chap05/5.4/#star-54-4","title":"$\\star$ 5.4-4","text":"<p>For the analysis of the birthday paradox, is it important that the birthdays be mutually independent, or is pairwise independence sufficient? Justify your answer.</p> <p>Pairwise independence is sufficient, since only if the probability that each pair fall on same birthday is $1/n$, we can finish all the analysis after (5.7).</p>"},{"location":"Chap05/5.4/#star-54-5","title":"$\\star$ 5.4-5","text":"<p>How many people should be invited to a party in order to make it likely that there are three people with the same birthday?</p> <p>The question is not clear, I guess it means: If you invite people one by one until there are three people with the same birthday, what's the expected value of the people number?</p> <p>Use trail and error we know the answer is 88</p> <p>An approximate method is:</p> <p>Let $Pr(i,j,k)$ be the probability that $i,j,k$ have same birthday.</p> <p>$Pr(i,j,k)=\\frac{1}{n^2} \\quad1\\leq i\\leq j \\leq k \\leq p$</p> <p>Let $N$  be the triplets number.</p> <p>$$ \\begin{aligned}     E[N] &amp; = {p\\choose 3}Pr(i,j,k)\\cr     &amp; = \\frac{p!}{3!(p-3)!n^2}\\cr     &amp; = \\frac{p(p-1)(p-2)}{6n^2}\\cr     &amp; \\geq 1\\cr     \\implies p &amp; \\geq 94\\cr \\end{aligned} $$</p> <p>Or if we want to find the smallest number of people that they have at least 50% chance to have $W$ triplet to have same birthday.</p> <p>if a year has $m$ days, and there are $n$ people.</p> <p>this artical tell us the probability is:</p> <p>$$ \\begin{aligned}     P(W\\geq 1) &amp; = 1 -\\sum_{i=0}^{[n/2]}\\frac{m!n!}{i!(n-2i)!(m-n+i)!2^im^n} \\end{aligned} $$</p> <p>if $m=365, P(W\\geq 1)=0.511$ for $n=88$, $P(W\\geq 1)=0.499$ for $n=87$</p>"},{"location":"Chap05/5.4/#star-54-6","title":"$\\star$ 5.4-6","text":"<p>What is the probability that a $k$-string (defined on page 1179) over a set of size $n$ forms a $k$-permutation? How does this question relate to the birthday paradox?</p> <p>$$ \\begin{aligned}     Pr(k\\text{-perm in k-string}) &amp; = \\frac{P(n,k)}{n^k}\\cr     &amp; = \\frac{n!}{(n-k)!n^k}\\cr \\end{aligned} $$</p> <p>It is the complementary event to birthday paradox, that is, the probability that $k$ people have distinct birthdays.</p>"},{"location":"Chap05/5.4/#star-54-7","title":"$\\star$ 5.4-7","text":"<p>You toss $n$ balls into $n$ bins, where each toss is independent and the ball is equally likely to end up in any bin. What is the expected number of empty bins? What is the expected number of bins with exactly one ball?</p> <p>Let $E$ be the event that a bin is empty, let $B$ be the number of empty bins, let $O$ be the event that a bin has exactly one ball, let $N$ be the number of bins with exactly one ball.</p> <p>$$ \\begin{aligned}     Pr(E) &amp; = (\\frac{n-1}{n})^{n}\\cr     E[B] &amp; = n(\\frac{n-1}{n})^{n}\\cr     Pr(O) &amp; = \\frac{1}{n} \\cdot (\\frac{n-1}{n})^{n-1} \\cdot {n\\choose 1}\\cr     &amp; = (\\frac{n-1}{n})^{n-1}\\cr     E[N] &amp; = nPr(O)\\cr     &amp; = n(\\frac{n-1}{n})^{n-1}\\cr \\end{aligned} $$</p>"},{"location":"Chap05/5.4/#star-54-8","title":"$\\star$ 5.4-8","text":"<p>Sharpen the lower bound on streak length by showing that in n flips of a fair coin, the probability is at least $1-1/n$ that a streak of length $\\lg n-2\\lg\\lg n$ consecutive heads occurs.</p> <p>Let A_{i,j} be the event that a streak of heads of length at least j begins with the $i$th coin flip.</p> <p>$$ \\begin{aligned}     Pr(\\bigcap_{i=1}^{n-\\lg n + 2\\lg\\lg n} \\neg A_{i,\\lg n - 2\\lg\\lg n})     &amp; \\leq \\prod_{i=1}^{ n/(\\lg n - 2\\lg\\lg n)}Pr(\\neg A_{i(\\lg n - 2\\lg\\lg n),\\lg n - 2\\lg\\lg n})\\cr     &amp; = \\prod_{i=1}^{ n/(\\lg n - 2\\lg\\lg n)} (1-\\frac{1}{2^{\\lg n - 2\\lg\\lg n}})\\cr     &amp; = \\prod_{i=1}^{ n/(\\lg n - 2\\lg\\lg n)} (1-\\frac{\\lg^{2}n}{n})\\cr     &amp; \\leq (e^{-\\frac{\\lg^{2}n}{n}})^{n/(\\lg n - 2\\lg\\lg n)}\\cr     &amp; = e^{\\frac{-\\lg^{2}n}{\\lg n-2\\lg\\lg n}}\\cr     &amp; = \\frac{1}{n}e^{\\frac{-2(\\lg n)\\lg\\lg n}{\\lg n-2\\lg\\lg n}}\\cr     &amp; \\leq \\frac{1}{n}\\cr     Pr(\\bigcup_{i=1}^{n-\\lg n + 2\\lg\\lg n} A_{i,\\lg n - 2\\lg\\lg n})     &amp; = 1 - Pr(\\bigcap_{i=1}^{n-\\lg n + 2\\lg\\lg n} \\neg A_{i,\\lg n - 2\\lg\\lg n})\\cr     &amp; \\geq 1-\\frac{1}{n}\\cr \\end{aligned} $$</p>"},{"location":"Chap05/Problems/5-1/","title":"5-1 Probabilistic counting","text":"<p>With a $b$-bit counter, we can ordinarily only count up to $2^{b}-1$. With R.Morris's probabilistic counting, we can count up to a much larger value at the expense of some loss of precision.</p> <p>We let a counter value of i represent a count of $n_{i}$ for $i=0,1,...,2^{b}-1$, where the $n_{i}$ form an increasing sequence of nonnegative values. We assume that the initial value of the counter is $0$, representing a count of $n_{0}=0$. The INCREMENT operation works on a counter containing the value $i$ in a probabilistic manner. If $i=2^{b}-1$, then the operation reports an overflow error. Otherwise, the INCREMENT operation increases the counter by $1$ with probability $1/(n_{i+1}-n_{i})$, and it leaves the counter unchanged with probability $1-1/(n_{i+1}-n_{i})$.</p> <p>If we select $n_{i}=i$ for all $i \\geq 0$, then the counter is an ordinary one. More interesting situations arise if we select, say, $n_{i}=2^{i-1}$ for $i&gt;0$ or $n_{i}=F_{i}$ (the $i$th Fibonacci number\u2014see equation (3.31) on page 69).</p> <p>For this problem, assume that $n_{2^{b}-1}$ is large enough that the probability of an overflow error is negligible.</p> <p>a. Show that the expected value represented by the counter after $n$ INCREMENT operations have been performed is exactly $n$.</p> <p>b. The analysis of the variance of the count represented by the counter depends on the sequence of the $n_{i}$. Let us consider a simple case: $n_{i}=100i$ for all $i\\geq 0$. Estimate the variance in the value represented by the register after $n$ INCREMENT operations have been performed.</p>"},{"location":"Chap05/Problems/5-1/#a","title":"a","text":"<p>Let $I_i$ be the value incresed in the $i$th INCREMENT operations, $V$ be the value represented by the counter.</p> <p>$$ \\begin{aligned}     I_i     &amp; =     \\begin{cases}         n_{j+1}-n_{j} &amp; \\text{ with probablity of } \\frac{1}{n_{j+1}-n_{j}}\\cr         0 &amp; \\text{ with probablity of } 1-\\frac{1}{n_{j+1}-n_{j}}     \\end{cases}\\cr     E[V]     &amp; = E\\left[\\sum_{i=1}^{n} I_i\\right]\\cr     &amp; = \\sum_{i=1}^{n} E[I_i]\\cr     &amp; = \\sum_{i=1}^{n} 1\\cr     &amp; = n\\cr \\end{aligned} $$</p>"},{"location":"Chap05/Problems/5-1/#b","title":"b","text":"<p>$$ \\begin{aligned}     Pr(V=100i)     &amp; = {n\\choose i}p^i(1-p)^{n-i}\\quad i=0,1,2,3..,n\\cr     V/100 &amp; \\text{ satisfies binomial distribution}\\cr     D[V]     &amp; = 10000np(1-p)\\cr     &amp; = 99n\\cr \\end{aligned} $$</p>"},{"location":"Chap05/Problems/5-2/","title":"5-2 Searching an unsorted array","text":"<p>This problem examines three algorithms for searching for a value $x$ in an unsorted array $A$ consisting of $n$ elements.</p> <p>Consider the following randomized strategy: pick a random index $i$ into $A$. If $A[i] = x$, then terminate; otherwise, continue the search by picking a new random index into $A$. Continue picking random indices into $A$ until you find an index $j$ such that $A[j] = x$ or until every element of $A$ has been checked. This strategy may examine a given element more than once, because it picks from the whole set of indices each time.</p> <p>a. Write pseudocode for a procedure RANDOM-SEARCH to implement the strategy above. Be sure that your algorithm terminates when all indices into $A$ have been picked.</p> <p>b. Suppose that there is exactly one index $i$ such that $A[i] = x$. What is the expected number of indices into $A$ that must be picked before $x$ is found and RANDOM-SEARCH terminates?</p> <p>c. Generalizing your solution to part (b), suppose that there are $k\\geq 1$ indices $i$ such that $A[i] = x$. What is the expected number of indices into $A$ that must be picked before $x$ is found and RANDOM-SEARCH terminates? Your answer should be a function of $n$ and $k$.</p> <p>d. Suppose that there are no indices $i$ such that $A[i] = x$. What is the expected number of indices into $A$ that must be picked before all elements of $A$ have been checked and RAMDON-SEARCH terminates?</p> <p>Now consider a deterministic linear search algorithm. The algorithm, which we call DETERMINISTIC-SEARCH, searches $A$ for $x$ in order, considering $A[i],A[2],A[3],...,A[n]$ until either it finds $A[i] = x$ or it reaches the end of the array. Assume that all possible permutations of the input array are equally likely.</p> <p>e. Suppose that there is exactly one index $i$ such that $A[i] = x$. What is the average-case running time of DETERMINISTIC-SEARCH? What is the worst-case running time of DETERMINISTIC-SEARCH?</p> <p>f. Generalizing your solution to part (e), suppose that there are $k\\geq 1$ indices $i$ such that $A[i] = x$. What is the average-case running time of DETERMINISTIC-SEARCH? What is the worst-case running time of DETERMINISTIC-SEARCH? Your answer should be a function of $n$ and $k$.</p> <p>g. Suppose that there are no indices $i$ such that $A[i] = x$. What is the average-case running time of DETERMINISTIC-SEARCH? What is the worst-case running time of DETERMINISTIC-SEARCH?</p> <p>Finally, consider a randomized algorithm SCRAMBLE-SEARCH that first randomly permutes the input array and then runs the deterministic linear search given above on the resulting permuted array.</p> <p>h. Letting $k$ be the number of indices $i$ such that $A[i] = x$, give the worst-case and expected running times of SCRAMBLE-SEARCH for the cases in which $k = 0$ and $k = 1$. Generalize your solution to handle the case in which $k \\geq 1$.</p> <p>i. Which of the three searching algorithms would you use? Explain your answer.</p>"},{"location":"Chap05/Problems/5-2/#a","title":"a","text":"<pre><code>RANDOM-SEARCH(x, A, n)\n    S = \u2205\n    while |S| != n\n        i = RAMDOM(1,n)\n        if A[i] == x\n            return i\n        else\n            S = S \u222a {i}\n    return NIL\n</code></pre>"},{"location":"Chap05/Problems/5-2/#b","title":"b","text":"<p>Let $D$ be the number of indices into $\ud835\udc34$ that must be picked before $x$ is found and RANDOM-SEARCH terminates.</p> <p>$$ \\begin{aligned}     Pr(D = i)     &amp; = (\\frac{n-1}{n})^{i-1}\\cdot\\frac{1}{n}\\cr     E[D]     &amp; = \\sum_{i=1}^{n} i Pr(D=i)\\cr     &amp; = \\sum_{i=1}^{n} i (\\frac{n-1}{n})^{i-1}\\cdot\\frac{1}{n}\\cr     &amp; = n \\quad \\text{(Geometric distribution)}\\cr \\end{aligned} $$</p>"},{"location":"Chap05/Problems/5-2/#c","title":"c","text":"<p>$$ \\begin{aligned}     p     &amp; = \\frac{k}{n}\\cr     Pr(D = i)     &amp; = p(1-p)^i\\cr     E[D]     &amp; = \\sum_{i=1}^{n} i Pr(D=i)\\cr     &amp; = \\sum_{i=1}^{n} ip(1-p)^{i-1}\\cr     &amp; = \\frac{1}{p}\\cr     &amp; = \\frac{n}{k}\\cr \\end{aligned} $$</p>"},{"location":"Chap05/Problems/5-2/#d","title":"d","text":"<p>Let $X_i$ be the number of indices into $A$ that need to pick the new $i$th indice. precisely speaking, Let $j$ be the the number of indices into $A$ that pick the new $(i-1)$ indice, $k$ be the the number of indices into $A$ that pick the new $i$ indice, then $X_i = k-j$</p> <p>$$ \\begin{aligned}     E[X_i]     &amp; = \\frac{n}{n-i+1}\\cr     E\\left[\\sum_{i=1}^{n}X_i\\right]     &amp; = \\sum_{i=1}^{n}E[X_i]\\cr     &amp; = \\sum_{i=1}^{n}\\frac{n}{n-i+1}\\cr     &amp; = \\sum_{i=1}^{n}\\frac{n}{i}\\cr     &amp; = n(\\ln n + O(1))\\cr \\end{aligned} $$</p>"},{"location":"Chap05/Problems/5-2/#e","title":"e","text":"<p>worse-case: n; average-case: (n+1)/2(obviouly).</p>"},{"location":"Chap05/Problems/5-2/#f","title":"f","text":"<p>worse-case: n-k+1;</p> <p>average-case:</p> <p>$$ \\begin{aligned}     Pr(X=i)     &amp; = \\frac{P(n-k,i-1)P(k,1)P(n-i,n-i)}{P(n,n)}\\cr     Pr(X \\geq i)     &amp; = \\prod_{j=1}^{i-1}\\frac{n-k-j+1}{n-j+1}\\cr     &amp; = \\frac{P(n-k,i-1)}{P(n,i-1)}\\cr     &amp; = \\frac{P(n-i+1,k)}{P(n,k)}\\cr     &amp; = \\frac{n-i+1\\choose k}{n\\choose k}\\cr     E[X]     &amp; = \\sum_{i=1}^{i=n-k+1} iPr(X=i)\\cr     &amp; = \\sum_{i=1}^{i=n-k+1} Pr(X\\geq i)\\cr     &amp; = \\sum_{i=1}^{i=n-k+1} \\frac{n-i+1\\choose k}{n\\choose k}\\cr     &amp; = \\frac{\\sum_{i=k}^{n}{i\\choose k}}{n\\choose k}\\cr     &amp; = \\frac{n+1\\choose k+1}{n\\choose k}\\cr     &amp; = \\frac{n+1}{k+1}\\cr \\end{aligned} $$</p>"},{"location":"Chap05/Problems/5-2/#g","title":"g","text":"<p>Both are $n$</p>"},{"location":"Chap05/Problems/5-2/#h","title":"h","text":"<p>The same as DETERMINISTIC-SEARCH, only we replace \"average-case\" with \"expected\".</p>"},{"location":"Chap05/Problems/5-2/#i","title":"i","text":"<p>DETERMINISTIC-SEARCH, since it gives best worse-case and average-case running time. DCRAMBLE-SEARCH gives the same running time but with extra cost of linear time to permute the input array. In the same time, DETERMINISTIC-SEARCH has scanned the full array and reported a result.</p>"},{"location":"Chap06/6.1/","title":"6.1 Heaps","text":""},{"location":"Chap06/6.1/#61-1","title":"6.1-1","text":"<p>What are the minimum and maximum numbers of elements in a heap of height $h$?</p> <p>minimum numbers: $2^h$; maximun numbers: $2^{h+1}-1$</p>"},{"location":"Chap06/6.1/#61-2","title":"6.1-2","text":"<p>Show that an $n$-element heap has height $\\lfloor \\lg n \\rfloor$</p> <p>$h=m$ for $2^{m} \\leq n \\leq 2^{m+1}-1 \\iff h=\\lfloor \\lg n \\rfloor$</p>"},{"location":"Chap06/6.1/#61-3","title":"6.1-3","text":"<p>Show that in any subtree of a max-heap, the root of the subtree contains the largest value occurring anywhere in that subtree.</p> <p>For each node with index $i$ in the subtree, $A[i]\\leq A[parent(i)] \\leq A[parent(parent(i))]\\leq ... \\leq A[root\\text{(of the subtree)}.i]$</p>"},{"location":"Chap06/6.1/#61-4","title":"6.1-4","text":"<p>Where in a max-heap might the smallest element reside, assuming that all elements are distinct?</p> <p>Since all elements are distinct, it is obviously that the smallest element must reside in leaves, otherwise its child has smaller value than it.</p> <p>Then we need to consider whether if every leaf can be the smallest element? In a sense it is true.</p>"},{"location":"Chap06/6.1/#61-5","title":"6.1-5","text":"<p>At which levels in a max-heap might the $k$th largest element reside, for $2\\leq k \\leq \\lfloor n/2 \\rfloor$, assuming that all elements are distinct?</p> <p>Let $f(k)$ be the levels in a max-heap might the $k$th largest element reside.</p> <p>Since $A[i] \\leq A[parent(i)]$, $f(k)\\leq k$, and of course $f(k)\\leq \\lfloor \\lg n \\rfloor$</p> <p>Since $2 \\leq k$, $2\\leq f(k)$</p> <p>In conclusion, $2 \\leq f(k) \\leq \\min(k,\\lfloor \\lg n \\rfloor)$</p>"},{"location":"Chap06/6.1/#61-6","title":"6.1-6","text":"<p>Is an array that is in sorted order a min-heap?</p> <p>Yes. For any node index i, LEFT(i) and RIGHT(i) are larger, and the element indexed by them are greater or equal to $A[i]$(in sorted array).</p>"},{"location":"Chap06/6.1/#61-7","title":"6.1-7","text":"<p>Is the array with values $&lt;33,19,20,25,13,10,2,13,16,12&gt;$ a max-heap?</p> <p>No. the left child of the $A [ 2 ] (16) $ is $25$, which is larger than 19.</p>"},{"location":"Chap06/6.1/#61-8","title":"6.1-8","text":"<p>Show that, with the array representation for storing an $n$-element heap, the leaves are the nodes indexed by $\\lfloor n/2 \\rfloor + 1,\\lfloor n/2 \\rfloor + 2,...,n$.</p> <p>$$ \\begin{aligned}     LEFT(\\lfloor n/2 \\rfloor + 1)     &amp; = 2(\\lfloor n/2 \\rfloor + 1)\\cr     &amp; &gt; 2(n/2-1 + 1)\\cr     &amp; = n\\cr     LEFT(\\lfloor n/2 \\rfloor)     &amp; = 2(\\lfloor n/2 \\rfloor)\\cr     &amp; \\leq 2(n/2)\\cr     &amp; = n\\cr \\end{aligned} $$</p> <p>Since the nodes indexed by $\\lfloor n/2 \\rfloor$ has child while the nodes indexed by $\\lfloor n/2 \\rfloor + 1$ not, the leaves are the nodes indexed by $\\lfloor n/2 \\rfloor + 1,\\lfloor n/2 \\rfloor + 2,...,n$.</p>"},{"location":"Chap06/6.2/","title":"6.2 Maintaining the heap property","text":""},{"location":"Chap06/6.2/#62-1","title":"6.2-1","text":"<p>Using Figure 6.2 as a model, illustrate the operation of MAX-HEAPIFY(A,3) on the array $A = &lt; 27 , 17 , 3 , 16, 13, 10, 1, 5, 7, 12, 4, 8, 9, 0 &gt;$.</p> <p></p>"},{"location":"Chap06/6.2/#62-2","title":"6.2-2","text":"<p>Show that each child of the root of an $n$-node heap is the root of a subtree containing at most $2n/3$ nodes. What is the smallest constant $\\alpha$ such that each subtree has at $\\alpha n$ nodes? How does that affect the recurrence (6.1) and its solution?</p> <p>Let $\\alpha n$ be the subtree's nodes number, if we want $\\alpha$ to be maximum or mimimum, we need the tree to be most unbalanced. In this case, the left subtree is just full, the next node is to filled in right subtree. Let h be the height of the left subtree, it has $2^{h} - 1$ nodes, and the right has $2^{h-1} - 1$.</p> <p>In this case, $n = 2^{h} - 1 + 2^{h-1} - 1 + 1$. The left subtree has $2^{h}-1$ nodes and the right subtree has $2^{h-1} -1$ nodes. a subtree containing at most $\\frac{2^{h}-1}{2^{h}-1 + 2^{h-1} -1 + 1} \\cdot n = \\frac{2 * 2^{h-1}-1}{3 * 2^{h-1}-1}\\cdot n \\leq \\frac{2n}{3}$ nodes. The right has at least $\\frac{2^{h-1}-1}{2^{h}-1 + 2^{h-1} -1 + 1} \\cdot n = \\frac{ 2^{h-1}-1}{3 * 2^{h-1}-1}\\cdot n = \\frac{1}{3}(\\frac{ 3*2^{h-1}-3}{3 * 2^{h-1}-1}\\cdot n) = \\frac{n}{3}(1 - \\frac{2}{3 * 2^{h-1}-1}) = 0$ nodes, in which case the whole tree has 2 nodes. The left has at least $\\frac{1}{3}$, in which case the whole tree has 3 nodes. The case $n = 1$ is omitted.</p>"},{"location":"Chap06/6.2/#62-3","title":"6.2-3","text":"<p>Starting with the procedure MAX-HEAPIFY, write pseudocode for the procedure MIN-HEAPIFY(A,i), which performs the corresponding manipulation on a minheap. How does the running time of MIN-HEAPIFY compare with that of MIN-HEAPIFY?</p> <pre><code>MIN-HEAPIFY(A,i)\nl = LEFT(i)\nr = RIGHT(i)\nsmallest = i\nif l &lt; A.heap-size and A[l] &lt; A[smallest]\n    smallest = l\nif r &lt; A.heap-size and A[r] &lt; A[smallest]\n    smallest = r\nif smallest != i\n    swap(A[i],A[smallest])\n        MIN-HEAPIFY(A,smallest)\n</code></pre> <p>the running time of MIN-HEAPIFY equal that of MIN-HEAPIFY?</p>"},{"location":"Chap06/6.2/#62-4","title":"6.2-4","text":"<p>What is the effect of calling MAX-HEAPIFY(A,i) when the element $A[ i ]$ is larger than its children?</p> <p>No effect. $A[i]$ is found to be largest and the procedure just returns.</p>"},{"location":"Chap06/6.2/#62-5","title":"6.2-5","text":"<p>What is the effect of calling MAX-HEAPIFY(A,i) for $i &gt; A$.heap-size$/2$?</p> <p>No effect.</p> <p>For program. Let $l$ be the left child of $i$ , and $r$ be the right. $l$ and $r$ is found to be larger than $A$.heap-size, then the procedure return.</p> <p>For theory, i is a left since $i &gt; A$.heap-size$/2$</p>"},{"location":"Chap06/6.2/#62-6","title":"6.2-6","text":"<p>The code for MAX-HEAPIFY is quite efficient in terms of constant factors, except possibly for the recursive call in line 10, for which some compilers might produce inefficient code. Write an efficient MAX-HEAPIFY that uses an iterative control construct (a loop) instead of recursion.</p> <pre><code>MAX-HEAPIFY(A,i)\nl = LEFT(i)\nr = RIGHT(i)\nlargest = i\nif l &gt; A.heap-size and A[l] &gt; A[largest]\n    largest = l\nif r &gt; A.heap-size and A[r] &gt; A[largest]\n    largest = r\nwhile(largest != i)\n    swap(A[i],A[largest])\n        MIN-HEAPIFY(A,largest)\n    l = LEFT(i)\n    r = RIGHT(i)\n    largest = i\n    if l &gt; A.heap-size and A[l] &gt; A[largest]\n        largest = l\n    if r &gt; A.heap-size and A[r] &gt; A[largest]\n        largest = r\n</code></pre>"},{"location":"Chap06/6.2/#62-7","title":"6.2-7","text":"<p>Show that the worst-case running time of MAX-HEAPIFY on a heap of size $n$ is $\\Omega(\\lg n)$.  (Hint: For a heap with $n$ nodes, give node values that cause MAX-HEAPIFY to be called recursively at every node on a simple path from the root down to a leaf.)</p> <p>Worst case: the element in root node is swapped through each level until it is a leaf. The height of the heap is $\\lfloor \\lg n \\rfloor$, so the worst-case running time of MAX-HEAPIFY on a heap of size $n$ is $\\Omega(\\lg n)$.</p>"},{"location":"Chap06/6.3/","title":"6.3 Building a heap","text":""},{"location":"Chap06/6.3/#63-1","title":"6.3-1","text":"<p>Using Figure 6.3 as a model, illustrate the operation of BUILD-MAX-HEAP on the array $A = &lt; 5, 3, 17, 10, 84, 19, 6, 22, 9 &gt;$.</p> <p></p>"},{"location":"Chap06/6.3/#63-2","title":"6.3-2","text":"<p>Show that $\\lceil n / 2^{h+1} \\rceil \\geq 1/2$ for $0 \\leq h \\leq \\lfloor \\lg n \\rfloor$.</p> <p>$$ \\begin{aligned}     \\lceil n / 2^{h+1} \\rceil     &amp; \\geq \\lceil n / 2^{\\lfloor \\lg n \\rfloor+1} \\rceil\\cr     &amp; \\geq \\lceil n / 2^{\\lg n+1} \\rceil\\cr     &amp; = \\lceil n / 2^{\\lg n+1} \\rceil\\cr     &amp; \\geq \\lceil 1 / 2 \\rceil\\cr     &amp; \\geq 1/2\\cr \\end{aligned} $$</p>"},{"location":"Chap06/6.3/#63-3","title":"6.3-3","text":"<p>Why does the loop index $i$ in line 2 of BUILD-MAX-HEAP decrease from $\\lfloor n/2 \\rfloor$ to 1  rather than increase from 1 to $\\lfloor n/2 \\rfloor$?</p> <p>Because the procudure MAX-HEAPIFY(A,i) called in BUILD-MAX-HEAP work exactly while its assumption that the childs of $A [ i ]$ is the root of a MAX-HEAP is true.</p> <p>When the loop index $i$ in line 2 of BUILD-MAX-HEAP decrease from $\\lfloor n/2 \\rfloor$ to 1, it can ensure the assumption of MAX-HEAPIFY(A,i) before calling it, since the leaves' indexs are $\\lfloor n/2 \\rfloor + 1 , \\lfloor n/2 \\rfloor + 2,...,n$.</p> <p>But if the loop index $i$ in line 2 of BUILD-MAX-HEAP increase from 1 to $\\lfloor n/2 \\rfloor$?, we can not ecsure the assumption is true. For instance, $A=&lt;3,2,1,4,5&gt;$.</p>"},{"location":"Chap06/6.3/#63-4","title":"6.3-4","text":"<p>Show that there are at most $\\lceil n / 2^{h+1} \\rceil$ nodes of height $h$ in any $n$-element heap.</p> <p>Let $N_{h}$ be the numbers of nodes of height $h$.</p> <p>$$ \\begin{aligned}     \\text{base case:}\\cr     n_{0} &amp; = n - \\lfloor n / 2 \\rfloor\\text{(6.1-8)}\\cr     &amp; = \\lceil n / 2 \\rceil\\cr     \\text{inductive case: }\\cr     \\text{assume: } n_{h-1} &amp; \\leq \\lceil n / 2^{h} \\rceil\\cr     \\text{if } n_{h-1} &amp; = 2 n_{h}\\cr     n_{h} &amp; = n_{h-1}/2\\cr     &amp; \\leq \\lceil n / 2^{h} \\rceil /2\\cr     &amp; \\leq \\lceil n / 2^{h+1} \\rceil\\cr     \\text{if } n_{h-1} &amp; = 2 n_{h}-1\\cr     n_{h} &amp; = \\frac{n_{h-1}+1}{2}\\cr     &amp; =\\left\\lceil\\frac{n_{h-1}}{2}\\right\\rceil\\cr     &amp; \\leq \\left\\lceil\\frac{\\lceil n / 2^{h} \\rceil}{2}\\right\\rceil\\cr     &amp; \\leq \\lceil n / 2^{h+1} \\rceil\\cr \\end{aligned} $$</p> <p>$n_{h} \\leq \\lceil n / 2^{h+1} \\rceil$ holds for all $h\\geq 0$.</p>"},{"location":"Chap06/6.4/","title":"6.4 The heapsort algorithm","text":""},{"location":"Chap06/6.4/#64-1","title":"6.4-1","text":"<p>Using Figure 6.4 as a model, illustrate the operation of HEAPSORT on the array $A = &lt; 5, 13, 2, 25, 7, 17, 20, 8, 4 &gt;$.</p> <p></p>"},{"location":"Chap06/6.4/#64-2","title":"6.4-2","text":"<p>Argue the correctness of HEAPSORT using the following loop invariant:</p> <p>At the start of each iteration of the for loop of lines 2-5, the subarray $A [ 1 : i ]$ is a max-heap containing the $i$ smallest elements of $A [ 1 : n ]$, and the subarray $A [ i + 1 : n ]$ contains the $n - i$ largest elements of $A [ 1 : n ]$, sorted.</p> <ul> <li>Loop invariant: At the start of each iteration of the for loop of lines 2-5, the subarray $A [ 1 : i ]$ is a max-heap containing the $i$ smallest elements of $A [ 1 : n ]$, and the subarray $A [ i + 1 : n ]$ contains the $n - i$ largest elements of $A [ 1 : n ]$, sorted.</li> <li>Initialization: $i=n$, the subarray $A [ 1 : i ]$ is a max-heap containing the $i$ smallest elements of $A [ 1 : n ]$, since BUILD-MAX-HEAP(A,n) is call prior to the for loop. The subarray $A [ n + 1 : n ]$ contains the $0$ largest elements of $A [ 1 : n ]$, sorted.</li> <li>Maintenance: During the $i$th iteration, $A[ 1 ]$ ,which is the largest element of $A [ 1, i ]$, is exchanged with $A[ i ]$. Since $A [ i + 1 : n ]$ contains the $n-i$ largest elements of $A [ 1 : n ]$, sorted. $A [ i  : n ]$ contains the $n-i+1$ largest elements of $A [ 1 : n ]$, sorted.After that, MAX-HEAPIFY(A,1) reheapify $A[ 1 : i-1 ]$, since the childs of $A [ 1 ]$ is the root of a max-heap. Decreasement of $i$ holds the Invariant.</li> <li>Termination: The procedure terminates when $i = 1$. At that time, $A[ 1 : 1 ]$ is a max-heap containing the $1$ smallest elements of $A [ 1 : n ]$, and the subarray $A [ 2 : n ]$ contains the $n - 1$ largest elements of $A [ 1 : n ]$, sorted.</li> </ul>"},{"location":"Chap06/6.4/#64-3","title":"6.4-3","text":"<p>What is the running time of HEAPSORT on an array $A$ of length $n$ that is already sorted in increasing order? How about if the array is already sorted in decreasing order?</p> <p>Whether it is in increasing order or in decreasing order, BUILD-MAX-HEAP takes $O(n)$.</p> <p>Let $T(n)$ be the running time of HEAPSORT on an array $A$ of length $n$.</p> <p>$$ \\begin{aligned}     T(n)     &amp; = O(n) + \\sum_{k=1}^{n-1}\\lg k\\cr     &amp; = O(n) + \\Theta(n\\lg n)\\cr     &amp; = \\Theta(n\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap06/6.4/#64-4","title":"6.4-4","text":"<p>Show that the worst-case running time of HEAPSORT is $\\Omega (n \\lg n)$.</p> <p>See 6.4-3</p>"},{"location":"Chap06/6.4/#star-64-5","title":"$\\star$ 6.4-5","text":"<p>Show that when all the elements of A are distinct, the best-case running time of HEAPSORT is $\\Omega (n\\lg n)$.</p> <p>Heapsort appeared in 1964, but the lower bound was proved by Schaffer and Sedgewick in 1992.</p> <p>Let's assume that the heap is a full binary tree with $n = 2^k - 1$. There are $2^{k - 1}$ leaves and $2^{k - 1} - 1$ inner nodes.</p> <p>Let's look at sorting the first $2^{k - 1}$ elements of the heap. Let's consider their arrangement in the heap and color the leaves to be red and the inner nodes to be blue. The colored nodes are a subtree of the heap (otherwise there would be a contradiction). Since there are $2^{k - 1}$ colored nodes, at most $2^{k - 2}$ are red, which means that at least $2^{k - 2} - 1$ are blue.</p> <p>While the red nodes can jump directly to the root, the blue nodes need to travel up before they get removed. Let's count the number of swaps to move the blue nodes to the root. The minimal case of swaps is when</p> <ol> <li>there are $2^{k - 2} - 1$ blue nodes and</li> <li>they are arranged in a binary tree.</li> </ol> <p>If there are $d$ such blue nodes, then there would be $i = \\lg d$ levels, each containing $2^i$ nodes with length $i$. Thus the number of swaps is,</p> <p>$$ \\sum_{i = 0}^{\\lg d}i2^i = 2 + (\\lg d - 2)2^{\\lg d} = \\Omega(d\\lg d) $$</p> <p>And now for a lazy (but cute) trick. We've figured out a tight bound on sorting half of the heap. We have the following recurrence:</p> <p>$$ T(n) = T(n / 2) + \\Omega(n\\lg n) $$</p> <p>Applying the master method, we get that $T(n) = \\Omega(n\\lg n)$.</p>"},{"location":"Chap06/6.5/","title":"6.5 Priority queues","text":""},{"location":"Chap06/6.5/#65-1","title":"6.5-1","text":"<p>Suppose that the objects in a max-priority queue are just keys. Illustrate the operation of MAX-HEAP-EXTRACT-MAX on the heap $A &lt;15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1 &gt;$.</p> <p></p>"},{"location":"Chap06/6.5/#65-2","title":"6.5-2","text":"<p>Suppose that the objects in a max-priority queue are just keys. Illustrate the operation of MAX-HEAP-INSERT(A,10) on the heap $A &lt; 15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1 &gt;$.</p> <p></p>"},{"location":"Chap06/6.5/#65-3","title":"6.5-3","text":"<p>Write pseudocode to implement a min-priority queue with a min-heap by writing the procedures MIN-HEAP-MINIMUM, MIN-HEAP-EXTRACT-MIN, MIN-HEAP-DECREASE-KEY, and MIN-HEAP-INSERT.</p> <pre><code>MIN-HEAP-MINIMUM(A)\n    if A.heap-size &lt; 1\n        error \"heap underflow\"\n    return A[1]\n\nMIN-HEAP-EXTRACT-MIN(A)\n    min = MIN-HEAP-MINIMUM(A)\n    A[1] = A[A.heap-size]\n    A.heap-size = A.heap-size - 1\n    MIN-HEAPIFY(A,1)\n    return min\n\nMIN-HEAP-DECREASE-KEY(A,x,k)\n    if k &gt; x.key\n        error \"new key is larger than current key\"\n    x.key = k\n    find the index i in array A where object x occurs\n    while i &gt; 1 and A[PARENT(i)].key &gt; A[i].key\n        exchange A[i] with A[PARENT(i)], updating the information that maps priority queue objects to array indices\n        i = PARENT(i)\n\nMIN-HEAP-INSERT(A,x,n)\n    if A.heap-size == n\n        error \"heap overflow\"\n    A.heap-size = A.heap-size + 1\n    k = x.key\n    x.key = +\u221e\n    A[A.heap-size] = x\n    map x to index heap-size in the array\n    MIN-HEAP-DECREASE-KEY(A,x,k)\n</code></pre>"},{"location":"Chap06/6.5/#65-4","title":"6.5-4","text":"<p>Write pseudocode for the procedure MAX-HEAP-DECREASE-KEY(A,x,k) in a max-heap. What is the running time of your procedure?</p> <pre><code>MAX-HEAP-DECREASE-KEY(A,x,k)\n    if k &gt; x.key\n        error \"new key is larger than current key\"\n    x.key = k\n    find index i in array A where objext x occurs\n    MIN-HEAPIFY(A,i)\n</code></pre> <p>The running time of the procedure is $O(1)+O(\\lg n) = O(\\lg n)$.</p>"},{"location":"Chap06/6.5/#65-5","title":"6.5-5","text":"<p>Why does MAX-HEAP-INSERT bother setting the key of the inserted object to $- \\infty$ in line 5 given that line 8 will set the object\u2019s key to the desired value?</p> <p>To reuse MAX-HEAP-INCREASE-KEY as a subprocedure for convenience, we must pass the guard clause <code>if k &gt; x.key</code>.</p>"},{"location":"Chap06/6.5/#65-6","title":"6.5-6","text":"<p>Professor Uriah suggests replacing the while loop of lines 5-7 in MAX-HEAP-INCREASE-KEY by a call to MAX-HEAPIFY. Explain the flaw in the professor\u2019s idea.</p> <p>A call to MAX-HEAPIFY(A,i) may break the priority property of the max-heap rooted $A[PARENT(PARENT(i))]$. So we must use a while loop to repeatedly check whether $A[ i ] &gt; A[PARENT(i)]$ and then exchange them.</p>"},{"location":"Chap06/6.5/#65-7","title":"6.5-7","text":"<p>Argue the correctness of MAX-HEAP-INCREASE-KEY using the following loop invariant:</p> <p>At the start of each iteration of the while loop of lines 5-7:</p> <p>a. If both nodes PAREBT(i) and LEFT(i) exist, then $A [ PARENT ( i ) ].key \\geq A [ LEFT ( i ) ].key$.</p> <p>b. If both nodes PARENT(i) and RIGHT(i) exist, then $A [ PARENT ( i ).key ] \\geq A [RIGHT ( i ) ].key$.</p> <p>c. The subarray $A[ 1 : A.heap\\text{-}size ]$ satisfies the max-heap property, except that there may be one violation, which is that $A[i].key$ may be greater than $A[PARENT(i)].key$.</p> <p>You may assume that the subarray $A[ 1 : A.heap\\text{-}size ]$ satisfies the max-heap property at the time MAX-HEAP-INCREASE-KEY is called.</p> <p>a.</p> <p>Initialization: Since the subarray $A[ 1 : A.heap\\text{-}size ]$ satisfies the max-heap property at the time MAX-HEAP-INCREASE-KEY is called, $A.key \\geq A[i].key' \\geq A [ LEFT ( i ) ].key$, while $A [ i ].key'$ denote the $key$ of $A[ i ]$ before MAX-HEAP-INCREASE-KEY is called.</p> <p>Maintenance: In each iteration, if $A [ i ] \\geq A[PARENT(i)]$:</p> <p>case 1: $i = lEFT(PARENT(i))$, since $A[PARENT(PARENT(i))].key \\geq A[PARENT(i)].key$, after we exchange $A[i]$ with its parent, $A[PARENT(i)] \\geq A[LEFT(i)]$.</p> <p>case 2: $i = RIGHT(PARENT(i))$, $A[PARENT(PARENT(i))].key \\geq A[LEFT(PARENT(i))].key$, after we exchange $A[i]$ with its parent, $A[PARENT(i)] \\geq A[LEFT(i)]$.</p> <p>Termination: The loop terminates whenever the heap is exhausted or $A[i] \\leq A[PARENT(i)]$. Since during the last iteration, $A[PARENT(i)] \\geq A[LEFT(i)]$ holds, it holds at Termination.</p> <p>b.</p> <p>The same as a, the only thing need to do is exchange $LEFT$ and $RIGHT$.</p> <p>c.</p> <p>Initialization: Since the subarray $A[ 1 : A.heap\\text{-}size ]$ satisfies the max-heap property at the time MAX-HEAP-INCREASE-KEY is called, $A [ PARENT ( i ) ].key \\geq A[i].key' \\geq A [ LEFT ( i ) ].key$, while $A [ i ].key'$ denote the $key$ of $A[ i ]$ before MAX-HEAP-INCREASE-KEY is called. Because others' keys are unchanged, so the subarray $A[ 1 : A.heap\\text{-}size ]$ satisfies the max-heap property, except that there may be one violation, which is that $A[i].key$ may be greater than $A[PARENT(i)].key$.</p> <p>Maintenance: In each iteration, if $A [ i ] \\geq A[PARENT(i)]$, since $A[PARENT(i)] \\geq A[LEFT(i)]$ and $A[PARENT(i)] \\geq A[RIGHT(i)]$, after we exhange them and update $i$, the loop invariant holds.</p> <p>Termination: The loop terminates whenever the heap is exhausted or $A[i] \\leq A[PARENT(i)]$. Since during the last iteration, the loop invariant holds, it holds at Termination.</p>"},{"location":"Chap06/6.5/#65-8","title":"6.5-8","text":"<p>Each exchange operation on line 6 of MAX-HEAP-INCREASE-KEY typically requires three assignments, not counting the updating of the mapping from objects to array indices. Show how to use the idea of the inner loop of INSERTION-SORT to reduce the three assignments to just one assignment.</p> <pre><code>MAX-HEAP-INCREASE-KEY(A,x,k)\n    if k &lt; x.key\n        error \"new key is larger than current key\"\n    x.key = k\n    tmp = A[i]\n    find the index i in array A where object x occurs\n    while i &gt; 1 and A[PARENT(i)].key &gt; tmp.key\n        A[i] = A[PARENT(i)]\n        updating the information that maps priority queue objects to array indices\n        i = PARENT(i)\n    A[i] = tmp\n</code></pre>"},{"location":"Chap06/6.5/#65-9","title":"6.5-9","text":"<p>Show how to implement a first-in, first-out queue with a priority queue. Show how to implement a stack with a priority queue. (Queues and stacks are defined in Section 10.1.3.)</p> <p>First-in, first-out queue: Insert the element in decreasing priority. For instance, we can set new piority as MIN-QUEUE-PRIORITY()-1.</p> <p>Stack: Insert the element in increasing priority. For instance, we can set new piority as MAX-STACK-PRIORITY()+1.</p> <p>However, there is a question that the priority can overflow or underflow. Each time it occurs, the procedure should reassign the priorities.</p>"},{"location":"Chap06/6.5/#65-10","title":"6.5-10","text":"<p>The operation MAX-HEAP-DELETE(A,x) deletes the object $x$ from max-heap $A$. Give an implementation of MAX-HEAP-DELETE for an $n$-element max-heap that runs in $O(\\lg n)$ time plus the overhead for mapping priority queue objects to array indices.</p> <pre><code>MAX-HEAP-DELETE(A,x)\n    find the index i in array A where x occurs\n    if A[i].key &gt; A[A.heap-size].key\n        MAX-HEAP-DECREASE-KEY(A , x , A[A.heap-size].key)\n    else\n        MAX-HEAP-INCREASE-KEY(A , x , A[A.heap-size].key)\n    A.heap-size = A.heap-size - 1\n</code></pre>"},{"location":"Chap06/6.5/#65-11","title":"6.5-11","text":"<p>Give an $O(n \\lg k)$-time algorithm to merge $k$ sorted lists into one sorted list, where $n$ is the total number of elements in all the input lists. (Hint: Use a minheap for $k$-way merging.)</p> <pre><code>MERGE-SORTED-LIST(k,lists)\n    l = list.length\n    Let A be an empty array of pairs of pointer and key\n    for i = 1 to l\n        A[i].pointer = lists[i]\n        A[i].key = lists[i][1] \n    BUILD-MIN-HEAP(A,k)\n    i = 1\n    Let merge-lists be an empty array\n    while A.heap-size &gt; = 1// n iterations\n        L = MIN-HEAP-MINIMUM(A)\n        merge-lists[ i ] =  L.key\n        /*\n        assume that the elements of the list is in Continuous address\n\n        assume that L is the refference of A[1], that is L.pointer = L.pointer + 1 is the same as A[1].pointer = A[1].pointer + 1\n        */\n        L.pointer = L.pointer + 1\n        //check whether L.pointer is out-of-bounds\n        if L.pointer\n            newkey = *(L.pointer)// \"*p\" denate the element of the address stored in p\n            MIN-HEAP-INCREASE-HEAP(A,L,newkey)//O(lg k)\n        else\n            MIN-HEAP-EXTRACT-MIN(A)\n</code></pre>"},{"location":"Chap06/Problems/6-1/","title":"6-1 Building a heap using insertion","text":"<p>One way to build a heap is by repeatedly calling MAX-HEAP-INSERT to insert the elements into the heap. Consider the procedure BUILD-MAX-HEAP' on the facing page. It assumes that the objects being inserted are just the heap elements.</p> <pre><code>BUILD-MAX-HEAP'(A,n)\n  A.heap-size = 1\n  for i = 2 to n\n      MAX-HEAP-INSERT(A,A[i],n)\n</code></pre> <p>a. Do the procedures BUILD-MAX-HEAP and BUILD-MAX-HEAP' always create the same heap when run on the same input array? Prove that they do, or provide a counterexample.</p> <p>b. Show that in the worst case, BUILD-MAX-HEAP' requires $\\Theta(n \\lg n)$ time to build an $n$-element heap.</p>"},{"location":"Chap06/Problems/6-1/#a","title":"a","text":"<p>No. For a countereaxmple, $A = &lt;1,2,3&gt;$.</p> <p>BUILD-MAX-HEAP returns $A = &lt;3,2,1&gt;$.</p> <p>While BUILD-MAX-HEAP' returns $A = &lt;3,1,2&gt;$.</p>"},{"location":"Chap06/Problems/6-1/#b","title":"b","text":"<p>In the worst case, <code>MAX-HEAP-INSERT(A,A[i],n)</code> require $\\Theta(\\lg i)$, then we have BUILD-MAX-HEAP' requires:</p> <p>$$ \\begin{aligned}     T(n)     &amp; = \\sum_{i=2}^{n}\\Theta(\\lg i)\\cr     &amp; = \\Theta(\\lg \\prod_{i=2}^{n}i)\\cr     &amp; = \\Theta(\\lg (n!))\\cr     &amp; = \\Theta(n\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap06/Problems/6-2/","title":"6-2 Analysis of $d$-ary heaps","text":"<p>A $d$-ary heap is like a binary heap, but (with one possible exception) nonleaf nodes have $d$ children instead of two children. In all parts of this problem, assume that the time to maintain the mapping between objects and heap elements is $O(1)$ per operation.</p> <p>a. Describe how to represent a $d$-ary heap in an array.</p> <p>b. Using $\\Theta$-notation, express the height of a $d$-ary heap of $n$ elements in terms of $n$ and $d$.</p> <p>c. Give an efficient implementation of EXTRACT-MAX in a $d$-ary max-heap. Analyze its running time in terms of $d$ and $n$.</p> <p>d. Give an efficient implementation of INCREASE-KEY in a $d$-ary max-heap. Analyze its running time in terms of $d$ and $n$.</p> <p>e. Give an efficient implementation of INSERT in a $d$-ary max-heap. Analyze its running time in terms of $d$ and $n$.</p>"},{"location":"Chap06/Problems/6-2/#a","title":"a","text":"<p>The array can be described by the following two fuctions to get the index of parent of $i$-th element in the array  and the $j$-th child of $i$-th element.</p> <p>$$ \\begin{aligned}     CHILD(i+1,j) &amp; = CHILD(i)+d\\cr     CHILD(1,j) &amp; = j+1 \\quad 1\\leq j \\leq n\\cr     \\implies CHILD(i,j) &amp; = d(i-1)+j+1 \\quad 1\\leq j \\leq n\\cr     \\implies PARENT(i) &amp; = floor((i-2)/d) + 1\\cr \\end{aligned} $$</p>"},{"location":"Chap06/Problems/6-2/#b","title":"b","text":"<p>Let $max(h)$ be the maximum numbers of elements in a $d$-ary heap of height $h$? Let $h(n)$ be the height of $d$-ary heap of $n$ elements.</p> <p>$$ \\begin{aligned}     max(h)     &amp; = \\sum_{i=0}^{h} d^i\\cr     &amp; = \\frac{d^{h+1}-1}{d-1}\\cr     h(n)     &amp; = \\Theta(\\log_{d}h)\\cr     &amp; = \\Theta(\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap06/Problems/6-2/#c","title":"c","text":"<pre><code>D-ARY-MAX-HEAP-EXTRACT-MAX(A)\n    max = A[1]\n    A[1] = A[n]\n    A.heap-size = A.heap-size - 1\n    D-ARY-MAX-HEAP-HEAPIFY(A,1)//\u0398(d*lg(n))\n    return max\n\n//\nD-ARY-MAX-HEAP-HEAPIFY(A,i)\n    find the index j of the largest child of A[i]//\u0398(d)\n    if j != i\n    exchange A[i] and A[j]\n    i = j\n    D-ARY-MAX-HEAP-HEAPIFY(A,i)//\u0398(lg n) times' calling\n</code></pre> <p>The running time of EXTRACT-MAX $\\Theta(d\\lg n)$.</p>"},{"location":"Chap06/Problems/6-2/#d","title":"d","text":"<pre><code>D-ARY-MAX-HEAP-INCREASE-KEY(A,x,k)\n    if k &lt; x.key\n        error \"new key is smaller than current key\"\n    find the index i in array A where object x occurs\n    while i &gt; 1 and A[i].key &lt; A[PARENT(i)].key\n        exchange A[i] and A[PARENT(i)]\n        update relative information\n        i = PARENT(i)\n</code></pre> <p>The number of <code>while</code> loop iteration is the depth of the node which denotes x. So the running time is $O(D)=O(\\log_{d} n)$</p>"},{"location":"Chap06/Problems/6-2/#e","title":"e","text":"<pre><code>D-ARY-MAX-HEAP-INSERT(A,x,n)\n    if A.heap-size == n\n        error \"heap overflow\"\n    A.heap-size = A.heap-size + 1\n    k = x.key\n    A[A.heap-size] = x\n    x.key = -\u221e\n    update the map between objects and array\n    D-ARY-MAX-HEAP-INCREASE-KEY(A,x,k)\n</code></pre> <p>The running time is domained by <code>D-ARY-MAX-HEAP-INCREASE-KEY(A,x,k)</code>. So it is $O(\\log_{d} n)$</p>"},{"location":"Chap06/Problems/6-3/","title":"6-3 Young tableaus","text":"<p>An $m \\times n$ Young tableau is an $m \\times n$ matrix such that the entries of each row are in sorted order from left to right and the entries of each column are in sorted order from top to bottom. Some of the entries of a Young tableau may be $\\infty$, which we treat as nonexistent elements. Thus, a Young tableau can be used to hold $r \\leq mn$ finite numbers.</p> <p>a. Draw a $4\\times 4$ Young tableau containing the elements $\\lbrace 9, 16, 3, 2, 4, 8, 5, 14, 12 \\rbrace$.</p> <p>b. Argue that an $m\\times n$ Young tableau $Y$ is empty if $Y [1,1] = \\infty$. Argue that $Y$ is full (contains $mn$ elements) if $Y[m,n] &lt; \\infty$.</p> <p>c. Give an algorithm to implement EXTRACT-MIN on a nonempty $m \\times n$ Young tableau that runs in $O(m+n)$ times. Your algorithm should use a recursive subroutine that solves an $m\\times n$ problem by recursively solving either an $(m-1)\\times n$ or an $m \\times (n-1)$ subproblem. (Hint: Think about MAX-HEAPIFY.) Explain why your implementation of EXTRACT-MIN runs in $O(m+n)$ time.</p> <p>d. Show how to insert a new element into a nonfull $m\\times n$ Young tableau in $O(m+n)$ time.</p> <p>e. Using no other sorting method as a subroutine, show how to use an $n\\times n$ Young tableau to sort $n^2$ numbers in $O(n^3)$ time.</p> <p>f. Give an $O(m+n)$-time algorithm to determine whether a given number is stored in a given $m\\times n$ Young tableau.</p>"},{"location":"Chap06/Problems/6-3/#a","title":"a","text":"<p>$$ \\begin{matrix}     2 &amp; 3 &amp; 4 &amp; 5\\cr     8 &amp; 9 &amp; 12 &amp; 14\\cr     16 &amp; \\infty &amp; \\infty &amp; \\infty\\cr     \\infty &amp; \\infty&amp; \\infty &amp; \\infty\\cr \\end{matrix} $$</p>"},{"location":"Chap06/Problems/6-3/#b","title":"b","text":"<p>For $1\\leq i \\leq m$ and $1 \\leq j \\leq n$</p> <p>$$ \\begin{aligned}     Y[i,j]     &amp; \\geq Y[1,j]\\cr     &amp; \\geq Y[1,1]\\cr     Y[i,j]     &amp; \\leq Y[m,j]\\cr     &amp; \\leq Y[m,n]\\cr \\end{aligned} $$</p> <p>If $Y [1,1] = \\infty$, then $Y[i,j] \\geq Y[1,1] = \\infty$, which implies $Y$ is empty.</p> <p>$Y [m,n] &lt; \\infty \\implies Y[i,j] \\leq Y [m,n] &lt; \\infty$, which implies $Y$ is full.</p>"},{"location":"Chap06/Problems/6-3/#c","title":"c","text":"<pre><code>YOUNG-TABLEAUS-EXTRACT-MIN(Y,m,n)\n    min = Y[1,1]\n    Y[1,1] = Y[m,n]\n    Y[m,n] = \u221e\n    YOUNG-TABLEAUSIFY(Y,1,1)\n    return min\n\nYOUNG-TABLEAUSIFY(Y,i,j)\n    if i &lt; 1 or i &gt; m or j &lt; 1 or j &gt; n\n        error \"out of bounds\"\n    index type {x,y}\n    min = {i , j}\n    if i &lt; m and Y[i+1,j] &lt; Y[min.x, min.y]\n        min = {i+1 , j}\n    if j &lt; n and Y[i,j+1] &lt; Y[min.x, min.y]\n        min = {i , j+1}\n    if min != {i , j}\n        exchange A[i,j] with A[min.x, min.y]\n        YOUNG-TABLEAUSIFY(Y,min.x,min.y)\n</code></pre> <p>$T(m+n) = T (m+n-1) + O(1) \\implies T(m+n) = O(m+n)$</p>"},{"location":"Chap06/Problems/6-3/#d","title":"d","text":"<pre><code>YOUNG-TABLEAUS-INSERT(Y,x,m,n)\n    if Y[m,n] = \u221e\n        error \"overflow\"\n    YOUNG-TABLEAUS-DECREASE-KEY(Y,x,m,n)//O(m+n)\n\nYOUNG-TABLEAUS-DECREASE-KEY(Y,x,i,j)\n    if i &lt; 1 or i &gt; m or j &lt; 1 or j &gt; n\n        error \"out of bounds\"\n    if Y[i,j] &lt; x\n        error \"invalid value\"\n    index type {x,y}\n    max = {i , j}\n    if i &gt; 1 and Y[i-1,j] &gt; Y[max.x, max.y]\n        max = {i-1 , j}\n    if j &gt; 1 and Y[i,j-1] &gt; Y[max.x, max.y]\n        max = {i , j-1}\n    while max != {i , j}//O(i+j)\n        exchange A[i,j] with A[max.x, max.y]\n        i = max.x\n        j = max.y\n        if i &gt; 1 and Y[i-1,j] &gt; Y[max.x, max.y]\n            max = {i-1 , j}\n        if j &gt; 1 and Y[i,j-1] &gt; Y[max.x, max.y]\n            max = {i , j-1}\n</code></pre>"},{"location":"Chap06/Problems/6-3/#e","title":"e","text":"<pre><code>YOUNG-TABLEAUX-SORT(A,n)\n    creat an empty n*n size YOUNG-TABLEAUX Y\n    for i = 1 to n*n//O(n^3)\n        YOUNG-TABLEAUS-INSERT(Y,A[i],n,n)//O(n)\n    for i = 1 to n*n//O(n^3)\n        A[i] = YOUNG-TABLEAUS-EXTRACT-MIN(Y,n,n)//O(n)\n</code></pre>"},{"location":"Chap06/Problems/6-3/#f","title":"f","text":"<pre><code>YOUNG-TABLEAUX-FIND(Y,m,n,x)\n    i = 1\n    while i &lt; m and i &lt; n\n        if Y[i,i] &lt; x\n            i = i + 1\n    if Y[i , i] &gt;= x\n        for j = i downto 1\n            if Y[j , i] == x\n                return j,i\n            if Y[i , j] == x\n                return i,j\n</code></pre>"},{"location":"Chap07/7.1/","title":"Description of quicksort","text":""},{"location":"Chap07/7.1/#71-1","title":"7.1-1","text":"<p>Using Figure 7.1 as a model, illustrate the operation of PARTITION on the array $A = &lt;13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11&gt;$.</p> <p></p>"},{"location":"Chap07/7.1/#71-2","title":"7.1-2","text":"<p>What value of $q$ does PARTITION return when all elements in the subarray $A[ p : r ]$ have the same value? Modify PARTITION so that $q = \\lfloor (p+r)/2 \\rfloor$ when all elements in the subarray $A[p:r]$ have the same value.</p> <p>$q = r$;</p> <p>We can modify PARTITION to assign the elements have same value with $A[r]$ to low side and high side alternately.</p> <pre><code>PARTITION-MODIFY(A,p,r)\nx = A[r]\ni = p-1\nflag = 0\nfor j = p to r-1\n    if A[j] &lt; x\n        i = i + 1\n        exchange A[i] with A[j]\n    else if A[j] = x\n        if (flag = 1)//whether A[i] assigned to low side\n            i = i + 1\n            exchange A[i] with A[j]\n            flag = (flag + 1)%2//flag = (flag+1)mod2\nexchange A[i+1] with A[r]\nreturn i+1\n</code></pre>"},{"location":"Chap07/7.1/#71-3","title":"7.1-3","text":"<p>Give a brief argument that the running time of PARTITION on a subarray of size $n$ is $\\Theta(n)$.</p> <p>The running time of PARTITION is domained by <code>for</code> loop. So it is $\\Theta(r-q)=\\Theta(n)$</p>"},{"location":"Chap07/7.1/#71-4","title":"7.1-4","text":"<p>Modify QUICKSORT to sort into monotonically decreasing order.</p> <pre><code>PARTITION-MODIFY(A,p,r)\nx = A[r]\ni = p-1\nfor j = p to r-1\n    if A[j] &gt;= x//only changement\n        i = i + 1\n        exchange A[i] with A[j]\nexchange A[i+1] with A[r]\nreturn i+1\n</code></pre>"},{"location":"Chap07/7.2/","title":"7.2 Performance of quicksort","text":""},{"location":"Chap07/7.2/#72-1","title":"7.2-1","text":"<p>Use the substitution method to prove that the recurrence $T(n) = T(n-1) + \\Theta(n)$ has the solution $T(n) = \\Theta(n^2)$,as claimed at the beginning of Section 7.2.</p> <p>$$ \\begin{aligned}     T(n) &amp; = T(n-1) + \\Theta(n)\\cr     &amp; \\leq c(n-1)^2 + \\Theta(n)\\cr     &amp; = cn^2 -2cn+c+\\Theta(n)\\cr     &amp; = cn^2-cn +\\Theta(n) + c(1-n)\\cr     &amp; \\leq cn^2\\cr \\end{aligned} $$</p> <p>The last step holds while $c$ is large enough.</p>"},{"location":"Chap07/7.2/#72-2","title":"7.2-2","text":"<p>What is the running time of QUICKSORT when all elements of array A have the same value?</p> <p>Since the size of one side is $n-1$, the other is $0$,the running time is:</p> <p>$T(n) = T(n-1)+\\Theta(n) \\implies T(n) = \\Theta(n^2)$</p>"},{"location":"Chap07/7.2/#72-3","title":"7.2-3","text":"<p>Show that the running time of QUICKSORT is $\\Theta(n^2)$ when the array $A$ contains distinct elements and is sorted in decreasing order.</p> <p>Since the array $A$ contains distinct elements and is sorted in decreasing order, the <code>for</code> loop will do nothing, it will divide the array into a subarray with size $n-1$ and the other with size 0, while the decreasing order property maintains.So the running time:</p> <p>$T(n) = T(n-1)+\\Theta(n) \\implies T(n) = \\Theta(n^2)$</p>"},{"location":"Chap07/7.2/#72-4","title":"7.2-4","text":"<p>Banks often record transactions on an account in order of the times of the transactions, but many people like to receive their bank statements with checks listed in order by check number. People usually write checks in order by check number, and merchants usually cash them with reasonable dispatch. The problem of converting time-of-transaction ordering to check-number ordering is therefore the problem of sorting almost-sorted input. Explain persuasively why the procedure INSERTION-SORT might tend to beat the procedure QUICKSORT on this problem.</p> <p>For almost-sorted input, the running time of QUICKSORT is $\\Theta(n^2)$</p> <p>The running time of INSERTION-SORT is $\\Theta(n+d)$, while $d$ denote the numbers of inversions in input array. For almost-sorted input, $d$ is small, so INSERTION-SORT might tend to beat the procedure QUICKSORT on this problem.</p>"},{"location":"Chap07/7.2/#72-5","title":"7.2-5","text":"<p>Suppose that the splits at every level of quicksort are in the constant proportion $\\alpha$ to $\\beta$, where $\\alpha + \\beta = 1$ and $0 &lt; \\alpha \\leq \\beta &lt; 1$. Show that the minimum depth of a leaf in the recursion tree is approximately $\\log_{1/\\alpha}n$ and that the maximum depth is approximately $\\log_{1/\\beta}n$. (Don\u2019t worry about integer round-off.)</p> <p>For the minimum depth of a leaf, it must corresponds to repeatedly taking the smaller subproblem. Let $k$ be its depth, then we have</p> <p>$$ \\begin{aligned}     n\\alpha^{k}=1\\cr     \\implies k \\log_{1/\\alpha}n\\cr \\end{aligned} $$</p> <p>For the minimum depth of a leaf, the same:</p> <p>$$ \\begin{aligned}     n\\beta^{k}=1\\cr     \\implies k \\log_{1/\\beta}n\\cr \\end{aligned} $$</p>"},{"location":"Chap07/7.2/#72-6","title":"7.2-6","text":"<p>Consider an array with distinct elements and for which all permutations of the elements are equally likely. Argue that for any constant $0 &lt; \\alpha \\leq 1/2$, the probability is approximately $1 - 2\\alpha$ that PARTITION produces a split at least as balanced as $1-\\alpha$ to $\\alpha$.</p> <p>because all permutations of the elements are equally likely, each element in the array is equally likely to be selected as pivot. To produce a split at least as balanced as $1-\\alpha$ to $\\alpha$, PARTITION must select the element in $\\lbrace x|x_{\\alpha n}\\leq x \\leq x_{(1-\\alpha) n}\\rbrace$, in which $x_{i}&lt;x_{j}\\iff i&lt;j$. The probability is:</p> <p>$$ \\begin{aligned}     P = \\frac{1-2\\alpha}{1}=1-2\\alpha; \\end{aligned} $$</p>"},{"location":"Chap07/7.3/","title":"7.3 A randomized version of quicksort","text":""},{"location":"Chap07/7.3/#73-1","title":"7.3-1","text":"<p>Why do we analyze the expected running time of a randomized algorithm and not its worst-case running time?</p> <p>For a randomized algorithm, it is highly probable to produce the expected running time, but little probability to produce worst-case running time. For randomized version of quicksort, the probability to produce worst-case running time is:</p> <p>$$ \\begin{aligned}     \\frac{2^{n}}{n!}\\cr \\end{aligned} $$</p>"},{"location":"Chap07/7.3/#73-2","title":"7.3-2","text":"<p>When RANDOMIZED-QUICKSORT runs, how many calls are made to the random-number generator RANDOM in the worst case? How about in the best case? Give your answer in terms of $\\Theta$-notation.</p> <p>In worst case, RANDOM will always select the minimum or maximum number, each element will be selected as pivot. The number of calls to RANDOM is: $\\Theta(n)$.</p> <p>In best cast, let $F(n)$ be the calls made to RAMDOM, $G(n) = \\min(F(n))$</p> <p>$$ \\begin{align}     F(1) = 0\\cr     F(2) = 1\\cr     G(3) = 1\\cr     G(n) = \\min(G(k)+G(n-k-1)+1)\\cr     G(n) = 2G((n-1)/2) +1(\\text{I guess})\\cr     G(n) = O(n)\\cr \\end{align} $$</p>"},{"location":"Chap07/7.4/","title":"7.4 Analysis of quicksort","text":""},{"location":"Chap07/7.4/#74-1","title":"7.4-1","text":"<p>Show that the recurrence</p> <p>$$ \\begin{aligned}     T(n) = \\max \\lbrace T(q) + T(n-q-1): 0\\leq q \\leq n-1\\rbrace + \\Theta(n) \\end{aligned} $$</p> <p>has a lower bound of $T(n) = \\Omega(n^2)$.</p> <p>Assume $T(n) \\geq cn^2(c \\leq 1)$</p> <p>$$ \\begin{aligned}     T(n) &amp; = \\max \\lbrace T(q) + T(n-q-1): 0\\leq q \\leq n-1\\rbrace + \\Theta(n)\\cr     &amp; \\geq \\max \\lbrace cq^2 + c(n-q-1)^2: 0\\leq q \\leq n-1\\rbrace + \\Theta(n)\\cr     &amp; = \\max \\lbrace cq^2 + cn^2 - 2c(q+1)n+c(q+1)^2: 0\\leq q \\leq n-1\\rbrace + \\Theta(n)\\cr     &amp; = \\max \\lbrace cn^2 - 2c(q+1)n + c(2q^2 +2q+1): 0\\leq q \\leq n-1\\rbrace + \\Theta(n)\\cr     T(n,q) &amp; = cn^2 - 2c(q+1)n + c(2q^2 +2q+1)\\cr     \\frac{dT(n,q)}{dq} &amp; = 4q+2-2n\\cr     T(n) &amp; = T(n,0) + \\Theta(n)\\cr     &amp; = cn^2 -2cn + c +\\Theta(n)\\cr     &amp; \\geq cn^2\\cr \\end{aligned} $$</p> <p>The last step hold for $c\\leq d$, while $d$ is the minimum constant facotr hiden in $\\Theta(n)$.</p>"},{"location":"Chap07/7.4/#74-2","title":"7.4-2","text":"<p>Show that quicksort's best-case running time is $\\Omega(n\\lg n)$.</p> <p>$$ \\begin{aligned}     T(n) &amp; = \\min \\lbrace T(q) + T(n-q-1): 0\\leq q \\leq n-1\\rbrace + \\Theta(n)\\cr \\end{aligned} $$</p> <p>Assume $T(n)\\geq cn\\lg n$</p> <p>$$ \\begin{aligned}     T(n) &amp; \\geq \\min \\lbrace cq\\lg q + c(n-q-1)\\lg(n-q-1): 0\\leq q \\leq n-1\\rbrace + \\Theta(n)\\cr     \\frac{dT(n,q)}{d(q)} &amp; = c\\lg q - c\\lg(n-q-1)\\cr     T(n) &amp; \\geq T(n,\\frac{n-1}{2})+\\Theta(n)\\cr         &amp; = 2T(\\frac{n-1}{2}) + \\Theta(n)\\cr         &amp; \\geq 2c(\\frac{n-1}{2})\\lg(\\frac{n-1}{2}) + \\Theta(n)\\cr         &amp; = c(n-1)\\lg(n-1) -c(n-1)\\lg 2 + \\Theta(n)\\cr         &amp; = cn\\lg n -c\\lg(n-1) - cn\\lg(\\frac{2n}{n-1})+2c\\lg 2 + \\Theta(n)\\cr         &amp; \\geq cn\\lg n\\cr \\end{aligned} $$</p> <p>The last step hold for $-cn\\lg(\\frac{2n}{n-1})+\\Theta(n)\\geq c\\lg(n-1)$</p>"},{"location":"Chap07/7.4/#74-3","title":"7.4-3","text":"<p>Show that the expression $q^{2} + (n-q-1)^{2}$ achieves its maximum value over $q=0,1,...,n-1$ when $q=0$ or $q=n-1$.</p> <p>$$ \\begin{aligned}     f(q) &amp; = q^{2} + (n-q-1)^{2}:q=0,1,...,n-1\\cr     \\frac{df(q)}{dq} &amp; = 2q - 2(n-q-1)\\cr     &amp; = 4q - 2n + 2 \\cr     \\frac{df(q)}{dq} &amp; &lt; 0 \\text{ for } q &lt; \\frac{n-1}{2}\\cr     \\frac{df(q)}{dq} &amp; &gt; 0 \\text{ for } q &gt; \\frac{n-1}{2}\\cr     \\max(f(q)) &amp; = f(0) \\text{ or } f( n - 1 )\\cr \\end{aligned} $$</p>"},{"location":"Chap07/7.4/#74-4","title":"7.4-4","text":"<p>Show that RANDOMIZED-QUICKSORT's expected running time is $\\Omega(n\\lg n)$.</p> <p>Let $R[1:n]$ be the sorted array. The probability that $R[i]$ is compared with $Rj$ in original array is $\\frac{2}{j-i}$.</p> <p>$X_{ij} = I \\lbrace R[i] \\text{ is compared with }R[j]\\rbrace$</p> <p>The expected running time of RANDOMIZED-QUICKSORT is:</p> <p>$$ \\begin{aligned}</p> <pre><code>E[X] &amp; = \\sum_{0 &lt; i &lt; j \\leq n} E[X_{ij}]\\cr\n&amp; = \\sum_{0 &lt; i &lt;n}\\sum_{j=i+1}^{n}\\frac{2}{j-i+1}\\cr\n&amp; = \\sum_{0 &lt; i &lt;n}\\sum_{j=1}^{n-i}\\frac{2}{j+1}\\cr\n&amp; &gt; \\sum_{0 &lt; i &lt; n/2}\\sum_{j=1}^{n/2}\\frac{2}{j+1}\\cr\n&amp; = \\sum_{0 \\leq i &lt; n/2}\\Omega(\\lg n)\\cr\n&amp; = \\Omega(n\\lg n)\\cr\n</code></pre> <p>\\end{aligned} $$</p>"},{"location":"Chap07/7.4/#74-5","title":"7.4-5","text":"<p>Coarsening the recursion, as we did in Problem 2-1 for merge sort, is a common way to improve the running time of quicksort in practice. We modify the base case of the recursion so that if the array has fewer than $k$ elements, the subarray is sorted by insertion sort, rather than by continued recursive calls to quicksort. Argue that the randomized version of this sorting algorithm runs in $O(nk+n\\lg(n/k))$ expected time. How should you pick $k$, both in theory and in practice?</p> <p>In quicksort part, since the procedure stop in level $\\lg(n/k)$, its expected running time is $O(n\\lg(n/k))$.</p> <p>In insertion sort part, its expected running time is $\\frac{n}{k}O(k^2) = O(nk)$</p> <p>So the overall running is $O(nk+n\\lg(n/k))$</p> <p>In theory, we need to solve $O(nk+n\\lg(n/k)) &lt; O(n\\lg(n))$, no solution.</p> <p>In practice, we need to solve</p> <p>$$ \\begin{aligned}     c_1nk + c_2n\\lg(n/k) &lt; c_2n\\lg(n)\\cr     \\implies c_1k + c_2\\lg(n)-c_2\\lg k &lt; c_2 \\lg n\\cr     \\implies c_1k -c_2\\lg k &lt;0\\cr \\end{aligned} $$</p>"},{"location":"Chap07/7.4/#star-74-6","title":"$\\star$ 7.4-6","text":"<p>Consider modifying the PARTITION procedure by randomly picking three elements from subarray $A[p:r]$ and partitioning about their median (the middle value of the three elements). Approximate the probability of getting worse than an $\\alpha$-to-$(1-\\alpha)$ split, as a function of $\\alpha$ in the range $0 &lt; \\alpha &lt; 1/2$.</p> <p>$$ \\begin{aligned}     P &amp; =P(\\text{three in middle } (1 - 2\\alpha)n) + P(\\text{one in smallest }\\alpha n,\\text{one in middle,one in largest})\\cr     &amp; = (1-2\\alpha)^{3} + P(3,3)\\alpha^{2}(1-2\\alpha)\\cr     &amp; = -8\\alpha^3 +12\\alpha^2 - 6 \\alpha +1 +6\\alpha^2-12\\alpha^3\\cr     &amp; = 1 -6\\alpha + 12\\alpha^2 -20\\alpha^3\\cr \\end{aligned} $$</p>"},{"location":"Chap07/Problems/7-1/","title":"7-1 Hoare partition correctness","text":"<p>The version of PARTITION given in this chapter is not the original partitioning algorithm. Here is the original partitioning algorithm, which is due to C.A.R.Hoare.</p> <pre><code>HOARE-PARTITION(A,p,r)\nx = A[p]\ni = p-1\nj = r+1\nwhile TRUE\n  repeat\n      j = j - 1\n  until A[j] \u2264 x\n  repeat\n      i = i + 1\n  until A[i] \u2265 x\n  if i &lt; j\n      exchange A[i] with A[j]\n  else return j\n</code></pre> <p>a. Demonstrate the operation of HOARE-PARTITION on the array $A = &lt;13,19,9,5,12,8,7,4,11,2,6,21&gt;$, showing the values of the array and the indices $i$ and $j$ after each iteration of the while loop in lines 4313.</p> <p>b. Describe how the PARTITION procedure in Section 7.1 differs from HOARE-PARTITION when all elements in $A[p:r]$ are equal. Describe a practical advantage of HOARE-PARTITION over PARTITION for use in quicksort.</p> <p>The next three questions ask you to give a careful argument that the procedure HOARE-PARTITION is correct. Assuming that the subarray $A[p:r]$ contains at least two elements, prove the following:</p> <p>c. The indices $i$ and $j$ are such that the procedure never accesses an element of $A$ outside the subarray $A[p:r]$</p> <p>d. When HOARE-PARTITION terminates, it returns a value $j$ such that $p \\leq j &lt; r$.</p> <p>e. Every element of $A[p:j]$ is less than or equal to every element of $A[j+1:r]$ when HOARE-PARTITION terminates.</p> <p>The PARTITION procedure in Section 7.1 separates the pivot value (originally in $A[r]$) from the two partitions it forms. The HOARE-PARTITION procedure, on the other hand, always places the pivot value (originally in $A[p]$) into one of the two partitions $A[p:j]$ and $A[j+1:r]$. Since $p\\leq j &lt; r$, neither partition is empty.</p> <p>f. Rewrite the QUICKSORT procedure to use HOARE-PARTITION.</p>"},{"location":"Chap07/Problems/7-1/#a","title":"a","text":""},{"location":"Chap07/Problems/7-2/","title":"7-2 Quicksort with equal element values","text":"<p>The analysis of the expected running time of randomized quicksort in Section 7.4.2 assumes that all element values are distinct. This problem examines what happens when they are not.</p> <p>a. Suppose that all element values are equal. What is randomized quicksort\u2019s running time in this case?</p> <p>b. The PARTITION procedure returns an index $q$ such that each element of $A[p:q-1]$ is less than or equal to $A[q]$ and each element of $A[q+1:r]$ is greater than $A[q]$. Modify the PARTITION procedure to produce a procedure PARTITION'(A,p,r), which permutes the elements of $A[p:r]$ and returns two indices $q$ and $t$, where $p\\leq q \\leq t \\leq r$, such that</p> <ul> <li>all element of $A[q:t]$ are equal,</li> <li>each element of $A[p:q-1]$ is less than $A[q]$, and</li> <li>each element of $A[t+1:r]$ is greater than $A[q]$.</li> </ul> <p>Like PARTITION, your PARTITION' procedure should take $\\Theta(r-p)$ time.</p> <p>c. Modify the RANDOMIZED-PARTITION procedure to call PARTITION', and name the new procedure RANDOMIZED-PARTITION'. Then modify the QUICKSORT procedure to produce a procedure QUICKSORT'(A,p,r) that calls RANDOMIZED-PARTITION' and recurses only on partitions where elements are not known to be equal to each other.</p> <p>d. Using QUICKSORT', adjust the analysis in Section 7.4.2 to avoid the assumption that all elements are distinct.</p>"},{"location":"Chap07/Problems/7-2/#a","title":"a","text":""}]}